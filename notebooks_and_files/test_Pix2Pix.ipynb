{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Test_Pix2Pix.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "datw7X-OVYYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811c55f0-d494-4315-a760-ae1af6201a9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOJmMOFC6WH7",
        "outputId": "a91d5462-608c-4a27-bc0c-59124632bd1e"
      },
      "source": [
        "%cd ./drive/MyDrive/pix2pix_retry/pix2pix-pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/pix2pix_retry/pix2pix-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0GNnsAZ-3lF",
        "outputId": "d7ac9a10-aa24-4df4-f0a3-b4b6c9fd03b4"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/pix2pix_retry/pix2pix-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgv84j4NEh14"
      },
      "source": [
        "!cp -r '/content/drive/.shortcut-targets-by-id/1Z3gSm62J6rZcWbU37gk_GAhqfY-6gLw8/pix2pix_code/.'  '/content/drive/MyDrive/pix2pixCODE' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikh9CrFQgtMj",
        "outputId": "32e3e2eb-78b6-4baa-f7e4-30dc3b68833c"
      },
      "source": [
        "!git clone https://github.com/mrzhu-cool/pix2pix-pytorch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pix2pix-pytorch'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eC-QfkYg0KC"
      },
      "source": [
        "!unrar x   './soccer_data.rar' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC1TiPXSh4ii"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "from math import log10\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from networks import define_G, define_D, GANLoss, get_scheduler, update_learning_rate\n",
        "from data import get_training_set, get_test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvyThFKkmVYr"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seER2rI5iM1F",
        "outputId": "a89eb250-68fe-4e34-9aff-4c68e454e784"
      },
      "source": [
        "!python train.py --dataset ./football --direction a2b --threads 1 --batch_size 20 --cuda "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=20, beta1=0.5, cuda=True, dataset='./football', direction='a2b', epoch_count=1, input_nc=3, lamb=10, lr=0.0002, lr_decay_iters=50, lr_policy='lambda', ndf=64, ngf=64, niter=100, niter_decay=100, output_nc=3, seed=123, test_batch_size=1, threads=1)\n",
            "===> Loading datasets\n",
            "===> Building models\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "===> Epoch[1](1/10): Loss_D: 1.5782 Loss_G: 26.1390\n",
            "===> Epoch[1](2/10): Loss_D: 11.2616 Loss_G: 12.4522\n",
            "===> Epoch[1](3/10): Loss_D: 1.3540 Loss_G: 9.6698\n",
            "===> Epoch[1](4/10): Loss_D: 1.6183 Loss_G: 7.7556\n",
            "===> Epoch[1](5/10): Loss_D: 1.0786 Loss_G: 6.3947\n",
            "===> Epoch[1](6/10): Loss_D: 0.8948 Loss_G: 5.8227\n",
            "===> Epoch[1](7/10): Loss_D: 0.9255 Loss_G: 4.5257\n",
            "===> Epoch[1](8/10): Loss_D: 0.7412 Loss_G: 4.2083\n",
            "===> Epoch[1](9/10): Loss_D: 0.8093 Loss_G: 3.7129\n",
            "===> Epoch[1](10/10): Loss_D: 0.8614 Loss_G: 3.6754\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.5326 dB\n",
            "===> Epoch[2](1/10): Loss_D: 0.8272 Loss_G: 2.8300\n",
            "===> Epoch[2](2/10): Loss_D: 0.6722 Loss_G: 2.5564\n",
            "===> Epoch[2](3/10): Loss_D: 0.6289 Loss_G: 2.1980\n",
            "===> Epoch[2](4/10): Loss_D: 0.5805 Loss_G: 2.2323\n",
            "===> Epoch[2](5/10): Loss_D: 0.5247 Loss_G: 2.1120\n",
            "===> Epoch[2](6/10): Loss_D: 0.4715 Loss_G: 2.0086\n",
            "===> Epoch[2](7/10): Loss_D: 0.4386 Loss_G: 1.9639\n",
            "===> Epoch[2](8/10): Loss_D: 0.4010 Loss_G: 2.0347\n",
            "===> Epoch[2](9/10): Loss_D: 0.4510 Loss_G: 2.0217\n",
            "===> Epoch[2](10/10): Loss_D: 0.4565 Loss_G: 2.1620\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.5908 dB\n",
            "===> Epoch[3](1/10): Loss_D: 0.4785 Loss_G: 2.4583\n",
            "===> Epoch[3](2/10): Loss_D: 0.6278 Loss_G: 2.4834\n",
            "===> Epoch[3](3/10): Loss_D: 0.7621 Loss_G: 2.5107\n",
            "===> Epoch[3](4/10): Loss_D: 0.6029 Loss_G: 2.1340\n",
            "===> Epoch[3](5/10): Loss_D: 0.4148 Loss_G: 2.2180\n",
            "===> Epoch[3](6/10): Loss_D: 0.4274 Loss_G: 2.4079\n",
            "===> Epoch[3](7/10): Loss_D: 0.4927 Loss_G: 2.3050\n",
            "===> Epoch[3](8/10): Loss_D: 0.4124 Loss_G: 1.9384\n",
            "===> Epoch[3](9/10): Loss_D: 0.2890 Loss_G: 1.9546\n",
            "===> Epoch[3](10/10): Loss_D: 0.2845 Loss_G: 1.8526\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.9601 dB\n",
            "===> Epoch[4](1/10): Loss_D: 0.2588 Loss_G: 1.9787\n",
            "===> Epoch[4](2/10): Loss_D: 0.2358 Loss_G: 1.9055\n",
            "===> Epoch[4](3/10): Loss_D: 0.2382 Loss_G: 1.9403\n",
            "===> Epoch[4](4/10): Loss_D: 0.2088 Loss_G: 2.0836\n",
            "===> Epoch[4](5/10): Loss_D: 0.2285 Loss_G: 2.0533\n",
            "===> Epoch[4](6/10): Loss_D: 0.2255 Loss_G: 1.9996\n",
            "===> Epoch[4](7/10): Loss_D: 0.2347 Loss_G: 2.0224\n",
            "===> Epoch[4](8/10): Loss_D: 0.2600 Loss_G: 2.0491\n",
            "===> Epoch[4](9/10): Loss_D: 0.2975 Loss_G: 2.3010\n",
            "===> Epoch[4](10/10): Loss_D: 0.3550 Loss_G: 2.1916\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.6405 dB\n",
            "===> Epoch[5](1/10): Loss_D: 0.4143 Loss_G: 2.1095\n",
            "===> Epoch[5](2/10): Loss_D: 0.3372 Loss_G: 2.0549\n",
            "===> Epoch[5](3/10): Loss_D: 0.2895 Loss_G: 2.1867\n",
            "===> Epoch[5](4/10): Loss_D: 0.2305 Loss_G: 2.0192\n",
            "===> Epoch[5](5/10): Loss_D: 0.2145 Loss_G: 2.1794\n",
            "===> Epoch[5](6/10): Loss_D: 0.1790 Loss_G: 2.1105\n",
            "===> Epoch[5](7/10): Loss_D: 0.1678 Loss_G: 2.2120\n",
            "===> Epoch[5](8/10): Loss_D: 0.2214 Loss_G: 2.1546\n",
            "===> Epoch[5](9/10): Loss_D: 0.2034 Loss_G: 2.1226\n",
            "===> Epoch[5](10/10): Loss_D: 0.1665 Loss_G: 1.9018\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3968 dB\n",
            "===> Epoch[6](1/10): Loss_D: 0.1652 Loss_G: 1.9473\n",
            "===> Epoch[6](2/10): Loss_D: 0.2027 Loss_G: 2.1703\n",
            "===> Epoch[6](3/10): Loss_D: 0.1948 Loss_G: 2.4895\n",
            "===> Epoch[6](4/10): Loss_D: 0.3417 Loss_G: 3.6984\n",
            "===> Epoch[6](5/10): Loss_D: 0.9350 Loss_G: 3.5571\n",
            "===> Epoch[6](6/10): Loss_D: 1.2465 Loss_G: 2.4514\n",
            "===> Epoch[6](7/10): Loss_D: 0.7430 Loss_G: 2.7037\n",
            "===> Epoch[6](8/10): Loss_D: 0.5043 Loss_G: 2.1709\n",
            "===> Epoch[6](9/10): Loss_D: 0.2210 Loss_G: 2.2629\n",
            "===> Epoch[6](10/10): Loss_D: 0.1837 Loss_G: 2.1640\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3135 dB\n",
            "===> Epoch[7](1/10): Loss_D: 0.1574 Loss_G: 2.0554\n",
            "===> Epoch[7](2/10): Loss_D: 0.1532 Loss_G: 2.1259\n",
            "===> Epoch[7](3/10): Loss_D: 0.1320 Loss_G: 2.0984\n",
            "===> Epoch[7](4/10): Loss_D: 0.1300 Loss_G: 2.2387\n",
            "===> Epoch[7](5/10): Loss_D: 0.1542 Loss_G: 2.0650\n",
            "===> Epoch[7](6/10): Loss_D: 0.1727 Loss_G: 2.1908\n",
            "===> Epoch[7](7/10): Loss_D: 0.1624 Loss_G: 2.0573\n",
            "===> Epoch[7](8/10): Loss_D: 0.1548 Loss_G: 2.2178\n",
            "===> Epoch[7](9/10): Loss_D: 0.1571 Loss_G: 2.2108\n",
            "===> Epoch[7](10/10): Loss_D: 0.1564 Loss_G: 2.2201\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.1635 dB\n",
            "===> Epoch[8](1/10): Loss_D: 0.1484 Loss_G: 2.2263\n",
            "===> Epoch[8](2/10): Loss_D: 0.1513 Loss_G: 2.2569\n",
            "===> Epoch[8](3/10): Loss_D: 0.1206 Loss_G: 2.2592\n",
            "===> Epoch[8](4/10): Loss_D: 0.1104 Loss_G: 2.2101\n",
            "===> Epoch[8](5/10): Loss_D: 0.1422 Loss_G: 2.1796\n",
            "===> Epoch[8](6/10): Loss_D: 0.1464 Loss_G: 2.2322\n",
            "===> Epoch[8](7/10): Loss_D: 0.1760 Loss_G: 2.4211\n",
            "===> Epoch[8](8/10): Loss_D: 0.2070 Loss_G: 2.3516\n",
            "===> Epoch[8](9/10): Loss_D: 0.2546 Loss_G: 2.3397\n",
            "===> Epoch[8](10/10): Loss_D: 0.2640 Loss_G: 2.2814\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.9905 dB\n",
            "===> Epoch[9](1/10): Loss_D: 0.1991 Loss_G: 2.3519\n",
            "===> Epoch[9](2/10): Loss_D: 0.1566 Loss_G: 2.3082\n",
            "===> Epoch[9](3/10): Loss_D: 0.1417 Loss_G: 2.2877\n",
            "===> Epoch[9](4/10): Loss_D: 0.1438 Loss_G: 2.2746\n",
            "===> Epoch[9](5/10): Loss_D: 0.1579 Loss_G: 2.3550\n",
            "===> Epoch[9](6/10): Loss_D: 0.2015 Loss_G: 2.1131\n",
            "===> Epoch[9](7/10): Loss_D: 0.3111 Loss_G: 2.5317\n",
            "===> Epoch[9](8/10): Loss_D: 0.5018 Loss_G: 2.8621\n",
            "===> Epoch[9](9/10): Loss_D: 0.5385 Loss_G: 2.6913\n",
            "===> Epoch[9](10/10): Loss_D: 0.4700 Loss_G: 2.2284\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 6.8224 dB\n",
            "===> Epoch[10](1/10): Loss_D: 0.3711 Loss_G: 2.5527\n",
            "===> Epoch[10](2/10): Loss_D: 0.3746 Loss_G: 2.5933\n",
            "===> Epoch[10](3/10): Loss_D: 0.3738 Loss_G: 2.2777\n",
            "===> Epoch[10](4/10): Loss_D: 0.3020 Loss_G: 1.8772\n",
            "===> Epoch[10](5/10): Loss_D: 0.2068 Loss_G: 1.9279\n",
            "===> Epoch[10](6/10): Loss_D: 0.1858 Loss_G: 1.8989\n",
            "===> Epoch[10](7/10): Loss_D: 0.2265 Loss_G: 2.0584\n",
            "===> Epoch[10](8/10): Loss_D: 0.2732 Loss_G: 1.9268\n",
            "===> Epoch[10](9/10): Loss_D: 0.2614 Loss_G: 1.8069\n",
            "===> Epoch[10](10/10): Loss_D: 0.2735 Loss_G: 1.7725\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.0179 dB\n",
            "===> Epoch[11](1/10): Loss_D: 0.2625 Loss_G: 1.8323\n",
            "===> Epoch[11](2/10): Loss_D: 0.2321 Loss_G: 1.9365\n",
            "===> Epoch[11](3/10): Loss_D: 0.2881 Loss_G: 1.8166\n",
            "===> Epoch[11](4/10): Loss_D: 0.2453 Loss_G: 1.7213\n",
            "===> Epoch[11](5/10): Loss_D: 0.1888 Loss_G: 1.7875\n",
            "===> Epoch[11](6/10): Loss_D: 0.2192 Loss_G: 1.6233\n",
            "===> Epoch[11](7/10): Loss_D: 0.2439 Loss_G: 1.6537\n",
            "===> Epoch[11](8/10): Loss_D: 0.2237 Loss_G: 1.7443\n",
            "===> Epoch[11](9/10): Loss_D: 0.2271 Loss_G: 1.7940\n",
            "===> Epoch[11](10/10): Loss_D: 0.2211 Loss_G: 1.7965\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 6.9793 dB\n",
            "===> Epoch[12](1/10): Loss_D: 0.2406 Loss_G: 1.5785\n",
            "===> Epoch[12](2/10): Loss_D: 0.1811 Loss_G: 1.7756\n",
            "===> Epoch[12](3/10): Loss_D: 0.2320 Loss_G: 1.8206\n",
            "===> Epoch[12](4/10): Loss_D: 0.2547 Loss_G: 1.8014\n",
            "===> Epoch[12](5/10): Loss_D: 0.2716 Loss_G: 1.6284\n",
            "===> Epoch[12](6/10): Loss_D: 0.2687 Loss_G: 1.8869\n",
            "===> Epoch[12](7/10): Loss_D: 0.3754 Loss_G: 1.9893\n",
            "===> Epoch[12](8/10): Loss_D: 0.4050 Loss_G: 1.8768\n",
            "===> Epoch[12](9/10): Loss_D: 0.5491 Loss_G: 2.1974\n",
            "===> Epoch[12](10/10): Loss_D: 0.3869 Loss_G: 1.8053\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.2410 dB\n",
            "===> Epoch[13](1/10): Loss_D: 0.3067 Loss_G: 1.6918\n",
            "===> Epoch[13](2/10): Loss_D: 0.3356 Loss_G: 1.6137\n",
            "===> Epoch[13](3/10): Loss_D: 0.2721 Loss_G: 1.8087\n",
            "===> Epoch[13](4/10): Loss_D: 0.3096 Loss_G: 1.5716\n",
            "===> Epoch[13](5/10): Loss_D: 0.2597 Loss_G: 1.5390\n",
            "===> Epoch[13](6/10): Loss_D: 0.3574 Loss_G: 1.6318\n",
            "===> Epoch[13](7/10): Loss_D: 0.2933 Loss_G: 1.8387\n",
            "===> Epoch[13](8/10): Loss_D: 0.2961 Loss_G: 1.4842\n",
            "===> Epoch[13](9/10): Loss_D: 0.2032 Loss_G: 1.4770\n",
            "===> Epoch[13](10/10): Loss_D: 0.2502 Loss_G: 1.3934\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.5468 dB\n",
            "===> Epoch[14](1/10): Loss_D: 0.2839 Loss_G: 1.5493\n",
            "===> Epoch[14](2/10): Loss_D: 0.2882 Loss_G: 1.3451\n",
            "===> Epoch[14](3/10): Loss_D: 0.2244 Loss_G: 1.5362\n",
            "===> Epoch[14](4/10): Loss_D: 0.2568 Loss_G: 1.3548\n",
            "===> Epoch[14](5/10): Loss_D: 0.3150 Loss_G: 1.5721\n",
            "===> Epoch[14](6/10): Loss_D: 0.2548 Loss_G: 1.6028\n",
            "===> Epoch[14](7/10): Loss_D: 0.2983 Loss_G: 1.4480\n",
            "===> Epoch[14](8/10): Loss_D: 0.2980 Loss_G: 1.7347\n",
            "===> Epoch[14](9/10): Loss_D: 0.4233 Loss_G: 1.6963\n",
            "===> Epoch[14](10/10): Loss_D: 0.3920 Loss_G: 1.7857\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.1272 dB\n",
            "===> Epoch[15](1/10): Loss_D: 0.3754 Loss_G: 1.3432\n",
            "===> Epoch[15](2/10): Loss_D: 0.3226 Loss_G: 1.6005\n",
            "===> Epoch[15](3/10): Loss_D: 0.3370 Loss_G: 1.4095\n",
            "===> Epoch[15](4/10): Loss_D: 0.3133 Loss_G: 1.5474\n",
            "===> Epoch[15](5/10): Loss_D: 0.3594 Loss_G: 1.4855\n",
            "===> Epoch[15](6/10): Loss_D: 0.3398 Loss_G: 1.6967\n",
            "===> Epoch[15](7/10): Loss_D: 0.3554 Loss_G: 1.3853\n",
            "===> Epoch[15](8/10): Loss_D: 0.3035 Loss_G: 1.3761\n",
            "===> Epoch[15](9/10): Loss_D: 0.2836 Loss_G: 1.5261\n",
            "===> Epoch[15](10/10): Loss_D: 0.2914 Loss_G: 1.3811\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.2282 dB\n",
            "===> Epoch[16](1/10): Loss_D: 0.2837 Loss_G: 1.2624\n",
            "===> Epoch[16](2/10): Loss_D: 0.2735 Loss_G: 1.4327\n",
            "===> Epoch[16](3/10): Loss_D: 0.2513 Loss_G: 1.5307\n",
            "===> Epoch[16](4/10): Loss_D: 0.2743 Loss_G: 1.4804\n",
            "===> Epoch[16](5/10): Loss_D: 0.2501 Loss_G: 1.3836\n",
            "===> Epoch[16](6/10): Loss_D: 0.3011 Loss_G: 1.3220\n",
            "===> Epoch[16](7/10): Loss_D: 0.2753 Loss_G: 1.3403\n",
            "===> Epoch[16](8/10): Loss_D: 0.2565 Loss_G: 1.3399\n",
            "===> Epoch[16](9/10): Loss_D: 0.2350 Loss_G: 1.5147\n",
            "===> Epoch[16](10/10): Loss_D: 0.2730 Loss_G: 1.3019\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.2266 dB\n",
            "===> Epoch[17](1/10): Loss_D: 0.2772 Loss_G: 1.2111\n",
            "===> Epoch[17](2/10): Loss_D: 0.2551 Loss_G: 1.3230\n",
            "===> Epoch[17](3/10): Loss_D: 0.2798 Loss_G: 1.4034\n",
            "===> Epoch[17](4/10): Loss_D: 0.3431 Loss_G: 1.4903\n",
            "===> Epoch[17](5/10): Loss_D: 0.2920 Loss_G: 1.4356\n",
            "===> Epoch[17](6/10): Loss_D: 0.2513 Loss_G: 1.4102\n",
            "===> Epoch[17](7/10): Loss_D: 0.2520 Loss_G: 1.4392\n",
            "===> Epoch[17](8/10): Loss_D: 0.3142 Loss_G: 1.5375\n",
            "===> Epoch[17](9/10): Loss_D: 0.2826 Loss_G: 1.5073\n",
            "===> Epoch[17](10/10): Loss_D: 0.3000 Loss_G: 1.4000\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.5210 dB\n",
            "===> Epoch[18](1/10): Loss_D: 0.2578 Loss_G: 1.3927\n",
            "===> Epoch[18](2/10): Loss_D: 0.2557 Loss_G: 1.3541\n",
            "===> Epoch[18](3/10): Loss_D: 0.2574 Loss_G: 1.4146\n",
            "===> Epoch[18](4/10): Loss_D: 0.2670 Loss_G: 1.6424\n",
            "===> Epoch[18](5/10): Loss_D: 0.2927 Loss_G: 1.3917\n",
            "===> Epoch[18](6/10): Loss_D: 0.2738 Loss_G: 1.4183\n",
            "===> Epoch[18](7/10): Loss_D: 0.2864 Loss_G: 1.2897\n",
            "===> Epoch[18](8/10): Loss_D: 0.2423 Loss_G: 1.2903\n",
            "===> Epoch[18](9/10): Loss_D: 0.3169 Loss_G: 1.3683\n",
            "===> Epoch[18](10/10): Loss_D: 0.3255 Loss_G: 1.3494\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.6501 dB\n",
            "===> Epoch[19](1/10): Loss_D: 0.2917 Loss_G: 1.2047\n",
            "===> Epoch[19](2/10): Loss_D: 0.2906 Loss_G: 1.3697\n",
            "===> Epoch[19](3/10): Loss_D: 0.2813 Loss_G: 1.4805\n",
            "===> Epoch[19](4/10): Loss_D: 0.2835 Loss_G: 1.4789\n",
            "===> Epoch[19](5/10): Loss_D: 0.2741 Loss_G: 1.4053\n",
            "===> Epoch[19](6/10): Loss_D: 0.2520 Loss_G: 1.2667\n",
            "===> Epoch[19](7/10): Loss_D: 0.2661 Loss_G: 1.3144\n",
            "===> Epoch[19](8/10): Loss_D: 0.2736 Loss_G: 1.3195\n",
            "===> Epoch[19](9/10): Loss_D: 0.2489 Loss_G: 1.4510\n",
            "===> Epoch[19](10/10): Loss_D: 0.3017 Loss_G: 1.5539\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.0796 dB\n",
            "===> Epoch[20](1/10): Loss_D: 0.2755 Loss_G: 1.4132\n",
            "===> Epoch[20](2/10): Loss_D: 0.2675 Loss_G: 1.3345\n",
            "===> Epoch[20](3/10): Loss_D: 0.2688 Loss_G: 1.3987\n",
            "===> Epoch[20](4/10): Loss_D: 0.2979 Loss_G: 1.2207\n",
            "===> Epoch[20](5/10): Loss_D: 0.2549 Loss_G: 1.3181\n",
            "===> Epoch[20](6/10): Loss_D: 0.2592 Loss_G: 1.2962\n",
            "===> Epoch[20](7/10): Loss_D: 0.2674 Loss_G: 1.3231\n",
            "===> Epoch[20](8/10): Loss_D: 0.2515 Loss_G: 1.4083\n",
            "===> Epoch[20](9/10): Loss_D: 0.2507 Loss_G: 1.3880\n",
            "===> Epoch[20](10/10): Loss_D: 0.2713 Loss_G: 1.2507\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.7431 dB\n",
            "===> Epoch[21](1/10): Loss_D: 0.2455 Loss_G: 1.3328\n",
            "===> Epoch[21](2/10): Loss_D: 0.2708 Loss_G: 1.4108\n",
            "===> Epoch[21](3/10): Loss_D: 0.2874 Loss_G: 1.3616\n",
            "===> Epoch[21](4/10): Loss_D: 0.2485 Loss_G: 1.5237\n",
            "===> Epoch[21](5/10): Loss_D: 0.3381 Loss_G: 1.3391\n",
            "===> Epoch[21](6/10): Loss_D: 0.3209 Loss_G: 1.4337\n",
            "===> Epoch[21](7/10): Loss_D: 0.3138 Loss_G: 1.2919\n",
            "===> Epoch[21](8/10): Loss_D: 0.2677 Loss_G: 1.5629\n",
            "===> Epoch[21](9/10): Loss_D: 0.3286 Loss_G: 1.5375\n",
            "===> Epoch[21](10/10): Loss_D: 0.3400 Loss_G: 1.6126\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.8062 dB\n",
            "===> Epoch[22](1/10): Loss_D: 0.3911 Loss_G: 1.4013\n",
            "===> Epoch[22](2/10): Loss_D: 0.3709 Loss_G: 1.5271\n",
            "===> Epoch[22](3/10): Loss_D: 0.4028 Loss_G: 1.3272\n",
            "===> Epoch[22](4/10): Loss_D: 0.3550 Loss_G: 1.5021\n",
            "===> Epoch[22](5/10): Loss_D: 0.3868 Loss_G: 1.4556\n",
            "===> Epoch[22](6/10): Loss_D: 0.3458 Loss_G: 1.5753\n",
            "===> Epoch[22](7/10): Loss_D: 0.3593 Loss_G: 1.2988\n",
            "===> Epoch[22](8/10): Loss_D: 0.2811 Loss_G: 1.2899\n",
            "===> Epoch[22](9/10): Loss_D: 0.2488 Loss_G: 1.3059\n",
            "===> Epoch[22](10/10): Loss_D: 0.2314 Loss_G: 1.3272\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.7696 dB\n",
            "===> Epoch[23](1/10): Loss_D: 0.2864 Loss_G: 1.2449\n",
            "===> Epoch[23](2/10): Loss_D: 0.2862 Loss_G: 1.4954\n",
            "===> Epoch[23](3/10): Loss_D: 0.3094 Loss_G: 1.2593\n",
            "===> Epoch[23](4/10): Loss_D: 0.2742 Loss_G: 1.2594\n",
            "===> Epoch[23](5/10): Loss_D: 0.2655 Loss_G: 1.2543\n",
            "===> Epoch[23](6/10): Loss_D: 0.2443 Loss_G: 1.3612\n",
            "===> Epoch[23](7/10): Loss_D: 0.2484 Loss_G: 1.3522\n",
            "===> Epoch[23](8/10): Loss_D: 0.2594 Loss_G: 1.2864\n",
            "===> Epoch[23](9/10): Loss_D: 0.3082 Loss_G: 1.3778\n",
            "===> Epoch[23](10/10): Loss_D: 0.3205 Loss_G: 1.3288\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3751 dB\n",
            "===> Epoch[24](1/10): Loss_D: 0.2836 Loss_G: 1.3520\n",
            "===> Epoch[24](2/10): Loss_D: 0.2687 Loss_G: 1.2965\n",
            "===> Epoch[24](3/10): Loss_D: 0.2438 Loss_G: 1.3291\n",
            "===> Epoch[24](4/10): Loss_D: 0.2463 Loss_G: 1.3524\n",
            "===> Epoch[24](5/10): Loss_D: 0.2596 Loss_G: 1.2859\n",
            "===> Epoch[24](6/10): Loss_D: 0.2740 Loss_G: 1.2624\n",
            "===> Epoch[24](7/10): Loss_D: 0.3041 Loss_G: 1.3972\n",
            "===> Epoch[24](8/10): Loss_D: 0.2444 Loss_G: 1.3139\n",
            "===> Epoch[24](9/10): Loss_D: 0.2133 Loss_G: 1.3121\n",
            "===> Epoch[24](10/10): Loss_D: 0.2442 Loss_G: 1.4012\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.7455 dB\n",
            "===> Epoch[25](1/10): Loss_D: 0.2623 Loss_G: 1.2294\n",
            "===> Epoch[25](2/10): Loss_D: 0.2765 Loss_G: 1.4129\n",
            "===> Epoch[25](3/10): Loss_D: 0.2910 Loss_G: 1.3007\n",
            "===> Epoch[25](4/10): Loss_D: 0.2656 Loss_G: 1.2208\n",
            "===> Epoch[25](5/10): Loss_D: 0.2653 Loss_G: 1.3121\n",
            "===> Epoch[25](6/10): Loss_D: 0.2304 Loss_G: 1.4029\n",
            "===> Epoch[25](7/10): Loss_D: 0.2603 Loss_G: 1.3847\n",
            "===> Epoch[25](8/10): Loss_D: 0.2684 Loss_G: 1.3821\n",
            "===> Epoch[25](9/10): Loss_D: 0.2269 Loss_G: 1.3834\n",
            "===> Epoch[25](10/10): Loss_D: 0.2251 Loss_G: 1.2474\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.5658 dB\n",
            "===> Epoch[26](1/10): Loss_D: 0.2437 Loss_G: 1.4115\n",
            "===> Epoch[26](2/10): Loss_D: 0.3105 Loss_G: 1.5393\n",
            "===> Epoch[26](3/10): Loss_D: 0.2651 Loss_G: 1.3640\n",
            "===> Epoch[26](4/10): Loss_D: 0.3026 Loss_G: 1.6139\n",
            "===> Epoch[26](5/10): Loss_D: 0.2822 Loss_G: 1.4593\n",
            "===> Epoch[26](6/10): Loss_D: 0.2888 Loss_G: 1.3609\n",
            "===> Epoch[26](7/10): Loss_D: 0.3310 Loss_G: 1.4953\n",
            "===> Epoch[26](8/10): Loss_D: 0.2829 Loss_G: 1.5402\n",
            "===> Epoch[26](9/10): Loss_D: 0.2865 Loss_G: 1.3686\n",
            "===> Epoch[26](10/10): Loss_D: 0.2412 Loss_G: 1.2093\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.4174 dB\n",
            "===> Epoch[27](1/10): Loss_D: 0.2066 Loss_G: 1.2718\n",
            "===> Epoch[27](2/10): Loss_D: 0.2712 Loss_G: 1.3968\n",
            "===> Epoch[27](3/10): Loss_D: 0.2617 Loss_G: 1.3581\n",
            "===> Epoch[27](4/10): Loss_D: 0.2711 Loss_G: 1.2742\n",
            "===> Epoch[27](5/10): Loss_D: 0.2711 Loss_G: 1.2886\n",
            "===> Epoch[27](6/10): Loss_D: 0.2567 Loss_G: 1.3932\n",
            "===> Epoch[27](7/10): Loss_D: 0.2635 Loss_G: 1.4850\n",
            "===> Epoch[27](8/10): Loss_D: 0.2933 Loss_G: 1.1262\n",
            "===> Epoch[27](9/10): Loss_D: 0.2421 Loss_G: 1.2224\n",
            "===> Epoch[27](10/10): Loss_D: 0.3185 Loss_G: 1.4493\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.0414 dB\n",
            "===> Epoch[28](1/10): Loss_D: 0.1860 Loss_G: 1.4754\n",
            "===> Epoch[28](2/10): Loss_D: 0.5144 Loss_G: 1.8429\n",
            "===> Epoch[28](3/10): Loss_D: 0.4327 Loss_G: 2.3213\n",
            "===> Epoch[28](4/10): Loss_D: 0.5045 Loss_G: 1.5883\n",
            "===> Epoch[28](5/10): Loss_D: 0.3642 Loss_G: 1.3735\n",
            "===> Epoch[28](6/10): Loss_D: 0.3028 Loss_G: 1.3567\n",
            "===> Epoch[28](7/10): Loss_D: 0.2207 Loss_G: 1.5191\n",
            "===> Epoch[28](8/10): Loss_D: 0.2297 Loss_G: 1.3860\n",
            "===> Epoch[28](9/10): Loss_D: 0.2137 Loss_G: 1.3760\n",
            "===> Epoch[28](10/10): Loss_D: 0.2555 Loss_G: 1.1582\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.1781 dB\n",
            "===> Epoch[29](1/10): Loss_D: 0.2520 Loss_G: 1.4670\n",
            "===> Epoch[29](2/10): Loss_D: 0.2419 Loss_G: 1.4588\n",
            "===> Epoch[29](3/10): Loss_D: 0.2455 Loss_G: 1.3329\n",
            "===> Epoch[29](4/10): Loss_D: 0.2290 Loss_G: 1.2427\n",
            "===> Epoch[29](5/10): Loss_D: 0.2312 Loss_G: 1.2754\n",
            "===> Epoch[29](6/10): Loss_D: 0.2485 Loss_G: 1.2633\n",
            "===> Epoch[29](7/10): Loss_D: 0.2136 Loss_G: 1.4395\n",
            "===> Epoch[29](8/10): Loss_D: 0.2507 Loss_G: 1.3809\n",
            "===> Epoch[29](9/10): Loss_D: 0.2179 Loss_G: 1.3339\n",
            "===> Epoch[29](10/10): Loss_D: 0.2433 Loss_G: 1.2205\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3279 dB\n",
            "===> Epoch[30](1/10): Loss_D: 0.2379 Loss_G: 1.3662\n",
            "===> Epoch[30](2/10): Loss_D: 0.2531 Loss_G: 1.3096\n",
            "===> Epoch[30](3/10): Loss_D: 0.2221 Loss_G: 1.5657\n",
            "===> Epoch[30](4/10): Loss_D: 0.2706 Loss_G: 1.2993\n",
            "===> Epoch[30](5/10): Loss_D: 0.2692 Loss_G: 1.4409\n",
            "===> Epoch[30](6/10): Loss_D: 0.3338 Loss_G: 1.2277\n",
            "===> Epoch[30](7/10): Loss_D: 0.2518 Loss_G: 1.2986\n",
            "===> Epoch[30](8/10): Loss_D: 0.2787 Loss_G: 1.2642\n",
            "===> Epoch[30](9/10): Loss_D: 0.2541 Loss_G: 1.3147\n",
            "===> Epoch[30](10/10): Loss_D: 0.2534 Loss_G: 1.4446\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.0820 dB\n",
            "===> Epoch[31](1/10): Loss_D: 0.3116 Loss_G: 1.3270\n",
            "===> Epoch[31](2/10): Loss_D: 0.2806 Loss_G: 1.4425\n",
            "===> Epoch[31](3/10): Loss_D: 0.2887 Loss_G: 1.2889\n",
            "===> Epoch[31](4/10): Loss_D: 0.2432 Loss_G: 1.3679\n",
            "===> Epoch[31](5/10): Loss_D: 0.2276 Loss_G: 1.3270\n",
            "===> Epoch[31](6/10): Loss_D: 0.2397 Loss_G: 1.3238\n",
            "===> Epoch[31](7/10): Loss_D: 0.2395 Loss_G: 1.2643\n",
            "===> Epoch[31](8/10): Loss_D: 0.2576 Loss_G: 1.4677\n",
            "===> Epoch[31](9/10): Loss_D: 0.2201 Loss_G: 1.2887\n",
            "===> Epoch[31](10/10): Loss_D: 0.2164 Loss_G: 1.3258\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.0688 dB\n",
            "===> Epoch[32](1/10): Loss_D: 0.2705 Loss_G: 1.3755\n",
            "===> Epoch[32](2/10): Loss_D: 0.2056 Loss_G: 1.5178\n",
            "===> Epoch[32](3/10): Loss_D: 0.2675 Loss_G: 1.2082\n",
            "===> Epoch[32](4/10): Loss_D: 0.2192 Loss_G: 1.2927\n",
            "===> Epoch[32](5/10): Loss_D: 0.2460 Loss_G: 1.2231\n",
            "===> Epoch[32](6/10): Loss_D: 0.2086 Loss_G: 1.3229\n",
            "===> Epoch[32](7/10): Loss_D: 0.2335 Loss_G: 1.4140\n",
            "===> Epoch[32](8/10): Loss_D: 0.2655 Loss_G: 1.2162\n",
            "===> Epoch[32](9/10): Loss_D: 0.2626 Loss_G: 1.3789\n",
            "===> Epoch[32](10/10): Loss_D: 0.2919 Loss_G: 1.3118\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.4789 dB\n",
            "===> Epoch[33](1/10): Loss_D: 0.2771 Loss_G: 1.3684\n",
            "===> Epoch[33](2/10): Loss_D: 0.2909 Loss_G: 1.3369\n",
            "===> Epoch[33](3/10): Loss_D: 0.2451 Loss_G: 1.3641\n",
            "===> Epoch[33](4/10): Loss_D: 0.2520 Loss_G: 1.2800\n",
            "===> Epoch[33](5/10): Loss_D: 0.2824 Loss_G: 1.4074\n",
            "===> Epoch[33](6/10): Loss_D: 0.3172 Loss_G: 1.4779\n",
            "===> Epoch[33](7/10): Loss_D: 0.3171 Loss_G: 1.3707\n",
            "===> Epoch[33](8/10): Loss_D: 0.3012 Loss_G: 1.2805\n",
            "===> Epoch[33](9/10): Loss_D: 0.2795 Loss_G: 1.2600\n",
            "===> Epoch[33](10/10): Loss_D: 0.2691 Loss_G: 1.2325\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.4314 dB\n",
            "===> Epoch[34](1/10): Loss_D: 0.2194 Loss_G: 1.3822\n",
            "===> Epoch[34](2/10): Loss_D: 0.2246 Loss_G: 1.3434\n",
            "===> Epoch[34](3/10): Loss_D: 0.2306 Loss_G: 1.2176\n",
            "===> Epoch[34](4/10): Loss_D: 0.2072 Loss_G: 1.4765\n",
            "===> Epoch[34](5/10): Loss_D: 0.2597 Loss_G: 1.4154\n",
            "===> Epoch[34](6/10): Loss_D: 0.2413 Loss_G: 1.2366\n",
            "===> Epoch[34](7/10): Loss_D: 0.1886 Loss_G: 1.3450\n",
            "===> Epoch[34](8/10): Loss_D: 0.2617 Loss_G: 1.4187\n",
            "===> Epoch[34](9/10): Loss_D: 0.2360 Loss_G: 1.3664\n",
            "===> Epoch[34](10/10): Loss_D: 0.2208 Loss_G: 1.2721\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3674 dB\n",
            "===> Epoch[35](1/10): Loss_D: 0.2485 Loss_G: 1.6084\n",
            "===> Epoch[35](2/10): Loss_D: 0.3004 Loss_G: 1.3065\n",
            "===> Epoch[35](3/10): Loss_D: 0.2107 Loss_G: 1.3699\n",
            "===> Epoch[35](4/10): Loss_D: 0.2580 Loss_G: 1.3775\n",
            "===> Epoch[35](5/10): Loss_D: 0.2441 Loss_G: 1.3760\n",
            "===> Epoch[35](6/10): Loss_D: 0.2163 Loss_G: 1.3235\n",
            "===> Epoch[35](7/10): Loss_D: 0.2132 Loss_G: 1.2039\n",
            "===> Epoch[35](8/10): Loss_D: 0.2329 Loss_G: 1.1130\n",
            "===> Epoch[35](9/10): Loss_D: 0.2393 Loss_G: 1.3870\n",
            "===> Epoch[35](10/10): Loss_D: 0.2723 Loss_G: 1.4615\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.9621 dB\n",
            "===> Epoch[36](1/10): Loss_D: 0.2563 Loss_G: 1.3286\n",
            "===> Epoch[36](2/10): Loss_D: 0.2688 Loss_G: 1.2478\n",
            "===> Epoch[36](3/10): Loss_D: 0.2701 Loss_G: 1.1390\n",
            "===> Epoch[36](4/10): Loss_D: 0.2253 Loss_G: 1.5241\n",
            "===> Epoch[36](5/10): Loss_D: 0.3039 Loss_G: 1.4348\n",
            "===> Epoch[36](6/10): Loss_D: 0.2548 Loss_G: 1.2628\n",
            "===> Epoch[36](7/10): Loss_D: 0.1954 Loss_G: 1.2738\n",
            "===> Epoch[36](8/10): Loss_D: 0.2161 Loss_G: 1.4522\n",
            "===> Epoch[36](9/10): Loss_D: 0.2404 Loss_G: 1.4038\n",
            "===> Epoch[36](10/10): Loss_D: 0.2720 Loss_G: 1.5585\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.6568 dB\n",
            "===> Epoch[37](1/10): Loss_D: 0.3246 Loss_G: 1.4380\n",
            "===> Epoch[37](2/10): Loss_D: 0.2446 Loss_G: 1.1498\n",
            "===> Epoch[37](3/10): Loss_D: 0.2115 Loss_G: 1.5182\n",
            "===> Epoch[37](4/10): Loss_D: 0.3030 Loss_G: 1.3660\n",
            "===> Epoch[37](5/10): Loss_D: 0.3010 Loss_G: 1.4771\n",
            "===> Epoch[37](6/10): Loss_D: 0.3163 Loss_G: 1.4018\n",
            "===> Epoch[37](7/10): Loss_D: 0.2667 Loss_G: 1.3964\n",
            "===> Epoch[37](8/10): Loss_D: 0.2790 Loss_G: 1.2866\n",
            "===> Epoch[37](9/10): Loss_D: 0.2388 Loss_G: 1.2022\n",
            "===> Epoch[37](10/10): Loss_D: 0.2157 Loss_G: 1.1941\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 7.9161 dB\n",
            "===> Epoch[38](1/10): Loss_D: 0.2429 Loss_G: 1.5975\n",
            "===> Epoch[38](2/10): Loss_D: 0.3090 Loss_G: 1.3963\n",
            "===> Epoch[38](3/10): Loss_D: 0.3244 Loss_G: 1.2267\n",
            "===> Epoch[38](4/10): Loss_D: 0.2734 Loss_G: 1.1777\n",
            "===> Epoch[38](5/10): Loss_D: 0.2757 Loss_G: 1.2894\n",
            "===> Epoch[38](6/10): Loss_D: 0.2550 Loss_G: 1.2426\n",
            "===> Epoch[38](7/10): Loss_D: 0.2374 Loss_G: 1.2494\n",
            "===> Epoch[38](8/10): Loss_D: 0.2676 Loss_G: 1.2387\n",
            "===> Epoch[38](9/10): Loss_D: 0.2872 Loss_G: 1.3184\n",
            "===> Epoch[38](10/10): Loss_D: 0.2690 Loss_G: 1.2130\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.8890 dB\n",
            "===> Epoch[39](1/10): Loss_D: 0.2369 Loss_G: 1.2831\n",
            "===> Epoch[39](2/10): Loss_D: 0.2625 Loss_G: 1.2753\n",
            "===> Epoch[39](3/10): Loss_D: 0.2597 Loss_G: 1.5036\n",
            "===> Epoch[39](4/10): Loss_D: 0.3226 Loss_G: 1.3339\n",
            "===> Epoch[39](5/10): Loss_D: 0.3055 Loss_G: 1.2910\n",
            "===> Epoch[39](6/10): Loss_D: 0.3067 Loss_G: 1.2745\n",
            "===> Epoch[39](7/10): Loss_D: 0.2884 Loss_G: 1.3372\n",
            "===> Epoch[39](8/10): Loss_D: 0.2392 Loss_G: 1.3252\n",
            "===> Epoch[39](9/10): Loss_D: 0.2679 Loss_G: 1.2315\n",
            "===> Epoch[39](10/10): Loss_D: 0.2820 Loss_G: 1.0834\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.4523 dB\n",
            "===> Epoch[40](1/10): Loss_D: 0.2370 Loss_G: 1.1915\n",
            "===> Epoch[40](2/10): Loss_D: 0.2854 Loss_G: 1.3016\n",
            "===> Epoch[40](3/10): Loss_D: 0.3159 Loss_G: 1.3867\n",
            "===> Epoch[40](4/10): Loss_D: 0.3176 Loss_G: 1.4665\n",
            "===> Epoch[40](5/10): Loss_D: 0.3602 Loss_G: 1.3215\n",
            "===> Epoch[40](6/10): Loss_D: 0.3596 Loss_G: 1.3563\n",
            "===> Epoch[40](7/10): Loss_D: 0.3779 Loss_G: 1.2559\n",
            "===> Epoch[40](8/10): Loss_D: 0.2980 Loss_G: 1.1331\n",
            "===> Epoch[40](9/10): Loss_D: 0.2695 Loss_G: 1.4632\n",
            "===> Epoch[40](10/10): Loss_D: 0.3154 Loss_G: 1.3052\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.1995 dB\n",
            "===> Epoch[41](1/10): Loss_D: 0.2540 Loss_G: 1.1917\n",
            "===> Epoch[41](2/10): Loss_D: 0.2196 Loss_G: 1.2592\n",
            "===> Epoch[41](3/10): Loss_D: 0.2413 Loss_G: 1.2698\n",
            "===> Epoch[41](4/10): Loss_D: 0.2702 Loss_G: 1.3234\n",
            "===> Epoch[41](5/10): Loss_D: 0.2282 Loss_G: 1.2290\n",
            "===> Epoch[41](6/10): Loss_D: 0.2423 Loss_G: 1.0876\n",
            "===> Epoch[41](7/10): Loss_D: 0.2447 Loss_G: 1.3459\n",
            "===> Epoch[41](8/10): Loss_D: 0.2823 Loss_G: 1.3820\n",
            "===> Epoch[41](9/10): Loss_D: 0.2939 Loss_G: 1.1940\n",
            "===> Epoch[41](10/10): Loss_D: 0.2565 Loss_G: 1.0805\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.0385 dB\n",
            "===> Epoch[42](1/10): Loss_D: 0.2388 Loss_G: 1.1405\n",
            "===> Epoch[42](2/10): Loss_D: 0.2759 Loss_G: 1.4495\n",
            "===> Epoch[42](3/10): Loss_D: 0.2962 Loss_G: 1.1738\n",
            "===> Epoch[42](4/10): Loss_D: 0.2592 Loss_G: 1.1246\n",
            "===> Epoch[42](5/10): Loss_D: 0.2364 Loss_G: 1.2226\n",
            "===> Epoch[42](6/10): Loss_D: 0.2887 Loss_G: 1.3288\n",
            "===> Epoch[42](7/10): Loss_D: 0.2652 Loss_G: 1.3759\n",
            "===> Epoch[42](8/10): Loss_D: 0.2807 Loss_G: 1.1713\n",
            "===> Epoch[42](9/10): Loss_D: 0.2036 Loss_G: 1.1253\n",
            "===> Epoch[42](10/10): Loss_D: 0.2261 Loss_G: 1.3024\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.4276 dB\n",
            "===> Epoch[43](1/10): Loss_D: 0.2858 Loss_G: 1.1534\n",
            "===> Epoch[43](2/10): Loss_D: 0.2805 Loss_G: 1.2295\n",
            "===> Epoch[43](3/10): Loss_D: 0.2560 Loss_G: 1.0516\n",
            "===> Epoch[43](4/10): Loss_D: 0.1878 Loss_G: 1.2803\n",
            "===> Epoch[43](5/10): Loss_D: 0.2984 Loss_G: 1.1648\n",
            "===> Epoch[43](6/10): Loss_D: 0.2518 Loss_G: 1.2054\n",
            "===> Epoch[43](7/10): Loss_D: 0.2616 Loss_G: 1.2563\n",
            "===> Epoch[43](8/10): Loss_D: 0.2465 Loss_G: 1.0881\n",
            "===> Epoch[43](9/10): Loss_D: 0.2348 Loss_G: 1.1944\n",
            "===> Epoch[43](10/10): Loss_D: 0.2574 Loss_G: 1.4252\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.3237 dB\n",
            "===> Epoch[44](1/10): Loss_D: 0.2836 Loss_G: 1.1697\n",
            "===> Epoch[44](2/10): Loss_D: 0.2564 Loss_G: 1.1484\n",
            "===> Epoch[44](3/10): Loss_D: 0.2503 Loss_G: 1.1660\n",
            "===> Epoch[44](4/10): Loss_D: 0.2724 Loss_G: 1.2553\n",
            "===> Epoch[44](5/10): Loss_D: 0.2352 Loss_G: 1.2031\n",
            "===> Epoch[44](6/10): Loss_D: 0.2624 Loss_G: 1.1856\n",
            "===> Epoch[44](7/10): Loss_D: 0.2446 Loss_G: 1.2074\n",
            "===> Epoch[44](8/10): Loss_D: 0.2800 Loss_G: 1.2539\n",
            "===> Epoch[44](9/10): Loss_D: 0.2580 Loss_G: 1.1429\n",
            "===> Epoch[44](10/10): Loss_D: 0.2296 Loss_G: 1.0271\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.6953 dB\n",
            "===> Epoch[45](1/10): Loss_D: 0.2493 Loss_G: 1.1556\n",
            "===> Epoch[45](2/10): Loss_D: 0.2837 Loss_G: 1.2419\n",
            "===> Epoch[45](3/10): Loss_D: 0.2933 Loss_G: 1.1251\n",
            "===> Epoch[45](4/10): Loss_D: 0.2544 Loss_G: 1.0184\n",
            "===> Epoch[45](5/10): Loss_D: 0.2601 Loss_G: 1.1475\n",
            "===> Epoch[45](6/10): Loss_D: 0.2458 Loss_G: 1.2293\n",
            "===> Epoch[45](7/10): Loss_D: 0.2467 Loss_G: 1.0999\n",
            "===> Epoch[45](8/10): Loss_D: 0.3000 Loss_G: 1.1633\n",
            "===> Epoch[45](9/10): Loss_D: 0.2363 Loss_G: 1.0305\n",
            "===> Epoch[45](10/10): Loss_D: 0.2606 Loss_G: 1.3881\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 8.7953 dB\n",
            "===> Epoch[46](1/10): Loss_D: 0.3232 Loss_G: 1.2207\n",
            "===> Epoch[46](2/10): Loss_D: 0.3331 Loss_G: 1.1787\n",
            "===> Epoch[46](3/10): Loss_D: 0.3336 Loss_G: 1.2699\n",
            "===> Epoch[46](4/10): Loss_D: 0.3761 Loss_G: 1.4369\n",
            "===> Epoch[46](5/10): Loss_D: 0.4483 Loss_G: 1.4542\n",
            "===> Epoch[46](6/10): Loss_D: 0.3670 Loss_G: 1.2742\n",
            "===> Epoch[46](7/10): Loss_D: 0.3130 Loss_G: 1.1351\n",
            "===> Epoch[46](8/10): Loss_D: 0.2391 Loss_G: 1.1629\n",
            "===> Epoch[46](9/10): Loss_D: 0.2615 Loss_G: 1.0575\n",
            "===> Epoch[46](10/10): Loss_D: 0.2799 Loss_G: 1.1295\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.4320 dB\n",
            "===> Epoch[47](1/10): Loss_D: 0.2550 Loss_G: 1.1113\n",
            "===> Epoch[47](2/10): Loss_D: 0.2384 Loss_G: 0.9769\n",
            "===> Epoch[47](3/10): Loss_D: 0.2771 Loss_G: 1.1929\n",
            "===> Epoch[47](4/10): Loss_D: 0.2799 Loss_G: 1.2671\n",
            "===> Epoch[47](5/10): Loss_D: 0.3400 Loss_G: 1.1323\n",
            "===> Epoch[47](6/10): Loss_D: 0.2725 Loss_G: 1.2451\n",
            "===> Epoch[47](7/10): Loss_D: 0.3079 Loss_G: 1.2276\n",
            "===> Epoch[47](8/10): Loss_D: 0.2667 Loss_G: 1.2779\n",
            "===> Epoch[47](9/10): Loss_D: 0.2999 Loss_G: 1.0937\n",
            "===> Epoch[47](10/10): Loss_D: 0.2615 Loss_G: 1.0420\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.2072 dB\n",
            "===> Epoch[48](1/10): Loss_D: 0.2539 Loss_G: 1.2072\n",
            "===> Epoch[48](2/10): Loss_D: 0.2633 Loss_G: 1.0470\n",
            "===> Epoch[48](3/10): Loss_D: 0.2678 Loss_G: 1.0925\n",
            "===> Epoch[48](4/10): Loss_D: 0.3105 Loss_G: 1.0451\n",
            "===> Epoch[48](5/10): Loss_D: 0.2627 Loss_G: 1.1678\n",
            "===> Epoch[48](6/10): Loss_D: 0.2538 Loss_G: 1.0581\n",
            "===> Epoch[48](7/10): Loss_D: 0.2614 Loss_G: 1.1597\n",
            "===> Epoch[48](8/10): Loss_D: 0.2651 Loss_G: 1.1927\n",
            "===> Epoch[48](9/10): Loss_D: 0.2723 Loss_G: 0.9634\n",
            "===> Epoch[48](10/10): Loss_D: 0.2561 Loss_G: 1.0110\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.2506 dB\n",
            "===> Epoch[49](1/10): Loss_D: 0.2651 Loss_G: 1.1451\n",
            "===> Epoch[49](2/10): Loss_D: 0.2792 Loss_G: 1.0762\n",
            "===> Epoch[49](3/10): Loss_D: 0.2714 Loss_G: 0.9464\n",
            "===> Epoch[49](4/10): Loss_D: 0.2490 Loss_G: 1.0295\n",
            "===> Epoch[49](5/10): Loss_D: 0.2507 Loss_G: 1.1694\n",
            "===> Epoch[49](6/10): Loss_D: 0.2539 Loss_G: 1.1478\n",
            "===> Epoch[49](7/10): Loss_D: 0.2656 Loss_G: 1.0439\n",
            "===> Epoch[49](8/10): Loss_D: 0.2336 Loss_G: 1.1315\n",
            "===> Epoch[49](9/10): Loss_D: 0.3205 Loss_G: 1.2292\n",
            "===> Epoch[49](10/10): Loss_D: 0.3168 Loss_G: 1.2245\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 9.7272 dB\n",
            "===> Epoch[50](1/10): Loss_D: 0.3009 Loss_G: 1.0627\n",
            "===> Epoch[50](2/10): Loss_D: 0.2832 Loss_G: 1.2440\n",
            "===> Epoch[50](3/10): Loss_D: 0.3099 Loss_G: 1.0664\n",
            "===> Epoch[50](4/10): Loss_D: 0.2830 Loss_G: 1.1636\n",
            "===> Epoch[50](5/10): Loss_D: 0.2808 Loss_G: 1.1704\n",
            "===> Epoch[50](6/10): Loss_D: 0.2719 Loss_G: 1.0475\n",
            "===> Epoch[50](7/10): Loss_D: 0.2493 Loss_G: 0.9487\n",
            "===> Epoch[50](8/10): Loss_D: 0.2750 Loss_G: 0.9373\n",
            "===> Epoch[50](9/10): Loss_D: 0.2513 Loss_G: 0.9435\n",
            "===> Epoch[50](10/10): Loss_D: 0.2559 Loss_G: 1.1658\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.5255 dB\n",
            "Checkpoint saved to checkpoint./football\n",
            "===> Epoch[51](1/10): Loss_D: 0.2762 Loss_G: 1.0790\n",
            "===> Epoch[51](2/10): Loss_D: 0.2532 Loss_G: 1.0931\n",
            "===> Epoch[51](3/10): Loss_D: 0.2891 Loss_G: 0.9956\n",
            "===> Epoch[51](4/10): Loss_D: 0.2543 Loss_G: 1.0388\n",
            "===> Epoch[51](5/10): Loss_D: 0.2897 Loss_G: 1.1044\n",
            "===> Epoch[51](6/10): Loss_D: 0.2364 Loss_G: 1.1309\n",
            "===> Epoch[51](7/10): Loss_D: 0.2383 Loss_G: 1.0783\n",
            "===> Epoch[51](8/10): Loss_D: 0.2723 Loss_G: 1.0017\n",
            "===> Epoch[51](9/10): Loss_D: 0.2659 Loss_G: 1.1062\n",
            "===> Epoch[51](10/10): Loss_D: 0.2700 Loss_G: 1.0526\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.3260 dB\n",
            "===> Epoch[52](1/10): Loss_D: 0.2631 Loss_G: 1.1529\n",
            "===> Epoch[52](2/10): Loss_D: 0.3130 Loss_G: 1.0341\n",
            "===> Epoch[52](3/10): Loss_D: 0.3028 Loss_G: 1.1085\n",
            "===> Epoch[52](4/10): Loss_D: 0.3216 Loss_G: 0.9610\n",
            "===> Epoch[52](5/10): Loss_D: 0.2956 Loss_G: 1.0719\n",
            "===> Epoch[52](6/10): Loss_D: 0.2756 Loss_G: 1.0972\n",
            "===> Epoch[52](7/10): Loss_D: 0.2703 Loss_G: 1.0196\n",
            "===> Epoch[52](8/10): Loss_D: 0.2571 Loss_G: 1.1716\n",
            "===> Epoch[52](9/10): Loss_D: 0.2613 Loss_G: 1.0520\n",
            "===> Epoch[52](10/10): Loss_D: 0.2274 Loss_G: 0.9297\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.3960 dB\n",
            "===> Epoch[53](1/10): Loss_D: 0.2394 Loss_G: 1.0419\n",
            "===> Epoch[53](2/10): Loss_D: 0.2924 Loss_G: 1.0296\n",
            "===> Epoch[53](3/10): Loss_D: 0.2514 Loss_G: 1.0155\n",
            "===> Epoch[53](4/10): Loss_D: 0.2339 Loss_G: 1.0696\n",
            "===> Epoch[53](5/10): Loss_D: 0.2611 Loss_G: 1.0526\n",
            "===> Epoch[53](6/10): Loss_D: 0.2537 Loss_G: 0.9393\n",
            "===> Epoch[53](7/10): Loss_D: 0.2525 Loss_G: 1.0066\n",
            "===> Epoch[53](8/10): Loss_D: 0.2789 Loss_G: 1.0312\n",
            "===> Epoch[53](9/10): Loss_D: 0.2561 Loss_G: 0.9199\n",
            "===> Epoch[53](10/10): Loss_D: 0.2381 Loss_G: 0.9600\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.0534 dB\n",
            "===> Epoch[54](1/10): Loss_D: 0.2605 Loss_G: 0.8967\n",
            "===> Epoch[54](2/10): Loss_D: 0.2820 Loss_G: 1.0321\n",
            "===> Epoch[54](3/10): Loss_D: 0.2580 Loss_G: 1.0561\n",
            "===> Epoch[54](4/10): Loss_D: 0.2501 Loss_G: 0.9845\n",
            "===> Epoch[54](5/10): Loss_D: 0.2457 Loss_G: 1.0513\n",
            "===> Epoch[54](6/10): Loss_D: 0.2708 Loss_G: 1.0083\n",
            "===> Epoch[54](7/10): Loss_D: 0.2669 Loss_G: 0.9387\n",
            "===> Epoch[54](8/10): Loss_D: 0.2486 Loss_G: 1.0070\n",
            "===> Epoch[54](9/10): Loss_D: 0.2683 Loss_G: 1.0969\n",
            "===> Epoch[54](10/10): Loss_D: 0.2576 Loss_G: 1.0206\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.5597 dB\n",
            "===> Epoch[55](1/10): Loss_D: 0.2503 Loss_G: 0.9574\n",
            "===> Epoch[55](2/10): Loss_D: 0.2344 Loss_G: 1.0035\n",
            "===> Epoch[55](3/10): Loss_D: 0.2972 Loss_G: 0.9985\n",
            "===> Epoch[55](4/10): Loss_D: 0.2441 Loss_G: 1.0760\n",
            "===> Epoch[55](5/10): Loss_D: 0.2913 Loss_G: 0.9697\n",
            "===> Epoch[55](6/10): Loss_D: 0.2488 Loss_G: 1.0831\n",
            "===> Epoch[55](7/10): Loss_D: 0.2731 Loss_G: 1.0178\n",
            "===> Epoch[55](8/10): Loss_D: 0.2804 Loss_G: 0.9893\n",
            "===> Epoch[55](9/10): Loss_D: 0.2752 Loss_G: 0.9864\n",
            "===> Epoch[55](10/10): Loss_D: 0.2474 Loss_G: 1.0538\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.9410 dB\n",
            "===> Epoch[56](1/10): Loss_D: 0.2572 Loss_G: 1.0590\n",
            "===> Epoch[56](2/10): Loss_D: 0.2616 Loss_G: 1.1753\n",
            "===> Epoch[56](3/10): Loss_D: 0.3114 Loss_G: 1.0489\n",
            "===> Epoch[56](4/10): Loss_D: 0.2912 Loss_G: 1.0511\n",
            "===> Epoch[56](5/10): Loss_D: 0.3133 Loss_G: 0.9193\n",
            "===> Epoch[56](6/10): Loss_D: 0.2987 Loss_G: 1.0784\n",
            "===> Epoch[56](7/10): Loss_D: 0.2716 Loss_G: 1.0623\n",
            "===> Epoch[56](8/10): Loss_D: 0.2501 Loss_G: 0.9742\n",
            "===> Epoch[56](9/10): Loss_D: 0.2523 Loss_G: 0.9755\n",
            "===> Epoch[56](10/10): Loss_D: 0.2511 Loss_G: 0.9759\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.9821 dB\n",
            "===> Epoch[57](1/10): Loss_D: 0.2508 Loss_G: 1.0359\n",
            "===> Epoch[57](2/10): Loss_D: 0.2819 Loss_G: 0.9591\n",
            "===> Epoch[57](3/10): Loss_D: 0.2324 Loss_G: 1.0203\n",
            "===> Epoch[57](4/10): Loss_D: 0.2387 Loss_G: 1.0443\n",
            "===> Epoch[57](5/10): Loss_D: 0.2591 Loss_G: 1.0897\n",
            "===> Epoch[57](6/10): Loss_D: 0.2435 Loss_G: 1.0444\n",
            "===> Epoch[57](7/10): Loss_D: 0.2280 Loss_G: 1.0478\n",
            "===> Epoch[57](8/10): Loss_D: 0.2562 Loss_G: 0.9021\n",
            "===> Epoch[57](9/10): Loss_D: 0.2667 Loss_G: 1.0838\n",
            "===> Epoch[57](10/10): Loss_D: 0.2633 Loss_G: 1.0149\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.8262 dB\n",
            "===> Epoch[58](1/10): Loss_D: 0.2656 Loss_G: 0.8961\n",
            "===> Epoch[58](2/10): Loss_D: 0.2562 Loss_G: 1.0080\n",
            "===> Epoch[58](3/10): Loss_D: 0.2482 Loss_G: 1.0183\n",
            "===> Epoch[58](4/10): Loss_D: 0.2565 Loss_G: 0.9874\n",
            "===> Epoch[58](5/10): Loss_D: 0.2562 Loss_G: 1.0312\n",
            "===> Epoch[58](6/10): Loss_D: 0.2531 Loss_G: 1.1102\n",
            "===> Epoch[58](7/10): Loss_D: 0.2671 Loss_G: 0.9493\n",
            "===> Epoch[58](8/10): Loss_D: 0.2559 Loss_G: 0.9682\n",
            "===> Epoch[58](9/10): Loss_D: 0.2608 Loss_G: 0.9087\n",
            "===> Epoch[58](10/10): Loss_D: 0.2552 Loss_G: 1.1990\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.5588 dB\n",
            "===> Epoch[59](1/10): Loss_D: 0.2827 Loss_G: 0.9877\n",
            "===> Epoch[59](2/10): Loss_D: 0.2860 Loss_G: 1.1506\n",
            "===> Epoch[59](3/10): Loss_D: 0.3158 Loss_G: 0.8707\n",
            "===> Epoch[59](4/10): Loss_D: 0.3005 Loss_G: 1.0316\n",
            "===> Epoch[59](5/10): Loss_D: 0.3146 Loss_G: 1.0568\n",
            "===> Epoch[59](6/10): Loss_D: 0.3184 Loss_G: 1.2327\n",
            "===> Epoch[59](7/10): Loss_D: 0.3346 Loss_G: 0.9577\n",
            "===> Epoch[59](8/10): Loss_D: 0.2918 Loss_G: 0.9514\n",
            "===> Epoch[59](9/10): Loss_D: 0.2663 Loss_G: 1.1112\n",
            "===> Epoch[59](10/10): Loss_D: 0.2737 Loss_G: 1.0146\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.5390 dB\n",
            "===> Epoch[60](1/10): Loss_D: 0.2699 Loss_G: 0.9284\n",
            "===> Epoch[60](2/10): Loss_D: 0.2922 Loss_G: 0.8573\n",
            "===> Epoch[60](3/10): Loss_D: 0.2874 Loss_G: 1.0149\n",
            "===> Epoch[60](4/10): Loss_D: 0.2778 Loss_G: 0.9919\n",
            "===> Epoch[60](5/10): Loss_D: 0.2466 Loss_G: 0.9699\n",
            "===> Epoch[60](6/10): Loss_D: 0.2679 Loss_G: 0.8533\n",
            "===> Epoch[60](7/10): Loss_D: 0.2522 Loss_G: 0.9014\n",
            "===> Epoch[60](8/10): Loss_D: 0.2595 Loss_G: 1.0420\n",
            "===> Epoch[60](9/10): Loss_D: 0.2638 Loss_G: 1.0124\n",
            "===> Epoch[60](10/10): Loss_D: 0.2477 Loss_G: 1.1907\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.7556 dB\n",
            "===> Epoch[61](1/10): Loss_D: 0.2812 Loss_G: 0.8437\n",
            "===> Epoch[61](2/10): Loss_D: 0.2605 Loss_G: 0.9637\n",
            "===> Epoch[61](3/10): Loss_D: 0.2520 Loss_G: 1.0546\n",
            "===> Epoch[61](4/10): Loss_D: 0.2660 Loss_G: 0.9462\n",
            "===> Epoch[61](5/10): Loss_D: 0.2532 Loss_G: 0.9452\n",
            "===> Epoch[61](6/10): Loss_D: 0.2554 Loss_G: 0.9566\n",
            "===> Epoch[61](7/10): Loss_D: 0.2555 Loss_G: 0.8875\n",
            "===> Epoch[61](8/10): Loss_D: 0.2406 Loss_G: 1.0946\n",
            "===> Epoch[61](9/10): Loss_D: 0.3045 Loss_G: 0.9964\n",
            "===> Epoch[61](10/10): Loss_D: 0.3007 Loss_G: 1.1898\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 10.9884 dB\n",
            "===> Epoch[62](1/10): Loss_D: 0.3813 Loss_G: 0.9170\n",
            "===> Epoch[62](2/10): Loss_D: 0.3711 Loss_G: 1.1099\n",
            "===> Epoch[62](3/10): Loss_D: 0.3396 Loss_G: 1.0214\n",
            "===> Epoch[62](4/10): Loss_D: 0.3108 Loss_G: 0.9002\n",
            "===> Epoch[62](5/10): Loss_D: 0.2769 Loss_G: 0.9962\n",
            "===> Epoch[62](6/10): Loss_D: 0.2611 Loss_G: 0.9784\n",
            "===> Epoch[62](7/10): Loss_D: 0.2359 Loss_G: 1.1435\n",
            "===> Epoch[62](8/10): Loss_D: 0.2799 Loss_G: 0.8632\n",
            "===> Epoch[62](9/10): Loss_D: 0.2855 Loss_G: 1.0465\n",
            "===> Epoch[62](10/10): Loss_D: 0.3235 Loss_G: 0.9992\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.6918 dB\n",
            "===> Epoch[63](1/10): Loss_D: 0.3148 Loss_G: 0.9741\n",
            "===> Epoch[63](2/10): Loss_D: 0.3052 Loss_G: 0.9178\n",
            "===> Epoch[63](3/10): Loss_D: 0.2730 Loss_G: 1.0139\n",
            "===> Epoch[63](4/10): Loss_D: 0.2663 Loss_G: 1.0220\n",
            "===> Epoch[63](5/10): Loss_D: 0.2879 Loss_G: 0.9487\n",
            "===> Epoch[63](6/10): Loss_D: 0.2854 Loss_G: 1.0245\n",
            "===> Epoch[63](7/10): Loss_D: 0.2818 Loss_G: 0.9735\n",
            "===> Epoch[63](8/10): Loss_D: 0.2590 Loss_G: 0.9314\n",
            "===> Epoch[63](9/10): Loss_D: 0.2432 Loss_G: 0.8539\n",
            "===> Epoch[63](10/10): Loss_D: 0.2396 Loss_G: 0.9919\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.5057 dB\n",
            "===> Epoch[64](1/10): Loss_D: 0.2698 Loss_G: 0.9612\n",
            "===> Epoch[64](2/10): Loss_D: 0.2788 Loss_G: 0.8744\n",
            "===> Epoch[64](3/10): Loss_D: 0.2393 Loss_G: 0.9715\n",
            "===> Epoch[64](4/10): Loss_D: 0.2569 Loss_G: 0.9685\n",
            "===> Epoch[64](5/10): Loss_D: 0.2617 Loss_G: 0.9887\n",
            "===> Epoch[64](6/10): Loss_D: 0.2652 Loss_G: 0.9804\n",
            "===> Epoch[64](7/10): Loss_D: 0.2474 Loss_G: 0.7985\n",
            "===> Epoch[64](8/10): Loss_D: 0.2582 Loss_G: 0.8786\n",
            "===> Epoch[64](9/10): Loss_D: 0.2590 Loss_G: 0.9555\n",
            "===> Epoch[64](10/10): Loss_D: 0.2652 Loss_G: 0.9229\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.2119 dB\n",
            "===> Epoch[65](1/10): Loss_D: 0.2599 Loss_G: 0.8211\n",
            "===> Epoch[65](2/10): Loss_D: 0.2775 Loss_G: 0.8885\n",
            "===> Epoch[65](3/10): Loss_D: 0.2603 Loss_G: 0.9926\n",
            "===> Epoch[65](4/10): Loss_D: 0.2644 Loss_G: 0.8116\n",
            "===> Epoch[65](5/10): Loss_D: 0.2942 Loss_G: 1.1346\n",
            "===> Epoch[65](6/10): Loss_D: 0.3177 Loss_G: 1.0277\n",
            "===> Epoch[65](7/10): Loss_D: 0.3225 Loss_G: 1.0898\n",
            "===> Epoch[65](8/10): Loss_D: 0.3349 Loss_G: 0.8482\n",
            "===> Epoch[65](9/10): Loss_D: 0.3158 Loss_G: 0.9489\n",
            "===> Epoch[65](10/10): Loss_D: 0.2731 Loss_G: 1.0313\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.1397 dB\n",
            "===> Epoch[66](1/10): Loss_D: 0.2593 Loss_G: 1.0335\n",
            "===> Epoch[66](2/10): Loss_D: 0.3182 Loss_G: 0.9080\n",
            "===> Epoch[66](3/10): Loss_D: 0.3345 Loss_G: 0.9209\n",
            "===> Epoch[66](4/10): Loss_D: 0.2776 Loss_G: 1.0148\n",
            "===> Epoch[66](5/10): Loss_D: 0.2547 Loss_G: 0.8956\n",
            "===> Epoch[66](6/10): Loss_D: 0.2492 Loss_G: 0.9175\n",
            "===> Epoch[66](7/10): Loss_D: 0.2613 Loss_G: 0.9323\n",
            "===> Epoch[66](8/10): Loss_D: 0.2570 Loss_G: 0.9800\n",
            "===> Epoch[66](9/10): Loss_D: 0.2393 Loss_G: 0.9863\n",
            "===> Epoch[66](10/10): Loss_D: 0.2572 Loss_G: 0.8915\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.4551 dB\n",
            "===> Epoch[67](1/10): Loss_D: 0.2422 Loss_G: 1.0172\n",
            "===> Epoch[67](2/10): Loss_D: 0.2565 Loss_G: 0.8912\n",
            "===> Epoch[67](3/10): Loss_D: 0.2603 Loss_G: 0.8523\n",
            "===> Epoch[67](4/10): Loss_D: 0.2635 Loss_G: 1.0251\n",
            "===> Epoch[67](5/10): Loss_D: 0.3035 Loss_G: 0.8900\n",
            "===> Epoch[67](6/10): Loss_D: 0.3187 Loss_G: 0.7986\n",
            "===> Epoch[67](7/10): Loss_D: 0.2850 Loss_G: 0.9305\n",
            "===> Epoch[67](8/10): Loss_D: 0.2600 Loss_G: 0.8843\n",
            "===> Epoch[67](9/10): Loss_D: 0.2505 Loss_G: 1.0086\n",
            "===> Epoch[67](10/10): Loss_D: 0.2719 Loss_G: 0.8129\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.2076 dB\n",
            "===> Epoch[68](1/10): Loss_D: 0.2746 Loss_G: 1.0014\n",
            "===> Epoch[68](2/10): Loss_D: 0.2545 Loss_G: 1.0114\n",
            "===> Epoch[68](3/10): Loss_D: 0.3015 Loss_G: 0.7167\n",
            "===> Epoch[68](4/10): Loss_D: 0.2856 Loss_G: 0.8570\n",
            "===> Epoch[68](5/10): Loss_D: 0.2782 Loss_G: 0.9673\n",
            "===> Epoch[68](6/10): Loss_D: 0.2626 Loss_G: 0.8688\n",
            "===> Epoch[68](7/10): Loss_D: 0.2626 Loss_G: 0.8222\n",
            "===> Epoch[68](8/10): Loss_D: 0.2560 Loss_G: 0.9597\n",
            "===> Epoch[68](9/10): Loss_D: 0.2657 Loss_G: 0.8240\n",
            "===> Epoch[68](10/10): Loss_D: 0.2605 Loss_G: 1.0224\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.2019 dB\n",
            "===> Epoch[69](1/10): Loss_D: 0.2622 Loss_G: 0.9364\n",
            "===> Epoch[69](2/10): Loss_D: 0.2703 Loss_G: 0.8920\n",
            "===> Epoch[69](3/10): Loss_D: 0.2633 Loss_G: 0.8020\n",
            "===> Epoch[69](4/10): Loss_D: 0.2563 Loss_G: 0.8879\n",
            "===> Epoch[69](5/10): Loss_D: 0.2653 Loss_G: 0.8319\n",
            "===> Epoch[69](6/10): Loss_D: 0.2565 Loss_G: 1.1328\n",
            "===> Epoch[69](7/10): Loss_D: 0.2541 Loss_G: 0.8598\n",
            "===> Epoch[69](8/10): Loss_D: 0.2518 Loss_G: 0.8494\n",
            "===> Epoch[69](9/10): Loss_D: 0.2642 Loss_G: 0.9755\n",
            "===> Epoch[69](10/10): Loss_D: 0.2467 Loss_G: 0.8765\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.0567 dB\n",
            "===> Epoch[70](1/10): Loss_D: 0.2607 Loss_G: 0.9033\n",
            "===> Epoch[70](2/10): Loss_D: 0.2547 Loss_G: 0.8641\n",
            "===> Epoch[70](3/10): Loss_D: 0.2780 Loss_G: 0.8834\n",
            "===> Epoch[70](4/10): Loss_D: 0.2724 Loss_G: 0.8885\n",
            "===> Epoch[70](5/10): Loss_D: 0.2635 Loss_G: 0.8022\n",
            "===> Epoch[70](6/10): Loss_D: 0.2657 Loss_G: 0.8944\n",
            "===> Epoch[70](7/10): Loss_D: 0.2557 Loss_G: 0.9343\n",
            "===> Epoch[70](8/10): Loss_D: 0.2632 Loss_G: 0.9124\n",
            "===> Epoch[70](9/10): Loss_D: 0.2562 Loss_G: 0.9021\n",
            "===> Epoch[70](10/10): Loss_D: 0.2634 Loss_G: 0.8082\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.0717 dB\n",
            "===> Epoch[71](1/10): Loss_D: 0.2584 Loss_G: 0.8768\n",
            "===> Epoch[71](2/10): Loss_D: 0.2644 Loss_G: 0.8939\n",
            "===> Epoch[71](3/10): Loss_D: 0.2567 Loss_G: 0.9246\n",
            "===> Epoch[71](4/10): Loss_D: 0.2655 Loss_G: 0.7844\n",
            "===> Epoch[71](5/10): Loss_D: 0.2625 Loss_G: 1.0114\n",
            "===> Epoch[71](6/10): Loss_D: 0.2982 Loss_G: 0.7881\n",
            "===> Epoch[71](7/10): Loss_D: 0.3105 Loss_G: 1.0857\n",
            "===> Epoch[71](8/10): Loss_D: 0.3262 Loss_G: 0.9122\n",
            "===> Epoch[71](9/10): Loss_D: 0.2960 Loss_G: 0.9424\n",
            "===> Epoch[71](10/10): Loss_D: 0.2796 Loss_G: 0.8631\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.6133 dB\n",
            "===> Epoch[72](1/10): Loss_D: 0.2647 Loss_G: 0.8835\n",
            "===> Epoch[72](2/10): Loss_D: 0.2581 Loss_G: 0.8863\n",
            "===> Epoch[72](3/10): Loss_D: 0.2620 Loss_G: 0.9263\n",
            "===> Epoch[72](4/10): Loss_D: 0.2528 Loss_G: 0.9009\n",
            "===> Epoch[72](5/10): Loss_D: 0.2422 Loss_G: 0.8372\n",
            "===> Epoch[72](6/10): Loss_D: 0.2579 Loss_G: 0.9501\n",
            "===> Epoch[72](7/10): Loss_D: 0.2613 Loss_G: 0.8288\n",
            "===> Epoch[72](8/10): Loss_D: 0.2645 Loss_G: 0.8304\n",
            "===> Epoch[72](9/10): Loss_D: 0.2593 Loss_G: 0.8999\n",
            "===> Epoch[72](10/10): Loss_D: 0.2613 Loss_G: 0.8686\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.5256 dB\n",
            "===> Epoch[73](1/10): Loss_D: 0.2606 Loss_G: 0.7760\n",
            "===> Epoch[73](2/10): Loss_D: 0.2523 Loss_G: 0.8696\n",
            "===> Epoch[73](3/10): Loss_D: 0.2581 Loss_G: 0.8786\n",
            "===> Epoch[73](4/10): Loss_D: 0.2739 Loss_G: 0.7311\n",
            "===> Epoch[73](5/10): Loss_D: 0.2531 Loss_G: 0.9272\n",
            "===> Epoch[73](6/10): Loss_D: 0.2482 Loss_G: 0.9593\n",
            "===> Epoch[73](7/10): Loss_D: 0.2626 Loss_G: 0.8897\n",
            "===> Epoch[73](8/10): Loss_D: 0.2770 Loss_G: 0.8007\n",
            "===> Epoch[73](9/10): Loss_D: 0.2736 Loss_G: 0.9145\n",
            "===> Epoch[73](10/10): Loss_D: 0.2897 Loss_G: 0.7552\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.6441 dB\n",
            "===> Epoch[74](1/10): Loss_D: 0.2755 Loss_G: 0.7591\n",
            "===> Epoch[74](2/10): Loss_D: 0.2574 Loss_G: 0.9283\n",
            "===> Epoch[74](3/10): Loss_D: 0.2478 Loss_G: 0.8788\n",
            "===> Epoch[74](4/10): Loss_D: 0.2513 Loss_G: 0.9079\n",
            "===> Epoch[74](5/10): Loss_D: 0.2475 Loss_G: 0.8907\n",
            "===> Epoch[74](6/10): Loss_D: 0.2642 Loss_G: 0.8352\n",
            "===> Epoch[74](7/10): Loss_D: 0.2708 Loss_G: 0.8610\n",
            "===> Epoch[74](8/10): Loss_D: 0.2585 Loss_G: 0.9170\n",
            "===> Epoch[74](9/10): Loss_D: 0.2617 Loss_G: 0.7482\n",
            "===> Epoch[74](10/10): Loss_D: 0.2579 Loss_G: 0.8808\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.5911 dB\n",
            "===> Epoch[75](1/10): Loss_D: 0.2515 Loss_G: 1.0500\n",
            "===> Epoch[75](2/10): Loss_D: 0.2703 Loss_G: 0.7823\n",
            "===> Epoch[75](3/10): Loss_D: 0.2597 Loss_G: 0.8684\n",
            "===> Epoch[75](4/10): Loss_D: 0.2538 Loss_G: 0.9080\n",
            "===> Epoch[75](5/10): Loss_D: 0.2615 Loss_G: 0.7708\n",
            "===> Epoch[75](6/10): Loss_D: 0.2679 Loss_G: 0.8792\n",
            "===> Epoch[75](7/10): Loss_D: 0.2568 Loss_G: 0.9928\n",
            "===> Epoch[75](8/10): Loss_D: 0.2716 Loss_G: 0.7610\n",
            "===> Epoch[75](9/10): Loss_D: 0.2756 Loss_G: 0.9071\n",
            "===> Epoch[75](10/10): Loss_D: 0.2698 Loss_G: 0.8424\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.3098 dB\n",
            "===> Epoch[76](1/10): Loss_D: 0.2650 Loss_G: 0.9181\n",
            "===> Epoch[76](2/10): Loss_D: 0.2654 Loss_G: 0.8542\n",
            "===> Epoch[76](3/10): Loss_D: 0.2740 Loss_G: 0.9121\n",
            "===> Epoch[76](4/10): Loss_D: 0.2674 Loss_G: 0.8676\n",
            "===> Epoch[76](5/10): Loss_D: 0.2643 Loss_G: 0.8748\n",
            "===> Epoch[76](6/10): Loss_D: 0.2578 Loss_G: 0.8330\n",
            "===> Epoch[76](7/10): Loss_D: 0.2642 Loss_G: 0.9313\n",
            "===> Epoch[76](8/10): Loss_D: 0.2700 Loss_G: 0.8132\n",
            "===> Epoch[76](9/10): Loss_D: 0.2745 Loss_G: 0.7720\n",
            "===> Epoch[76](10/10): Loss_D: 0.2558 Loss_G: 0.8448\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.9345 dB\n",
            "===> Epoch[77](1/10): Loss_D: 0.2606 Loss_G: 0.8874\n",
            "===> Epoch[77](2/10): Loss_D: 0.2687 Loss_G: 0.8215\n",
            "===> Epoch[77](3/10): Loss_D: 0.2801 Loss_G: 0.9654\n",
            "===> Epoch[77](4/10): Loss_D: 0.3013 Loss_G: 0.8028\n",
            "===> Epoch[77](5/10): Loss_D: 0.2807 Loss_G: 0.9937\n",
            "===> Epoch[77](6/10): Loss_D: 0.2999 Loss_G: 0.8828\n",
            "===> Epoch[77](7/10): Loss_D: 0.3144 Loss_G: 0.9990\n",
            "===> Epoch[77](8/10): Loss_D: 0.3429 Loss_G: 0.8471\n",
            "===> Epoch[77](9/10): Loss_D: 0.2905 Loss_G: 0.8614\n",
            "===> Epoch[77](10/10): Loss_D: 0.2649 Loss_G: 0.9503\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.1790 dB\n",
            "===> Epoch[78](1/10): Loss_D: 0.2727 Loss_G: 0.8359\n",
            "===> Epoch[78](2/10): Loss_D: 0.2899 Loss_G: 0.8949\n",
            "===> Epoch[78](3/10): Loss_D: 0.3006 Loss_G: 0.8262\n",
            "===> Epoch[78](4/10): Loss_D: 0.2964 Loss_G: 0.8371\n",
            "===> Epoch[78](5/10): Loss_D: 0.2687 Loss_G: 0.8242\n",
            "===> Epoch[78](6/10): Loss_D: 0.2507 Loss_G: 0.9634\n",
            "===> Epoch[78](7/10): Loss_D: 0.2532 Loss_G: 0.8286\n",
            "===> Epoch[78](8/10): Loss_D: 0.2612 Loss_G: 0.8112\n",
            "===> Epoch[78](9/10): Loss_D: 0.2573 Loss_G: 0.9409\n",
            "===> Epoch[78](10/10): Loss_D: 0.2536 Loss_G: 0.8701\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.5240 dB\n",
            "===> Epoch[79](1/10): Loss_D: 0.2480 Loss_G: 0.9391\n",
            "===> Epoch[79](2/10): Loss_D: 0.2566 Loss_G: 0.8238\n",
            "===> Epoch[79](3/10): Loss_D: 0.2493 Loss_G: 0.8009\n",
            "===> Epoch[79](4/10): Loss_D: 0.2563 Loss_G: 0.8146\n",
            "===> Epoch[79](5/10): Loss_D: 0.2643 Loss_G: 0.7960\n",
            "===> Epoch[79](6/10): Loss_D: 0.2557 Loss_G: 0.8054\n",
            "===> Epoch[79](7/10): Loss_D: 0.2531 Loss_G: 0.7755\n",
            "===> Epoch[79](8/10): Loss_D: 0.2490 Loss_G: 0.8363\n",
            "===> Epoch[79](9/10): Loss_D: 0.2524 Loss_G: 0.8914\n",
            "===> Epoch[79](10/10): Loss_D: 0.2719 Loss_G: 0.8010\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.8200 dB\n",
            "===> Epoch[80](1/10): Loss_D: 0.2526 Loss_G: 0.8600\n",
            "===> Epoch[80](2/10): Loss_D: 0.2616 Loss_G: 0.8580\n",
            "===> Epoch[80](3/10): Loss_D: 0.2621 Loss_G: 0.8562\n",
            "===> Epoch[80](4/10): Loss_D: 0.2684 Loss_G: 0.8407\n",
            "===> Epoch[80](5/10): Loss_D: 0.2606 Loss_G: 0.8371\n",
            "===> Epoch[80](6/10): Loss_D: 0.2726 Loss_G: 0.8817\n",
            "===> Epoch[80](7/10): Loss_D: 0.2378 Loss_G: 1.0598\n",
            "===> Epoch[80](8/10): Loss_D: 0.2755 Loss_G: 0.7841\n",
            "===> Epoch[80](9/10): Loss_D: 0.2703 Loss_G: 0.9226\n",
            "===> Epoch[80](10/10): Loss_D: 0.2627 Loss_G: 0.7982\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.1998 dB\n",
            "===> Epoch[81](1/10): Loss_D: 0.2525 Loss_G: 0.8243\n",
            "===> Epoch[81](2/10): Loss_D: 0.2630 Loss_G: 0.7756\n",
            "===> Epoch[81](3/10): Loss_D: 0.2650 Loss_G: 0.8726\n",
            "===> Epoch[81](4/10): Loss_D: 0.2634 Loss_G: 0.8468\n",
            "===> Epoch[81](5/10): Loss_D: 0.2496 Loss_G: 0.8839\n",
            "===> Epoch[81](6/10): Loss_D: 0.2478 Loss_G: 0.7541\n",
            "===> Epoch[81](7/10): Loss_D: 0.2493 Loss_G: 0.8505\n",
            "===> Epoch[81](8/10): Loss_D: 0.2576 Loss_G: 0.8483\n",
            "===> Epoch[81](9/10): Loss_D: 0.2482 Loss_G: 0.9854\n",
            "===> Epoch[81](10/10): Loss_D: 0.2501 Loss_G: 0.8827\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.1361 dB\n",
            "===> Epoch[82](1/10): Loss_D: 0.2617 Loss_G: 0.8828\n",
            "===> Epoch[82](2/10): Loss_D: 0.2601 Loss_G: 0.8520\n",
            "===> Epoch[82](3/10): Loss_D: 0.2519 Loss_G: 0.8103\n",
            "===> Epoch[82](4/10): Loss_D: 0.2514 Loss_G: 0.8101\n",
            "===> Epoch[82](5/10): Loss_D: 0.2523 Loss_G: 0.8550\n",
            "===> Epoch[82](6/10): Loss_D: 0.2545 Loss_G: 0.8213\n",
            "===> Epoch[82](7/10): Loss_D: 0.2581 Loss_G: 0.7886\n",
            "===> Epoch[82](8/10): Loss_D: 0.2584 Loss_G: 0.8380\n",
            "===> Epoch[82](9/10): Loss_D: 0.2497 Loss_G: 0.9301\n",
            "===> Epoch[82](10/10): Loss_D: 0.2721 Loss_G: 0.8289\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.6257 dB\n",
            "===> Epoch[83](1/10): Loss_D: 0.2790 Loss_G: 0.8185\n",
            "===> Epoch[83](2/10): Loss_D: 0.2680 Loss_G: 1.0058\n",
            "===> Epoch[83](3/10): Loss_D: 0.2605 Loss_G: 0.9045\n",
            "===> Epoch[83](4/10): Loss_D: 0.2750 Loss_G: 0.7266\n",
            "===> Epoch[83](5/10): Loss_D: 0.2687 Loss_G: 0.8114\n",
            "===> Epoch[83](6/10): Loss_D: 0.2727 Loss_G: 0.8479\n",
            "===> Epoch[83](7/10): Loss_D: 0.2737 Loss_G: 0.7659\n",
            "===> Epoch[83](8/10): Loss_D: 0.2558 Loss_G: 0.8603\n",
            "===> Epoch[83](9/10): Loss_D: 0.2654 Loss_G: 0.8057\n",
            "===> Epoch[83](10/10): Loss_D: 0.2530 Loss_G: 0.9048\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 12.9642 dB\n",
            "===> Epoch[84](1/10): Loss_D: 0.2617 Loss_G: 0.8954\n",
            "===> Epoch[84](2/10): Loss_D: 0.2615 Loss_G: 0.9008\n",
            "===> Epoch[84](3/10): Loss_D: 0.2585 Loss_G: 0.8656\n",
            "===> Epoch[84](4/10): Loss_D: 0.2599 Loss_G: 0.8033\n",
            "===> Epoch[84](5/10): Loss_D: 0.2552 Loss_G: 0.7313\n",
            "===> Epoch[84](6/10): Loss_D: 0.2536 Loss_G: 0.8454\n",
            "===> Epoch[84](7/10): Loss_D: 0.2588 Loss_G: 0.7920\n",
            "===> Epoch[84](8/10): Loss_D: 0.2486 Loss_G: 0.8629\n",
            "===> Epoch[84](9/10): Loss_D: 0.2560 Loss_G: 0.8376\n",
            "===> Epoch[84](10/10): Loss_D: 0.2784 Loss_G: 0.7196\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.4459 dB\n",
            "===> Epoch[85](1/10): Loss_D: 0.2577 Loss_G: 0.7226\n",
            "===> Epoch[85](2/10): Loss_D: 0.2571 Loss_G: 0.8346\n",
            "===> Epoch[85](3/10): Loss_D: 0.2591 Loss_G: 0.8360\n",
            "===> Epoch[85](4/10): Loss_D: 0.2764 Loss_G: 0.7993\n",
            "===> Epoch[85](5/10): Loss_D: 0.2956 Loss_G: 1.0733\n",
            "===> Epoch[85](6/10): Loss_D: 0.3218 Loss_G: 0.8312\n",
            "===> Epoch[85](7/10): Loss_D: 0.3289 Loss_G: 0.9413\n",
            "===> Epoch[85](8/10): Loss_D: 0.3295 Loss_G: 0.8720\n",
            "===> Epoch[85](9/10): Loss_D: 0.3048 Loss_G: 0.7504\n",
            "===> Epoch[85](10/10): Loss_D: 0.2841 Loss_G: 0.8597\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.5523 dB\n",
            "===> Epoch[86](1/10): Loss_D: 0.3057 Loss_G: 0.8062\n",
            "===> Epoch[86](2/10): Loss_D: 0.2863 Loss_G: 0.7736\n",
            "===> Epoch[86](3/10): Loss_D: 0.2607 Loss_G: 0.8559\n",
            "===> Epoch[86](4/10): Loss_D: 0.2606 Loss_G: 0.7732\n",
            "===> Epoch[86](5/10): Loss_D: 0.2543 Loss_G: 0.9061\n",
            "===> Epoch[86](6/10): Loss_D: 0.2527 Loss_G: 0.8680\n",
            "===> Epoch[86](7/10): Loss_D: 0.2585 Loss_G: 0.8814\n",
            "===> Epoch[86](8/10): Loss_D: 0.2779 Loss_G: 0.8293\n",
            "===> Epoch[86](9/10): Loss_D: 0.2811 Loss_G: 0.7850\n",
            "===> Epoch[86](10/10): Loss_D: 0.2615 Loss_G: 0.7749\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.6736 dB\n",
            "===> Epoch[87](1/10): Loss_D: 0.2591 Loss_G: 0.9250\n",
            "===> Epoch[87](2/10): Loss_D: 0.2484 Loss_G: 0.9031\n",
            "===> Epoch[87](3/10): Loss_D: 0.2621 Loss_G: 0.6963\n",
            "===> Epoch[87](4/10): Loss_D: 0.2508 Loss_G: 0.8132\n",
            "===> Epoch[87](5/10): Loss_D: 0.2509 Loss_G: 0.8358\n",
            "===> Epoch[87](6/10): Loss_D: 0.2564 Loss_G: 0.8195\n",
            "===> Epoch[87](7/10): Loss_D: 0.2526 Loss_G: 0.7601\n",
            "===> Epoch[87](8/10): Loss_D: 0.2599 Loss_G: 0.7985\n",
            "===> Epoch[87](9/10): Loss_D: 0.2573 Loss_G: 0.7653\n",
            "===> Epoch[87](10/10): Loss_D: 0.2639 Loss_G: 0.7077\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.1902 dB\n",
            "===> Epoch[88](1/10): Loss_D: 0.2458 Loss_G: 0.7943\n",
            "===> Epoch[88](2/10): Loss_D: 0.2564 Loss_G: 0.8409\n",
            "===> Epoch[88](3/10): Loss_D: 0.2438 Loss_G: 0.8570\n",
            "===> Epoch[88](4/10): Loss_D: 0.2571 Loss_G: 0.8357\n",
            "===> Epoch[88](5/10): Loss_D: 0.2551 Loss_G: 0.8372\n",
            "===> Epoch[88](6/10): Loss_D: 0.2452 Loss_G: 0.7825\n",
            "===> Epoch[88](7/10): Loss_D: 0.2536 Loss_G: 0.9559\n",
            "===> Epoch[88](8/10): Loss_D: 0.2731 Loss_G: 0.8806\n",
            "===> Epoch[88](9/10): Loss_D: 0.2733 Loss_G: 0.9840\n",
            "===> Epoch[88](10/10): Loss_D: 0.2685 Loss_G: 0.8051\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.5594 dB\n",
            "===> Epoch[89](1/10): Loss_D: 0.2697 Loss_G: 0.9122\n",
            "===> Epoch[89](2/10): Loss_D: 0.2719 Loss_G: 0.7661\n",
            "===> Epoch[89](3/10): Loss_D: 0.2582 Loss_G: 0.9126\n",
            "===> Epoch[89](4/10): Loss_D: 0.2566 Loss_G: 0.8285\n",
            "===> Epoch[89](5/10): Loss_D: 0.2553 Loss_G: 0.8137\n",
            "===> Epoch[89](6/10): Loss_D: 0.2531 Loss_G: 0.7334\n",
            "===> Epoch[89](7/10): Loss_D: 0.2504 Loss_G: 0.8105\n",
            "===> Epoch[89](8/10): Loss_D: 0.2475 Loss_G: 0.8301\n",
            "===> Epoch[89](9/10): Loss_D: 0.2488 Loss_G: 0.8357\n",
            "===> Epoch[89](10/10): Loss_D: 0.2531 Loss_G: 0.8584\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.1380 dB\n",
            "===> Epoch[90](1/10): Loss_D: 0.2463 Loss_G: 0.8295\n",
            "===> Epoch[90](2/10): Loss_D: 0.2506 Loss_G: 0.7527\n",
            "===> Epoch[90](3/10): Loss_D: 0.2525 Loss_G: 0.8538\n",
            "===> Epoch[90](4/10): Loss_D: 0.2473 Loss_G: 0.8986\n",
            "===> Epoch[90](5/10): Loss_D: 0.2689 Loss_G: 0.7516\n",
            "===> Epoch[90](6/10): Loss_D: 0.2613 Loss_G: 0.8402\n",
            "===> Epoch[90](7/10): Loss_D: 0.2562 Loss_G: 0.8926\n",
            "===> Epoch[90](8/10): Loss_D: 0.2534 Loss_G: 0.8228\n",
            "===> Epoch[90](9/10): Loss_D: 0.2634 Loss_G: 0.8140\n",
            "===> Epoch[90](10/10): Loss_D: 0.2563 Loss_G: 0.8008\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.8203 dB\n",
            "===> Epoch[91](1/10): Loss_D: 0.2593 Loss_G: 0.8662\n",
            "===> Epoch[91](2/10): Loss_D: 0.2530 Loss_G: 0.8061\n",
            "===> Epoch[91](3/10): Loss_D: 0.2590 Loss_G: 0.8843\n",
            "===> Epoch[91](4/10): Loss_D: 0.2608 Loss_G: 0.9108\n",
            "===> Epoch[91](5/10): Loss_D: 0.2748 Loss_G: 0.6408\n",
            "===> Epoch[91](6/10): Loss_D: 0.2645 Loss_G: 0.9278\n",
            "===> Epoch[91](7/10): Loss_D: 0.2809 Loss_G: 0.8663\n",
            "===> Epoch[91](8/10): Loss_D: 0.2850 Loss_G: 0.8295\n",
            "===> Epoch[91](9/10): Loss_D: 0.2859 Loss_G: 0.8067\n",
            "===> Epoch[91](10/10): Loss_D: 0.2959 Loss_G: 1.0306\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.5308 dB\n",
            "===> Epoch[92](1/10): Loss_D: 0.3173 Loss_G: 0.8308\n",
            "===> Epoch[92](2/10): Loss_D: 0.3117 Loss_G: 0.8984\n",
            "===> Epoch[92](3/10): Loss_D: 0.3094 Loss_G: 0.8400\n",
            "===> Epoch[92](4/10): Loss_D: 0.2942 Loss_G: 0.8004\n",
            "===> Epoch[92](5/10): Loss_D: 0.2719 Loss_G: 0.7530\n",
            "===> Epoch[92](6/10): Loss_D: 0.2614 Loss_G: 0.8304\n",
            "===> Epoch[92](7/10): Loss_D: 0.2699 Loss_G: 0.7554\n",
            "===> Epoch[92](8/10): Loss_D: 0.2618 Loss_G: 0.8093\n",
            "===> Epoch[92](9/10): Loss_D: 0.2557 Loss_G: 0.8372\n",
            "===> Epoch[92](10/10): Loss_D: 0.2585 Loss_G: 0.8383\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.2938 dB\n",
            "===> Epoch[93](1/10): Loss_D: 0.2548 Loss_G: 0.7969\n",
            "===> Epoch[93](2/10): Loss_D: 0.2550 Loss_G: 0.7261\n",
            "===> Epoch[93](3/10): Loss_D: 0.2529 Loss_G: 0.8216\n",
            "===> Epoch[93](4/10): Loss_D: 0.2597 Loss_G: 0.7471\n",
            "===> Epoch[93](5/10): Loss_D: 0.2552 Loss_G: 0.7607\n",
            "===> Epoch[93](6/10): Loss_D: 0.2579 Loss_G: 0.7819\n",
            "===> Epoch[93](7/10): Loss_D: 0.2502 Loss_G: 0.9220\n",
            "===> Epoch[93](8/10): Loss_D: 0.2630 Loss_G: 0.7813\n",
            "===> Epoch[93](9/10): Loss_D: 0.2624 Loss_G: 0.8438\n",
            "===> Epoch[93](10/10): Loss_D: 0.2698 Loss_G: 0.8331\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.2197 dB\n",
            "===> Epoch[94](1/10): Loss_D: 0.2611 Loss_G: 0.8037\n",
            "===> Epoch[94](2/10): Loss_D: 0.2628 Loss_G: 0.8226\n",
            "===> Epoch[94](3/10): Loss_D: 0.2522 Loss_G: 0.8123\n",
            "===> Epoch[94](4/10): Loss_D: 0.2565 Loss_G: 0.7658\n",
            "===> Epoch[94](5/10): Loss_D: 0.2645 Loss_G: 0.7709\n",
            "===> Epoch[94](6/10): Loss_D: 0.2628 Loss_G: 0.8774\n",
            "===> Epoch[94](7/10): Loss_D: 0.2858 Loss_G: 0.7169\n",
            "===> Epoch[94](8/10): Loss_D: 0.2830 Loss_G: 0.9331\n",
            "===> Epoch[94](9/10): Loss_D: 0.3072 Loss_G: 0.7951\n",
            "===> Epoch[94](10/10): Loss_D: 0.3061 Loss_G: 0.8586\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.2820 dB\n",
            "===> Epoch[95](1/10): Loss_D: 0.3020 Loss_G: 0.8054\n",
            "===> Epoch[95](2/10): Loss_D: 0.2888 Loss_G: 0.8094\n",
            "===> Epoch[95](3/10): Loss_D: 0.2769 Loss_G: 0.7053\n",
            "===> Epoch[95](4/10): Loss_D: 0.2608 Loss_G: 0.8722\n",
            "===> Epoch[95](5/10): Loss_D: 0.2760 Loss_G: 0.7193\n",
            "===> Epoch[95](6/10): Loss_D: 0.2709 Loss_G: 0.9286\n",
            "===> Epoch[95](7/10): Loss_D: 0.2675 Loss_G: 0.8823\n",
            "===> Epoch[95](8/10): Loss_D: 0.2580 Loss_G: 0.8135\n",
            "===> Epoch[95](9/10): Loss_D: 0.2640 Loss_G: 0.7009\n",
            "===> Epoch[95](10/10): Loss_D: 0.2733 Loss_G: 0.8480\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 14.7715 dB\n",
            "===> Epoch[96](1/10): Loss_D: 0.2913 Loss_G: 0.8434\n",
            "===> Epoch[96](2/10): Loss_D: 0.2852 Loss_G: 0.8278\n",
            "===> Epoch[96](3/10): Loss_D: 0.2930 Loss_G: 0.8108\n",
            "===> Epoch[96](4/10): Loss_D: 0.2822 Loss_G: 0.9145\n",
            "===> Epoch[96](5/10): Loss_D: 0.2903 Loss_G: 0.7047\n",
            "===> Epoch[96](6/10): Loss_D: 0.2784 Loss_G: 0.7915\n",
            "===> Epoch[96](7/10): Loss_D: 0.2686 Loss_G: 0.8167\n",
            "===> Epoch[96](8/10): Loss_D: 0.2644 Loss_G: 0.8509\n",
            "===> Epoch[96](9/10): Loss_D: 0.2634 Loss_G: 0.7697\n",
            "===> Epoch[96](10/10): Loss_D: 0.2536 Loss_G: 0.8198\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.6072 dB\n",
            "===> Epoch[97](1/10): Loss_D: 0.2719 Loss_G: 0.7395\n",
            "===> Epoch[97](2/10): Loss_D: 0.2617 Loss_G: 0.8913\n",
            "===> Epoch[97](3/10): Loss_D: 0.2569 Loss_G: 0.8051\n",
            "===> Epoch[97](4/10): Loss_D: 0.2658 Loss_G: 0.6825\n",
            "===> Epoch[97](5/10): Loss_D: 0.2628 Loss_G: 0.7942\n",
            "===> Epoch[97](6/10): Loss_D: 0.2483 Loss_G: 0.8233\n",
            "===> Epoch[97](7/10): Loss_D: 0.2550 Loss_G: 0.7642\n",
            "===> Epoch[97](8/10): Loss_D: 0.2489 Loss_G: 0.7555\n",
            "===> Epoch[97](9/10): Loss_D: 0.2554 Loss_G: 0.7793\n",
            "===> Epoch[97](10/10): Loss_D: 0.2504 Loss_G: 0.7826\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.1852 dB\n",
            "===> Epoch[98](1/10): Loss_D: 0.2540 Loss_G: 0.7719\n",
            "===> Epoch[98](2/10): Loss_D: 0.2536 Loss_G: 0.7209\n",
            "===> Epoch[98](3/10): Loss_D: 0.2605 Loss_G: 0.7588\n",
            "===> Epoch[98](4/10): Loss_D: 0.2510 Loss_G: 0.8294\n",
            "===> Epoch[98](5/10): Loss_D: 0.2558 Loss_G: 0.8343\n",
            "===> Epoch[98](6/10): Loss_D: 0.2524 Loss_G: 0.7735\n",
            "===> Epoch[98](7/10): Loss_D: 0.2594 Loss_G: 0.9119\n",
            "===> Epoch[98](8/10): Loss_D: 0.2618 Loss_G: 0.7767\n",
            "===> Epoch[98](9/10): Loss_D: 0.2537 Loss_G: 0.7106\n",
            "===> Epoch[98](10/10): Loss_D: 0.2445 Loss_G: 0.8176\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 13.0763 dB\n",
            "===> Epoch[99](1/10): Loss_D: 0.2552 Loss_G: 0.8371\n",
            "===> Epoch[99](2/10): Loss_D: 0.2724 Loss_G: 0.7031\n",
            "===> Epoch[99](3/10): Loss_D: 0.2528 Loss_G: 0.6915\n",
            "===> Epoch[99](4/10): Loss_D: 0.2577 Loss_G: 0.6817\n",
            "===> Epoch[99](5/10): Loss_D: 0.2587 Loss_G: 0.7783\n",
            "===> Epoch[99](6/10): Loss_D: 0.2614 Loss_G: 0.9276\n",
            "===> Epoch[99](7/10): Loss_D: 0.2677 Loss_G: 0.7584\n",
            "===> Epoch[99](8/10): Loss_D: 0.2517 Loss_G: 0.8488\n",
            "===> Epoch[99](9/10): Loss_D: 0.2598 Loss_G: 0.8055\n",
            "===> Epoch[99](10/10): Loss_D: 0.2628 Loss_G: 0.7484\n",
            "learning rate = 0.0002000\n",
            "learning rate = 0.0002000\n",
            "===> Avg. PSNR: 11.8345 dB\n",
            "===> Epoch[100](1/10): Loss_D: 0.2611 Loss_G: 0.7952\n",
            "===> Epoch[100](2/10): Loss_D: 0.2550 Loss_G: 0.7785\n",
            "===> Epoch[100](3/10): Loss_D: 0.2538 Loss_G: 0.7310\n",
            "===> Epoch[100](4/10): Loss_D: 0.2474 Loss_G: 0.8250\n",
            "===> Epoch[100](5/10): Loss_D: 0.2564 Loss_G: 0.6984\n",
            "===> Epoch[100](6/10): Loss_D: 0.2440 Loss_G: 0.7814\n",
            "===> Epoch[100](7/10): Loss_D: 0.2594 Loss_G: 0.7559\n",
            "===> Epoch[100](8/10): Loss_D: 0.2566 Loss_G: 0.7629\n",
            "===> Epoch[100](9/10): Loss_D: 0.2565 Loss_G: 0.9462\n",
            "===> Epoch[100](10/10): Loss_D: 0.2604 Loss_G: 0.7375\n",
            "learning rate = 0.0001980\n",
            "learning rate = 0.0001980\n",
            "===> Avg. PSNR: 12.7302 dB\n",
            "Checkpoint saved to checkpoint./football\n",
            "===> Epoch[101](1/10): Loss_D: 0.2537 Loss_G: 0.6691\n",
            "===> Epoch[101](2/10): Loss_D: 0.2600 Loss_G: 0.7731\n",
            "===> Epoch[101](3/10): Loss_D: 0.2655 Loss_G: 0.7495\n",
            "===> Epoch[101](4/10): Loss_D: 0.2612 Loss_G: 0.7588\n",
            "===> Epoch[101](5/10): Loss_D: 0.2564 Loss_G: 0.7367\n",
            "===> Epoch[101](6/10): Loss_D: 0.2539 Loss_G: 0.8135\n",
            "===> Epoch[101](7/10): Loss_D: 0.2530 Loss_G: 0.8101\n",
            "===> Epoch[101](8/10): Loss_D: 0.2551 Loss_G: 0.6869\n",
            "===> Epoch[101](9/10): Loss_D: 0.2510 Loss_G: 0.8180\n",
            "===> Epoch[101](10/10): Loss_D: 0.2518 Loss_G: 0.6810\n",
            "learning rate = 0.0001960\n",
            "learning rate = 0.0001960\n",
            "===> Avg. PSNR: 13.5709 dB\n",
            "===> Epoch[102](1/10): Loss_D: 0.2712 Loss_G: 0.8786\n",
            "===> Epoch[102](2/10): Loss_D: 0.2805 Loss_G: 0.7144\n",
            "===> Epoch[102](3/10): Loss_D: 0.2854 Loss_G: 0.8791\n",
            "===> Epoch[102](4/10): Loss_D: 0.2924 Loss_G: 0.6646\n",
            "===> Epoch[102](5/10): Loss_D: 0.2752 Loss_G: 0.8131\n",
            "===> Epoch[102](6/10): Loss_D: 0.2749 Loss_G: 0.7516\n",
            "===> Epoch[102](7/10): Loss_D: 0.2764 Loss_G: 0.7209\n",
            "===> Epoch[102](8/10): Loss_D: 0.2583 Loss_G: 0.7665\n",
            "===> Epoch[102](9/10): Loss_D: 0.2475 Loss_G: 0.7933\n",
            "===> Epoch[102](10/10): Loss_D: 0.2522 Loss_G: 0.8246\n",
            "learning rate = 0.0001941\n",
            "learning rate = 0.0001941\n",
            "===> Avg. PSNR: 12.6840 dB\n",
            "===> Epoch[103](1/10): Loss_D: 0.2587 Loss_G: 0.7262\n",
            "===> Epoch[103](2/10): Loss_D: 0.2713 Loss_G: 0.7787\n",
            "===> Epoch[103](3/10): Loss_D: 0.2813 Loss_G: 0.7121\n",
            "===> Epoch[103](4/10): Loss_D: 0.2818 Loss_G: 0.9092\n",
            "===> Epoch[103](5/10): Loss_D: 0.2855 Loss_G: 0.7582\n",
            "===> Epoch[103](6/10): Loss_D: 0.2719 Loss_G: 0.7587\n",
            "===> Epoch[103](7/10): Loss_D: 0.2591 Loss_G: 0.8149\n",
            "===> Epoch[103](8/10): Loss_D: 0.2600 Loss_G: 0.7398\n",
            "===> Epoch[103](9/10): Loss_D: 0.2610 Loss_G: 0.8217\n",
            "===> Epoch[103](10/10): Loss_D: 0.2681 Loss_G: 0.8286\n",
            "learning rate = 0.0001921\n",
            "learning rate = 0.0001921\n",
            "===> Avg. PSNR: 13.5360 dB\n",
            "===> Epoch[104](1/10): Loss_D: 0.2687 Loss_G: 0.7178\n",
            "===> Epoch[104](2/10): Loss_D: 0.2530 Loss_G: 0.7175\n",
            "===> Epoch[104](3/10): Loss_D: 0.2563 Loss_G: 0.7688\n",
            "===> Epoch[104](4/10): Loss_D: 0.2607 Loss_G: 0.8474\n",
            "===> Epoch[104](5/10): Loss_D: 0.2674 Loss_G: 0.7542\n",
            "===> Epoch[104](6/10): Loss_D: 0.2595 Loss_G: 0.7837\n",
            "===> Epoch[104](7/10): Loss_D: 0.2613 Loss_G: 0.7853\n",
            "===> Epoch[104](8/10): Loss_D: 0.2568 Loss_G: 0.7584\n",
            "===> Epoch[104](9/10): Loss_D: 0.2640 Loss_G: 0.8120\n",
            "===> Epoch[104](10/10): Loss_D: 0.2590 Loss_G: 0.7252\n",
            "learning rate = 0.0001901\n",
            "learning rate = 0.0001901\n",
            "===> Avg. PSNR: 14.2564 dB\n",
            "===> Epoch[105](1/10): Loss_D: 0.2586 Loss_G: 0.8027\n",
            "===> Epoch[105](2/10): Loss_D: 0.2606 Loss_G: 0.7202\n",
            "===> Epoch[105](3/10): Loss_D: 0.2607 Loss_G: 0.7221\n",
            "===> Epoch[105](4/10): Loss_D: 0.2515 Loss_G: 0.7540\n",
            "===> Epoch[105](5/10): Loss_D: 0.2472 Loss_G: 0.8796\n",
            "===> Epoch[105](6/10): Loss_D: 0.2539 Loss_G: 0.8071\n",
            "===> Epoch[105](7/10): Loss_D: 0.2531 Loss_G: 0.7799\n",
            "===> Epoch[105](8/10): Loss_D: 0.2518 Loss_G: 0.8354\n",
            "===> Epoch[105](9/10): Loss_D: 0.2617 Loss_G: 0.6640\n",
            "===> Epoch[105](10/10): Loss_D: 0.2636 Loss_G: 0.7635\n",
            "learning rate = 0.0001881\n",
            "learning rate = 0.0001881\n",
            "===> Avg. PSNR: 13.9802 dB\n",
            "===> Epoch[106](1/10): Loss_D: 0.2568 Loss_G: 0.7537\n",
            "===> Epoch[106](2/10): Loss_D: 0.2518 Loss_G: 0.7724\n",
            "===> Epoch[106](3/10): Loss_D: 0.2541 Loss_G: 0.7060\n",
            "===> Epoch[106](4/10): Loss_D: 0.2512 Loss_G: 0.7521\n",
            "===> Epoch[106](5/10): Loss_D: 0.2520 Loss_G: 0.7706\n",
            "===> Epoch[106](6/10): Loss_D: 0.2519 Loss_G: 0.7769\n",
            "===> Epoch[106](7/10): Loss_D: 0.2480 Loss_G: 0.7104\n",
            "===> Epoch[106](8/10): Loss_D: 0.2555 Loss_G: 0.7754\n",
            "===> Epoch[106](9/10): Loss_D: 0.2531 Loss_G: 0.7225\n",
            "===> Epoch[106](10/10): Loss_D: 0.2485 Loss_G: 0.9204\n",
            "learning rate = 0.0001861\n",
            "learning rate = 0.0001861\n",
            "===> Avg. PSNR: 14.9285 dB\n",
            "===> Epoch[107](1/10): Loss_D: 0.2537 Loss_G: 0.7587\n",
            "===> Epoch[107](2/10): Loss_D: 0.2506 Loss_G: 0.7170\n",
            "===> Epoch[107](3/10): Loss_D: 0.2493 Loss_G: 0.8377\n",
            "===> Epoch[107](4/10): Loss_D: 0.2621 Loss_G: 0.8505\n",
            "===> Epoch[107](5/10): Loss_D: 0.2570 Loss_G: 0.7323\n",
            "===> Epoch[107](6/10): Loss_D: 0.2554 Loss_G: 0.7193\n",
            "===> Epoch[107](7/10): Loss_D: 0.2545 Loss_G: 0.7028\n",
            "===> Epoch[107](8/10): Loss_D: 0.2512 Loss_G: 0.7381\n",
            "===> Epoch[107](9/10): Loss_D: 0.2544 Loss_G: 0.7086\n",
            "===> Epoch[107](10/10): Loss_D: 0.2453 Loss_G: 0.7839\n",
            "learning rate = 0.0001842\n",
            "learning rate = 0.0001842\n",
            "===> Avg. PSNR: 13.3146 dB\n",
            "===> Epoch[108](1/10): Loss_D: 0.2551 Loss_G: 0.7283\n",
            "===> Epoch[108](2/10): Loss_D: 0.2647 Loss_G: 0.7744\n",
            "===> Epoch[108](3/10): Loss_D: 0.2794 Loss_G: 0.6602\n",
            "===> Epoch[108](4/10): Loss_D: 0.2698 Loss_G: 0.7982\n",
            "===> Epoch[108](5/10): Loss_D: 0.2579 Loss_G: 0.8749\n",
            "===> Epoch[108](6/10): Loss_D: 0.2633 Loss_G: 0.7491\n",
            "===> Epoch[108](7/10): Loss_D: 0.2562 Loss_G: 0.6505\n",
            "===> Epoch[108](8/10): Loss_D: 0.2565 Loss_G: 0.7114\n",
            "===> Epoch[108](9/10): Loss_D: 0.2517 Loss_G: 0.7167\n",
            "===> Epoch[108](10/10): Loss_D: 0.2568 Loss_G: 0.7526\n",
            "learning rate = 0.0001822\n",
            "learning rate = 0.0001822\n",
            "===> Avg. PSNR: 14.6140 dB\n",
            "===> Epoch[109](1/10): Loss_D: 0.2529 Loss_G: 0.7513\n",
            "===> Epoch[109](2/10): Loss_D: 0.2531 Loss_G: 0.6539\n",
            "===> Epoch[109](3/10): Loss_D: 0.2477 Loss_G: 0.8050\n",
            "===> Epoch[109](4/10): Loss_D: 0.2615 Loss_G: 0.6991\n",
            "===> Epoch[109](5/10): Loss_D: 0.2609 Loss_G: 0.7930\n",
            "===> Epoch[109](6/10): Loss_D: 0.2554 Loss_G: 0.7580\n",
            "===> Epoch[109](7/10): Loss_D: 0.2528 Loss_G: 0.7148\n",
            "===> Epoch[109](8/10): Loss_D: 0.2490 Loss_G: 0.6781\n",
            "===> Epoch[109](9/10): Loss_D: 0.2564 Loss_G: 0.7242\n",
            "===> Epoch[109](10/10): Loss_D: 0.2554 Loss_G: 0.7265\n",
            "learning rate = 0.0001802\n",
            "learning rate = 0.0001802\n",
            "===> Avg. PSNR: 13.9390 dB\n",
            "===> Epoch[110](1/10): Loss_D: 0.2527 Loss_G: 0.7038\n",
            "===> Epoch[110](2/10): Loss_D: 0.2667 Loss_G: 0.6734\n",
            "===> Epoch[110](3/10): Loss_D: 0.2578 Loss_G: 0.6969\n",
            "===> Epoch[110](4/10): Loss_D: 0.2516 Loss_G: 0.7553\n",
            "===> Epoch[110](5/10): Loss_D: 0.2498 Loss_G: 0.8420\n",
            "===> Epoch[110](6/10): Loss_D: 0.2529 Loss_G: 0.6453\n",
            "===> Epoch[110](7/10): Loss_D: 0.2577 Loss_G: 0.6851\n",
            "===> Epoch[110](8/10): Loss_D: 0.2537 Loss_G: 0.8513\n",
            "===> Epoch[110](9/10): Loss_D: 0.2540 Loss_G: 0.7013\n",
            "===> Epoch[110](10/10): Loss_D: 0.2523 Loss_G: 0.7509\n",
            "learning rate = 0.0001782\n",
            "learning rate = 0.0001782\n",
            "===> Avg. PSNR: 13.7325 dB\n",
            "===> Epoch[111](1/10): Loss_D: 0.2524 Loss_G: 0.8154\n",
            "===> Epoch[111](2/10): Loss_D: 0.2536 Loss_G: 0.6933\n",
            "===> Epoch[111](3/10): Loss_D: 0.2541 Loss_G: 0.7675\n",
            "===> Epoch[111](4/10): Loss_D: 0.2485 Loss_G: 0.7000\n",
            "===> Epoch[111](5/10): Loss_D: 0.2608 Loss_G: 0.7399\n",
            "===> Epoch[111](6/10): Loss_D: 0.2565 Loss_G: 0.7215\n",
            "===> Epoch[111](7/10): Loss_D: 0.2461 Loss_G: 0.7524\n",
            "===> Epoch[111](8/10): Loss_D: 0.2585 Loss_G: 0.7523\n",
            "===> Epoch[111](9/10): Loss_D: 0.2520 Loss_G: 0.7414\n",
            "===> Epoch[111](10/10): Loss_D: 0.2585 Loss_G: 0.7367\n",
            "learning rate = 0.0001762\n",
            "learning rate = 0.0001762\n",
            "===> Avg. PSNR: 14.4144 dB\n",
            "===> Epoch[112](1/10): Loss_D: 0.2502 Loss_G: 0.6509\n",
            "===> Epoch[112](2/10): Loss_D: 0.2585 Loss_G: 0.7224\n",
            "===> Epoch[112](3/10): Loss_D: 0.2617 Loss_G: 0.7075\n",
            "===> Epoch[112](4/10): Loss_D: 0.2502 Loss_G: 0.6931\n",
            "===> Epoch[112](5/10): Loss_D: 0.2510 Loss_G: 0.6974\n",
            "===> Epoch[112](6/10): Loss_D: 0.2551 Loss_G: 0.7455\n",
            "===> Epoch[112](7/10): Loss_D: 0.2557 Loss_G: 0.7839\n",
            "===> Epoch[112](8/10): Loss_D: 0.2560 Loss_G: 0.7206\n",
            "===> Epoch[112](9/10): Loss_D: 0.2521 Loss_G: 0.7716\n",
            "===> Epoch[112](10/10): Loss_D: 0.2649 Loss_G: 0.6460\n",
            "learning rate = 0.0001743\n",
            "learning rate = 0.0001743\n",
            "===> Avg. PSNR: 15.2075 dB\n",
            "===> Epoch[113](1/10): Loss_D: 0.2595 Loss_G: 0.7274\n",
            "===> Epoch[113](2/10): Loss_D: 0.2577 Loss_G: 0.6838\n",
            "===> Epoch[113](3/10): Loss_D: 0.2552 Loss_G: 0.7001\n",
            "===> Epoch[113](4/10): Loss_D: 0.2538 Loss_G: 0.6759\n",
            "===> Epoch[113](5/10): Loss_D: 0.2556 Loss_G: 0.8149\n",
            "===> Epoch[113](6/10): Loss_D: 0.2585 Loss_G: 0.7209\n",
            "===> Epoch[113](7/10): Loss_D: 0.2529 Loss_G: 0.6728\n",
            "===> Epoch[113](8/10): Loss_D: 0.2515 Loss_G: 0.6654\n",
            "===> Epoch[113](9/10): Loss_D: 0.2478 Loss_G: 0.7835\n",
            "===> Epoch[113](10/10): Loss_D: 0.2564 Loss_G: 0.7633\n",
            "learning rate = 0.0001723\n",
            "learning rate = 0.0001723\n",
            "===> Avg. PSNR: 14.7130 dB\n",
            "===> Epoch[114](1/10): Loss_D: 0.2580 Loss_G: 0.7707\n",
            "===> Epoch[114](2/10): Loss_D: 0.2569 Loss_G: 0.5802\n",
            "===> Epoch[114](3/10): Loss_D: 0.2556 Loss_G: 0.7239\n",
            "===> Epoch[114](4/10): Loss_D: 0.2505 Loss_G: 0.7629\n",
            "===> Epoch[114](5/10): Loss_D: 0.2560 Loss_G: 0.8237\n",
            "===> Epoch[114](6/10): Loss_D: 0.2534 Loss_G: 0.6669\n",
            "===> Epoch[114](7/10): Loss_D: 0.2615 Loss_G: 0.8015\n",
            "===> Epoch[114](8/10): Loss_D: 0.2597 Loss_G: 0.7531\n",
            "===> Epoch[114](9/10): Loss_D: 0.2627 Loss_G: 0.6743\n",
            "===> Epoch[114](10/10): Loss_D: 0.2531 Loss_G: 0.7608\n",
            "learning rate = 0.0001703\n",
            "learning rate = 0.0001703\n",
            "===> Avg. PSNR: 15.1695 dB\n",
            "===> Epoch[115](1/10): Loss_D: 0.2465 Loss_G: 0.7771\n",
            "===> Epoch[115](2/10): Loss_D: 0.2533 Loss_G: 0.7928\n",
            "===> Epoch[115](3/10): Loss_D: 0.2537 Loss_G: 0.7707\n",
            "===> Epoch[115](4/10): Loss_D: 0.2553 Loss_G: 0.8061\n",
            "===> Epoch[115](5/10): Loss_D: 0.2559 Loss_G: 0.7260\n",
            "===> Epoch[115](6/10): Loss_D: 0.2613 Loss_G: 0.6573\n",
            "===> Epoch[115](7/10): Loss_D: 0.2630 Loss_G: 0.7733\n",
            "===> Epoch[115](8/10): Loss_D: 0.2566 Loss_G: 0.6750\n",
            "===> Epoch[115](9/10): Loss_D: 0.2518 Loss_G: 0.6647\n",
            "===> Epoch[115](10/10): Loss_D: 0.2506 Loss_G: 0.7542\n",
            "learning rate = 0.0001683\n",
            "learning rate = 0.0001683\n",
            "===> Avg. PSNR: 16.1248 dB\n",
            "===> Epoch[116](1/10): Loss_D: 0.2521 Loss_G: 0.6078\n",
            "===> Epoch[116](2/10): Loss_D: 0.2576 Loss_G: 0.7635\n",
            "===> Epoch[116](3/10): Loss_D: 0.2543 Loss_G: 0.7461\n",
            "===> Epoch[116](4/10): Loss_D: 0.2492 Loss_G: 0.7791\n",
            "===> Epoch[116](5/10): Loss_D: 0.2519 Loss_G: 0.6533\n",
            "===> Epoch[116](6/10): Loss_D: 0.2652 Loss_G: 0.7434\n",
            "===> Epoch[116](7/10): Loss_D: 0.2605 Loss_G: 0.6889\n",
            "===> Epoch[116](8/10): Loss_D: 0.2562 Loss_G: 0.7471\n",
            "===> Epoch[116](9/10): Loss_D: 0.2580 Loss_G: 0.7290\n",
            "===> Epoch[116](10/10): Loss_D: 0.2491 Loss_G: 0.6700\n",
            "learning rate = 0.0001663\n",
            "learning rate = 0.0001663\n",
            "===> Avg. PSNR: 14.3827 dB\n",
            "===> Epoch[117](1/10): Loss_D: 0.2515 Loss_G: 0.7084\n",
            "===> Epoch[117](2/10): Loss_D: 0.2509 Loss_G: 0.6901\n",
            "===> Epoch[117](3/10): Loss_D: 0.2550 Loss_G: 0.8278\n",
            "===> Epoch[117](4/10): Loss_D: 0.2530 Loss_G: 0.6867\n",
            "===> Epoch[117](5/10): Loss_D: 0.2458 Loss_G: 0.6877\n",
            "===> Epoch[117](6/10): Loss_D: 0.2491 Loss_G: 0.9024\n",
            "===> Epoch[117](7/10): Loss_D: 0.2629 Loss_G: 0.7375\n",
            "===> Epoch[117](8/10): Loss_D: 0.2575 Loss_G: 0.6712\n",
            "===> Epoch[117](9/10): Loss_D: 0.2519 Loss_G: 0.6982\n",
            "===> Epoch[117](10/10): Loss_D: 0.2501 Loss_G: 0.6797\n",
            "learning rate = 0.0001644\n",
            "learning rate = 0.0001644\n",
            "===> Avg. PSNR: 14.4217 dB\n",
            "===> Epoch[118](1/10): Loss_D: 0.2581 Loss_G: 0.6690\n",
            "===> Epoch[118](2/10): Loss_D: 0.2578 Loss_G: 0.6553\n",
            "===> Epoch[118](3/10): Loss_D: 0.2514 Loss_G: 0.6680\n",
            "===> Epoch[118](4/10): Loss_D: 0.2490 Loss_G: 0.7407\n",
            "===> Epoch[118](5/10): Loss_D: 0.2592 Loss_G: 0.6796\n",
            "===> Epoch[118](6/10): Loss_D: 0.2639 Loss_G: 0.7038\n",
            "===> Epoch[118](7/10): Loss_D: 0.2517 Loss_G: 0.7621\n",
            "===> Epoch[118](8/10): Loss_D: 0.2486 Loss_G: 0.7746\n",
            "===> Epoch[118](9/10): Loss_D: 0.2495 Loss_G: 0.8554\n",
            "===> Epoch[118](10/10): Loss_D: 0.2599 Loss_G: 0.6587\n",
            "learning rate = 0.0001624\n",
            "learning rate = 0.0001624\n",
            "===> Avg. PSNR: 13.8818 dB\n",
            "===> Epoch[119](1/10): Loss_D: 0.2520 Loss_G: 0.6782\n",
            "===> Epoch[119](2/10): Loss_D: 0.2518 Loss_G: 0.6748\n",
            "===> Epoch[119](3/10): Loss_D: 0.2535 Loss_G: 0.7610\n",
            "===> Epoch[119](4/10): Loss_D: 0.2519 Loss_G: 0.7407\n",
            "===> Epoch[119](5/10): Loss_D: 0.2515 Loss_G: 0.7103\n",
            "===> Epoch[119](6/10): Loss_D: 0.2563 Loss_G: 0.6664\n",
            "===> Epoch[119](7/10): Loss_D: 0.2538 Loss_G: 0.8064\n",
            "===> Epoch[119](8/10): Loss_D: 0.2469 Loss_G: 0.6924\n",
            "===> Epoch[119](9/10): Loss_D: 0.2494 Loss_G: 0.7023\n",
            "===> Epoch[119](10/10): Loss_D: 0.2504 Loss_G: 0.6041\n",
            "learning rate = 0.0001604\n",
            "learning rate = 0.0001604\n",
            "===> Avg. PSNR: 14.6577 dB\n",
            "===> Epoch[120](1/10): Loss_D: 0.2572 Loss_G: 0.7118\n",
            "===> Epoch[120](2/10): Loss_D: 0.2549 Loss_G: 0.6702\n",
            "===> Epoch[120](3/10): Loss_D: 0.2540 Loss_G: 0.6471\n",
            "===> Epoch[120](4/10): Loss_D: 0.2504 Loss_G: 0.6592\n",
            "===> Epoch[120](5/10): Loss_D: 0.2441 Loss_G: 0.6903\n",
            "===> Epoch[120](6/10): Loss_D: 0.2504 Loss_G: 0.7463\n",
            "===> Epoch[120](7/10): Loss_D: 0.2525 Loss_G: 0.7388\n",
            "===> Epoch[120](8/10): Loss_D: 0.2570 Loss_G: 0.7206\n",
            "===> Epoch[120](9/10): Loss_D: 0.2501 Loss_G: 0.7138\n",
            "===> Epoch[120](10/10): Loss_D: 0.2427 Loss_G: 0.7300\n",
            "learning rate = 0.0001584\n",
            "learning rate = 0.0001584\n",
            "===> Avg. PSNR: 14.1146 dB\n",
            "===> Epoch[121](1/10): Loss_D: 0.2526 Loss_G: 0.7374\n",
            "===> Epoch[121](2/10): Loss_D: 0.2519 Loss_G: 0.7169\n",
            "===> Epoch[121](3/10): Loss_D: 0.2480 Loss_G: 0.7144\n",
            "===> Epoch[121](4/10): Loss_D: 0.2585 Loss_G: 0.6787\n",
            "===> Epoch[121](5/10): Loss_D: 0.2526 Loss_G: 0.7410\n",
            "===> Epoch[121](6/10): Loss_D: 0.2507 Loss_G: 0.7610\n",
            "===> Epoch[121](7/10): Loss_D: 0.2528 Loss_G: 0.7089\n",
            "===> Epoch[121](8/10): Loss_D: 0.2520 Loss_G: 0.6853\n",
            "===> Epoch[121](9/10): Loss_D: 0.2597 Loss_G: 0.6123\n",
            "===> Epoch[121](10/10): Loss_D: 0.2618 Loss_G: 0.6992\n",
            "learning rate = 0.0001564\n",
            "learning rate = 0.0001564\n",
            "===> Avg. PSNR: 13.6867 dB\n",
            "===> Epoch[122](1/10): Loss_D: 0.2520 Loss_G: 0.6702\n",
            "===> Epoch[122](2/10): Loss_D: 0.2394 Loss_G: 0.8002\n",
            "===> Epoch[122](3/10): Loss_D: 0.2523 Loss_G: 0.6431\n",
            "===> Epoch[122](4/10): Loss_D: 0.2526 Loss_G: 0.7107\n",
            "===> Epoch[122](5/10): Loss_D: 0.2575 Loss_G: 0.7298\n",
            "===> Epoch[122](6/10): Loss_D: 0.2533 Loss_G: 0.7152\n",
            "===> Epoch[122](7/10): Loss_D: 0.2518 Loss_G: 0.6524\n",
            "===> Epoch[122](8/10): Loss_D: 0.2536 Loss_G: 0.7207\n",
            "===> Epoch[122](9/10): Loss_D: 0.2560 Loss_G: 0.6025\n",
            "===> Epoch[122](10/10): Loss_D: 0.2532 Loss_G: 0.6963\n",
            "learning rate = 0.0001545\n",
            "learning rate = 0.0001545\n",
            "===> Avg. PSNR: 14.9388 dB\n",
            "===> Epoch[123](1/10): Loss_D: 0.2538 Loss_G: 0.7308\n",
            "===> Epoch[123](2/10): Loss_D: 0.2519 Loss_G: 0.6998\n",
            "===> Epoch[123](3/10): Loss_D: 0.2554 Loss_G: 0.6192\n",
            "===> Epoch[123](4/10): Loss_D: 0.2576 Loss_G: 0.6559\n",
            "===> Epoch[123](5/10): Loss_D: 0.2591 Loss_G: 0.6941\n",
            "===> Epoch[123](6/10): Loss_D: 0.2573 Loss_G: 0.7803\n",
            "===> Epoch[123](7/10): Loss_D: 0.2642 Loss_G: 0.6457\n",
            "===> Epoch[123](8/10): Loss_D: 0.2583 Loss_G: 0.6865\n",
            "===> Epoch[123](9/10): Loss_D: 0.2510 Loss_G: 0.7428\n",
            "===> Epoch[123](10/10): Loss_D: 0.2460 Loss_G: 0.7434\n",
            "learning rate = 0.0001525\n",
            "learning rate = 0.0001525\n",
            "===> Avg. PSNR: 13.5285 dB\n",
            "===> Epoch[124](1/10): Loss_D: 0.2529 Loss_G: 0.6660\n",
            "===> Epoch[124](2/10): Loss_D: 0.2524 Loss_G: 0.7486\n",
            "===> Epoch[124](3/10): Loss_D: 0.2582 Loss_G: 0.6006\n",
            "===> Epoch[124](4/10): Loss_D: 0.2621 Loss_G: 0.6270\n",
            "===> Epoch[124](5/10): Loss_D: 0.2609 Loss_G: 0.6534\n",
            "===> Epoch[124](6/10): Loss_D: 0.2494 Loss_G: 0.7708\n",
            "===> Epoch[124](7/10): Loss_D: 0.2544 Loss_G: 0.6921\n",
            "===> Epoch[124](8/10): Loss_D: 0.2573 Loss_G: 0.6616\n",
            "===> Epoch[124](9/10): Loss_D: 0.2551 Loss_G: 0.6875\n",
            "===> Epoch[124](10/10): Loss_D: 0.2563 Loss_G: 0.6857\n",
            "learning rate = 0.0001505\n",
            "learning rate = 0.0001505\n",
            "===> Avg. PSNR: 14.7252 dB\n",
            "===> Epoch[125](1/10): Loss_D: 0.2525 Loss_G: 0.6795\n",
            "===> Epoch[125](2/10): Loss_D: 0.2524 Loss_G: 0.6251\n",
            "===> Epoch[125](3/10): Loss_D: 0.2580 Loss_G: 0.7189\n",
            "===> Epoch[125](4/10): Loss_D: 0.2577 Loss_G: 0.6623\n",
            "===> Epoch[125](5/10): Loss_D: 0.2543 Loss_G: 0.6746\n",
            "===> Epoch[125](6/10): Loss_D: 0.2434 Loss_G: 0.8214\n",
            "===> Epoch[125](7/10): Loss_D: 0.2536 Loss_G: 0.6322\n",
            "===> Epoch[125](8/10): Loss_D: 0.2537 Loss_G: 0.7716\n",
            "===> Epoch[125](9/10): Loss_D: 0.2564 Loss_G: 0.5938\n",
            "===> Epoch[125](10/10): Loss_D: 0.2547 Loss_G: 0.6669\n",
            "learning rate = 0.0001485\n",
            "learning rate = 0.0001485\n",
            "===> Avg. PSNR: 14.5693 dB\n",
            "===> Epoch[126](1/10): Loss_D: 0.2532 Loss_G: 0.7896\n",
            "===> Epoch[126](2/10): Loss_D: 0.2501 Loss_G: 0.6727\n",
            "===> Epoch[126](3/10): Loss_D: 0.2571 Loss_G: 0.5478\n",
            "===> Epoch[126](4/10): Loss_D: 0.2468 Loss_G: 0.6564\n",
            "===> Epoch[126](5/10): Loss_D: 0.2476 Loss_G: 0.7048\n",
            "===> Epoch[126](6/10): Loss_D: 0.2516 Loss_G: 0.6443\n",
            "===> Epoch[126](7/10): Loss_D: 0.2533 Loss_G: 0.7184\n",
            "===> Epoch[126](8/10): Loss_D: 0.2543 Loss_G: 0.7198\n",
            "===> Epoch[126](9/10): Loss_D: 0.2517 Loss_G: 0.6435\n",
            "===> Epoch[126](10/10): Loss_D: 0.2492 Loss_G: 0.7769\n",
            "learning rate = 0.0001465\n",
            "learning rate = 0.0001465\n",
            "===> Avg. PSNR: 14.9070 dB\n",
            "===> Epoch[127](1/10): Loss_D: 0.2505 Loss_G: 0.7056\n",
            "===> Epoch[127](2/10): Loss_D: 0.2546 Loss_G: 0.6060\n",
            "===> Epoch[127](3/10): Loss_D: 0.2540 Loss_G: 0.7066\n",
            "===> Epoch[127](4/10): Loss_D: 0.2538 Loss_G: 0.7146\n",
            "===> Epoch[127](5/10): Loss_D: 0.2542 Loss_G: 0.6959\n",
            "===> Epoch[127](6/10): Loss_D: 0.2509 Loss_G: 0.6593\n",
            "===> Epoch[127](7/10): Loss_D: 0.2543 Loss_G: 0.7230\n",
            "===> Epoch[127](8/10): Loss_D: 0.2526 Loss_G: 0.7103\n",
            "===> Epoch[127](9/10): Loss_D: 0.2514 Loss_G: 0.7074\n",
            "===> Epoch[127](10/10): Loss_D: 0.2523 Loss_G: 0.6095\n",
            "learning rate = 0.0001446\n",
            "learning rate = 0.0001446\n",
            "===> Avg. PSNR: 14.3935 dB\n",
            "===> Epoch[128](1/10): Loss_D: 0.2554 Loss_G: 0.6576\n",
            "===> Epoch[128](2/10): Loss_D: 0.2482 Loss_G: 0.6752\n",
            "===> Epoch[128](3/10): Loss_D: 0.2507 Loss_G: 0.7450\n",
            "===> Epoch[128](4/10): Loss_D: 0.2540 Loss_G: 0.6762\n",
            "===> Epoch[128](5/10): Loss_D: 0.2509 Loss_G: 0.6204\n",
            "===> Epoch[128](6/10): Loss_D: 0.2566 Loss_G: 0.6982\n",
            "===> Epoch[128](7/10): Loss_D: 0.2507 Loss_G: 0.7410\n",
            "===> Epoch[128](8/10): Loss_D: 0.2557 Loss_G: 0.6025\n",
            "===> Epoch[128](9/10): Loss_D: 0.2554 Loss_G: 0.6193\n",
            "===> Epoch[128](10/10): Loss_D: 0.2493 Loss_G: 0.8113\n",
            "learning rate = 0.0001426\n",
            "learning rate = 0.0001426\n",
            "===> Avg. PSNR: 14.0357 dB\n",
            "===> Epoch[129](1/10): Loss_D: 0.2511 Loss_G: 0.6723\n",
            "===> Epoch[129](2/10): Loss_D: 0.2476 Loss_G: 0.6746\n",
            "===> Epoch[129](3/10): Loss_D: 0.2522 Loss_G: 0.6529\n",
            "===> Epoch[129](4/10): Loss_D: 0.2466 Loss_G: 0.6866\n",
            "===> Epoch[129](5/10): Loss_D: 0.2516 Loss_G: 0.7242\n",
            "===> Epoch[129](6/10): Loss_D: 0.2508 Loss_G: 0.6748\n",
            "===> Epoch[129](7/10): Loss_D: 0.2468 Loss_G: 0.6184\n",
            "===> Epoch[129](8/10): Loss_D: 0.2510 Loss_G: 0.6442\n",
            "===> Epoch[129](9/10): Loss_D: 0.2512 Loss_G: 0.7257\n",
            "===> Epoch[129](10/10): Loss_D: 0.2545 Loss_G: 0.6590\n",
            "learning rate = 0.0001406\n",
            "learning rate = 0.0001406\n",
            "===> Avg. PSNR: 15.4661 dB\n",
            "===> Epoch[130](1/10): Loss_D: 0.2513 Loss_G: 0.6189\n",
            "===> Epoch[130](2/10): Loss_D: 0.2552 Loss_G: 0.7127\n",
            "===> Epoch[130](3/10): Loss_D: 0.2630 Loss_G: 0.5881\n",
            "===> Epoch[130](4/10): Loss_D: 0.2466 Loss_G: 0.6987\n",
            "===> Epoch[130](5/10): Loss_D: 0.2519 Loss_G: 0.7625\n",
            "===> Epoch[130](6/10): Loss_D: 0.2497 Loss_G: 0.7035\n",
            "===> Epoch[130](7/10): Loss_D: 0.2505 Loss_G: 0.7407\n",
            "===> Epoch[130](8/10): Loss_D: 0.2565 Loss_G: 0.5673\n",
            "===> Epoch[130](9/10): Loss_D: 0.2519 Loss_G: 0.6422\n",
            "===> Epoch[130](10/10): Loss_D: 0.2485 Loss_G: 0.6390\n",
            "learning rate = 0.0001386\n",
            "learning rate = 0.0001386\n",
            "===> Avg. PSNR: 15.1843 dB\n",
            "===> Epoch[131](1/10): Loss_D: 0.2513 Loss_G: 0.6837\n",
            "===> Epoch[131](2/10): Loss_D: 0.2524 Loss_G: 0.7377\n",
            "===> Epoch[131](3/10): Loss_D: 0.2572 Loss_G: 0.6198\n",
            "===> Epoch[131](4/10): Loss_D: 0.2457 Loss_G: 0.7350\n",
            "===> Epoch[131](5/10): Loss_D: 0.2553 Loss_G: 0.6826\n",
            "===> Epoch[131](6/10): Loss_D: 0.2529 Loss_G: 0.7780\n",
            "===> Epoch[131](7/10): Loss_D: 0.2548 Loss_G: 0.6313\n",
            "===> Epoch[131](8/10): Loss_D: 0.2541 Loss_G: 0.6628\n",
            "===> Epoch[131](9/10): Loss_D: 0.2492 Loss_G: 0.6641\n",
            "===> Epoch[131](10/10): Loss_D: 0.2414 Loss_G: 0.7164\n",
            "learning rate = 0.0001366\n",
            "learning rate = 0.0001366\n",
            "===> Avg. PSNR: 14.0193 dB\n",
            "===> Epoch[132](1/10): Loss_D: 0.2506 Loss_G: 0.6245\n",
            "===> Epoch[132](2/10): Loss_D: 0.2573 Loss_G: 0.7030\n",
            "===> Epoch[132](3/10): Loss_D: 0.2538 Loss_G: 0.6293\n",
            "===> Epoch[132](4/10): Loss_D: 0.2477 Loss_G: 0.6472\n",
            "===> Epoch[132](5/10): Loss_D: 0.2451 Loss_G: 0.7408\n",
            "===> Epoch[132](6/10): Loss_D: 0.2506 Loss_G: 0.6744\n",
            "===> Epoch[132](7/10): Loss_D: 0.2518 Loss_G: 0.6245\n",
            "===> Epoch[132](8/10): Loss_D: 0.2473 Loss_G: 0.6402\n",
            "===> Epoch[132](9/10): Loss_D: 0.2482 Loss_G: 0.6823\n",
            "===> Epoch[132](10/10): Loss_D: 0.2556 Loss_G: 0.6945\n",
            "learning rate = 0.0001347\n",
            "learning rate = 0.0001347\n",
            "===> Avg. PSNR: 15.2390 dB\n",
            "===> Epoch[133](1/10): Loss_D: 0.2521 Loss_G: 0.5963\n",
            "===> Epoch[133](2/10): Loss_D: 0.2489 Loss_G: 0.7216\n",
            "===> Epoch[133](3/10): Loss_D: 0.2542 Loss_G: 0.6818\n",
            "===> Epoch[133](4/10): Loss_D: 0.2537 Loss_G: 0.6254\n",
            "===> Epoch[133](5/10): Loss_D: 0.2521 Loss_G: 0.6636\n",
            "===> Epoch[133](6/10): Loss_D: 0.2504 Loss_G: 0.6815\n",
            "===> Epoch[133](7/10): Loss_D: 0.2425 Loss_G: 0.7290\n",
            "===> Epoch[133](8/10): Loss_D: 0.2560 Loss_G: 0.6309\n",
            "===> Epoch[133](9/10): Loss_D: 0.2441 Loss_G: 0.7087\n",
            "===> Epoch[133](10/10): Loss_D: 0.2468 Loss_G: 0.6795\n",
            "learning rate = 0.0001327\n",
            "learning rate = 0.0001327\n",
            "===> Avg. PSNR: 15.1004 dB\n",
            "===> Epoch[134](1/10): Loss_D: 0.2512 Loss_G: 0.6025\n",
            "===> Epoch[134](2/10): Loss_D: 0.2569 Loss_G: 0.6844\n",
            "===> Epoch[134](3/10): Loss_D: 0.2529 Loss_G: 0.7542\n",
            "===> Epoch[134](4/10): Loss_D: 0.2546 Loss_G: 0.7063\n",
            "===> Epoch[134](5/10): Loss_D: 0.2525 Loss_G: 0.5482\n",
            "===> Epoch[134](6/10): Loss_D: 0.2565 Loss_G: 0.6036\n",
            "===> Epoch[134](7/10): Loss_D: 0.2569 Loss_G: 0.6171\n",
            "===> Epoch[134](8/10): Loss_D: 0.2506 Loss_G: 0.7415\n",
            "===> Epoch[134](9/10): Loss_D: 0.2538 Loss_G: 0.6718\n",
            "===> Epoch[134](10/10): Loss_D: 0.2525 Loss_G: 0.6819\n",
            "learning rate = 0.0001307\n",
            "learning rate = 0.0001307\n",
            "===> Avg. PSNR: 12.6493 dB\n",
            "===> Epoch[135](1/10): Loss_D: 0.2559 Loss_G: 0.6369\n",
            "===> Epoch[135](2/10): Loss_D: 0.2552 Loss_G: 0.5940\n",
            "===> Epoch[135](3/10): Loss_D: 0.2473 Loss_G: 0.6725\n",
            "===> Epoch[135](4/10): Loss_D: 0.2541 Loss_G: 0.6069\n",
            "===> Epoch[135](5/10): Loss_D: 0.2531 Loss_G: 0.6494\n",
            "===> Epoch[135](6/10): Loss_D: 0.2515 Loss_G: 0.7120\n",
            "===> Epoch[135](7/10): Loss_D: 0.2491 Loss_G: 0.6571\n",
            "===> Epoch[135](8/10): Loss_D: 0.2529 Loss_G: 0.6764\n",
            "===> Epoch[135](9/10): Loss_D: 0.2605 Loss_G: 0.7743\n",
            "===> Epoch[135](10/10): Loss_D: 0.2620 Loss_G: 0.6103\n",
            "learning rate = 0.0001287\n",
            "learning rate = 0.0001287\n",
            "===> Avg. PSNR: 14.3535 dB\n",
            "===> Epoch[136](1/10): Loss_D: 0.2546 Loss_G: 0.6352\n",
            "===> Epoch[136](2/10): Loss_D: 0.2489 Loss_G: 0.6285\n",
            "===> Epoch[136](3/10): Loss_D: 0.2506 Loss_G: 0.6819\n",
            "===> Epoch[136](4/10): Loss_D: 0.2474 Loss_G: 0.6557\n",
            "===> Epoch[136](5/10): Loss_D: 0.2513 Loss_G: 0.7044\n",
            "===> Epoch[136](6/10): Loss_D: 0.2497 Loss_G: 0.6263\n",
            "===> Epoch[136](7/10): Loss_D: 0.2472 Loss_G: 0.6575\n",
            "===> Epoch[136](8/10): Loss_D: 0.2523 Loss_G: 0.6590\n",
            "===> Epoch[136](9/10): Loss_D: 0.2527 Loss_G: 0.6467\n",
            "===> Epoch[136](10/10): Loss_D: 0.2527 Loss_G: 0.6574\n",
            "learning rate = 0.0001267\n",
            "learning rate = 0.0001267\n",
            "===> Avg. PSNR: 13.9020 dB\n",
            "===> Epoch[137](1/10): Loss_D: 0.2534 Loss_G: 0.6683\n",
            "===> Epoch[137](2/10): Loss_D: 0.2508 Loss_G: 0.6271\n",
            "===> Epoch[137](3/10): Loss_D: 0.2547 Loss_G: 0.6527\n",
            "===> Epoch[137](4/10): Loss_D: 0.2481 Loss_G: 0.7156\n",
            "===> Epoch[137](5/10): Loss_D: 0.2494 Loss_G: 0.6334\n",
            "===> Epoch[137](6/10): Loss_D: 0.2503 Loss_G: 0.6867\n",
            "===> Epoch[137](7/10): Loss_D: 0.2542 Loss_G: 0.6648\n",
            "===> Epoch[137](8/10): Loss_D: 0.2550 Loss_G: 0.5978\n",
            "===> Epoch[137](9/10): Loss_D: 0.2580 Loss_G: 0.6110\n",
            "===> Epoch[137](10/10): Loss_D: 0.2487 Loss_G: 0.8030\n",
            "learning rate = 0.0001248\n",
            "learning rate = 0.0001248\n",
            "===> Avg. PSNR: 15.5496 dB\n",
            "===> Epoch[138](1/10): Loss_D: 0.2487 Loss_G: 0.7438\n",
            "===> Epoch[138](2/10): Loss_D: 0.2468 Loss_G: 0.6209\n",
            "===> Epoch[138](3/10): Loss_D: 0.2527 Loss_G: 0.6904\n",
            "===> Epoch[138](4/10): Loss_D: 0.2569 Loss_G: 0.6032\n",
            "===> Epoch[138](5/10): Loss_D: 0.2534 Loss_G: 0.5889\n",
            "===> Epoch[138](6/10): Loss_D: 0.2566 Loss_G: 0.6317\n",
            "===> Epoch[138](7/10): Loss_D: 0.2525 Loss_G: 0.7120\n",
            "===> Epoch[138](8/10): Loss_D: 0.2503 Loss_G: 0.7061\n",
            "===> Epoch[138](9/10): Loss_D: 0.2604 Loss_G: 0.6092\n",
            "===> Epoch[138](10/10): Loss_D: 0.2548 Loss_G: 0.5999\n",
            "learning rate = 0.0001228\n",
            "learning rate = 0.0001228\n",
            "===> Avg. PSNR: 15.0737 dB\n",
            "===> Epoch[139](1/10): Loss_D: 0.2551 Loss_G: 0.6844\n",
            "===> Epoch[139](2/10): Loss_D: 0.2519 Loss_G: 0.6661\n",
            "===> Epoch[139](3/10): Loss_D: 0.2514 Loss_G: 0.6123\n",
            "===> Epoch[139](4/10): Loss_D: 0.2515 Loss_G: 0.7153\n",
            "===> Epoch[139](5/10): Loss_D: 0.2622 Loss_G: 0.6396\n",
            "===> Epoch[139](6/10): Loss_D: 0.2525 Loss_G: 0.5722\n",
            "===> Epoch[139](7/10): Loss_D: 0.2493 Loss_G: 0.7330\n",
            "===> Epoch[139](8/10): Loss_D: 0.2627 Loss_G: 0.6036\n",
            "===> Epoch[139](9/10): Loss_D: 0.2533 Loss_G: 0.6406\n",
            "===> Epoch[139](10/10): Loss_D: 0.2479 Loss_G: 0.6478\n",
            "learning rate = 0.0001208\n",
            "learning rate = 0.0001208\n",
            "===> Avg. PSNR: 15.8864 dB\n",
            "===> Epoch[140](1/10): Loss_D: 0.2516 Loss_G: 0.6031\n",
            "===> Epoch[140](2/10): Loss_D: 0.2508 Loss_G: 0.6418\n",
            "===> Epoch[140](3/10): Loss_D: 0.2495 Loss_G: 0.6293\n",
            "===> Epoch[140](4/10): Loss_D: 0.2504 Loss_G: 0.6849\n",
            "===> Epoch[140](5/10): Loss_D: 0.2456 Loss_G: 0.6930\n",
            "===> Epoch[140](6/10): Loss_D: 0.2499 Loss_G: 0.7001\n",
            "===> Epoch[140](7/10): Loss_D: 0.2508 Loss_G: 0.6215\n",
            "===> Epoch[140](8/10): Loss_D: 0.2529 Loss_G: 0.6194\n",
            "===> Epoch[140](9/10): Loss_D: 0.2557 Loss_G: 0.5988\n",
            "===> Epoch[140](10/10): Loss_D: 0.2539 Loss_G: 0.7228\n",
            "learning rate = 0.0001188\n",
            "learning rate = 0.0001188\n",
            "===> Avg. PSNR: 14.8889 dB\n",
            "===> Epoch[141](1/10): Loss_D: 0.2518 Loss_G: 0.5482\n",
            "===> Epoch[141](2/10): Loss_D: 0.2486 Loss_G: 0.6247\n",
            "===> Epoch[141](3/10): Loss_D: 0.2517 Loss_G: 0.5506\n",
            "===> Epoch[141](4/10): Loss_D: 0.2568 Loss_G: 0.6801\n",
            "===> Epoch[141](5/10): Loss_D: 0.2539 Loss_G: 0.6080\n",
            "===> Epoch[141](6/10): Loss_D: 0.2529 Loss_G: 0.5934\n",
            "===> Epoch[141](7/10): Loss_D: 0.2507 Loss_G: 0.6303\n",
            "===> Epoch[141](8/10): Loss_D: 0.2522 Loss_G: 0.7048\n",
            "===> Epoch[141](9/10): Loss_D: 0.2519 Loss_G: 0.6994\n",
            "===> Epoch[141](10/10): Loss_D: 0.2558 Loss_G: 0.6164\n",
            "learning rate = 0.0001168\n",
            "learning rate = 0.0001168\n",
            "===> Avg. PSNR: 15.3006 dB\n",
            "===> Epoch[142](1/10): Loss_D: 0.2528 Loss_G: 0.5786\n",
            "===> Epoch[142](2/10): Loss_D: 0.2504 Loss_G: 0.6318\n",
            "===> Epoch[142](3/10): Loss_D: 0.2537 Loss_G: 0.6985\n",
            "===> Epoch[142](4/10): Loss_D: 0.2530 Loss_G: 0.6362\n",
            "===> Epoch[142](5/10): Loss_D: 0.2512 Loss_G: 0.6488\n",
            "===> Epoch[142](6/10): Loss_D: 0.2532 Loss_G: 0.6410\n",
            "===> Epoch[142](7/10): Loss_D: 0.2511 Loss_G: 0.6467\n",
            "===> Epoch[142](8/10): Loss_D: 0.2520 Loss_G: 0.6990\n",
            "===> Epoch[142](9/10): Loss_D: 0.2434 Loss_G: 0.7248\n",
            "===> Epoch[142](10/10): Loss_D: 0.2525 Loss_G: 0.6039\n",
            "learning rate = 0.0001149\n",
            "learning rate = 0.0001149\n",
            "===> Avg. PSNR: 15.0678 dB\n",
            "===> Epoch[143](1/10): Loss_D: 0.2537 Loss_G: 0.5857\n",
            "===> Epoch[143](2/10): Loss_D: 0.2499 Loss_G: 0.6359\n",
            "===> Epoch[143](3/10): Loss_D: 0.2570 Loss_G: 0.6600\n",
            "===> Epoch[143](4/10): Loss_D: 0.2479 Loss_G: 0.6780\n",
            "===> Epoch[143](5/10): Loss_D: 0.2499 Loss_G: 0.6031\n",
            "===> Epoch[143](6/10): Loss_D: 0.2495 Loss_G: 0.6282\n",
            "===> Epoch[143](7/10): Loss_D: 0.2560 Loss_G: 0.6486\n",
            "===> Epoch[143](8/10): Loss_D: 0.2491 Loss_G: 0.7069\n",
            "===> Epoch[143](9/10): Loss_D: 0.2555 Loss_G: 0.5826\n",
            "===> Epoch[143](10/10): Loss_D: 0.2534 Loss_G: 0.5717\n",
            "learning rate = 0.0001129\n",
            "learning rate = 0.0001129\n",
            "===> Avg. PSNR: 14.0269 dB\n",
            "===> Epoch[144](1/10): Loss_D: 0.2493 Loss_G: 0.6350\n",
            "===> Epoch[144](2/10): Loss_D: 0.2470 Loss_G: 0.7119\n",
            "===> Epoch[144](3/10): Loss_D: 0.2532 Loss_G: 0.6338\n",
            "===> Epoch[144](4/10): Loss_D: 0.2508 Loss_G: 0.5783\n",
            "===> Epoch[144](5/10): Loss_D: 0.2486 Loss_G: 0.7174\n",
            "===> Epoch[144](6/10): Loss_D: 0.2531 Loss_G: 0.5736\n",
            "===> Epoch[144](7/10): Loss_D: 0.2548 Loss_G: 0.5893\n",
            "===> Epoch[144](8/10): Loss_D: 0.2443 Loss_G: 0.5768\n",
            "===> Epoch[144](9/10): Loss_D: 0.2479 Loss_G: 0.7087\n",
            "===> Epoch[144](10/10): Loss_D: 0.2532 Loss_G: 0.6769\n",
            "learning rate = 0.0001109\n",
            "learning rate = 0.0001109\n",
            "===> Avg. PSNR: 14.3066 dB\n",
            "===> Epoch[145](1/10): Loss_D: 0.2585 Loss_G: 0.5640\n",
            "===> Epoch[145](2/10): Loss_D: 0.2548 Loss_G: 0.6519\n",
            "===> Epoch[145](3/10): Loss_D: 0.2542 Loss_G: 0.6739\n",
            "===> Epoch[145](4/10): Loss_D: 0.2523 Loss_G: 0.6221\n",
            "===> Epoch[145](5/10): Loss_D: 0.2472 Loss_G: 0.6463\n",
            "===> Epoch[145](6/10): Loss_D: 0.2495 Loss_G: 0.6718\n",
            "===> Epoch[145](7/10): Loss_D: 0.2536 Loss_G: 0.6651\n",
            "===> Epoch[145](8/10): Loss_D: 0.2522 Loss_G: 0.6219\n",
            "===> Epoch[145](9/10): Loss_D: 0.2532 Loss_G: 0.6070\n",
            "===> Epoch[145](10/10): Loss_D: 0.2519 Loss_G: 0.6776\n",
            "learning rate = 0.0001089\n",
            "learning rate = 0.0001089\n",
            "===> Avg. PSNR: 15.7556 dB\n",
            "===> Epoch[146](1/10): Loss_D: 0.2548 Loss_G: 0.6460\n",
            "===> Epoch[146](2/10): Loss_D: 0.2510 Loss_G: 0.6023\n",
            "===> Epoch[146](3/10): Loss_D: 0.2529 Loss_G: 0.6513\n",
            "===> Epoch[146](4/10): Loss_D: 0.2546 Loss_G: 0.5951\n",
            "===> Epoch[146](5/10): Loss_D: 0.2477 Loss_G: 0.6386\n",
            "===> Epoch[146](6/10): Loss_D: 0.2519 Loss_G: 0.6152\n",
            "===> Epoch[146](7/10): Loss_D: 0.2482 Loss_G: 0.6749\n",
            "===> Epoch[146](8/10): Loss_D: 0.2494 Loss_G: 0.6050\n",
            "===> Epoch[146](9/10): Loss_D: 0.2427 Loss_G: 0.6370\n",
            "===> Epoch[146](10/10): Loss_D: 0.2520 Loss_G: 0.5950\n",
            "learning rate = 0.0001069\n",
            "learning rate = 0.0001069\n",
            "===> Avg. PSNR: 14.6870 dB\n",
            "===> Epoch[147](1/10): Loss_D: 0.2518 Loss_G: 0.5954\n",
            "===> Epoch[147](2/10): Loss_D: 0.2460 Loss_G: 0.6193\n",
            "===> Epoch[147](3/10): Loss_D: 0.2562 Loss_G: 0.5489\n",
            "===> Epoch[147](4/10): Loss_D: 0.2479 Loss_G: 0.5855\n",
            "===> Epoch[147](5/10): Loss_D: 0.2443 Loss_G: 0.7227\n",
            "===> Epoch[147](6/10): Loss_D: 0.2531 Loss_G: 0.6478\n",
            "===> Epoch[147](7/10): Loss_D: 0.2472 Loss_G: 0.6142\n",
            "===> Epoch[147](8/10): Loss_D: 0.2554 Loss_G: 0.5760\n",
            "===> Epoch[147](9/10): Loss_D: 0.2523 Loss_G: 0.7124\n",
            "===> Epoch[147](10/10): Loss_D: 0.2524 Loss_G: 0.6045\n",
            "learning rate = 0.0001050\n",
            "learning rate = 0.0001050\n",
            "===> Avg. PSNR: 16.1440 dB\n",
            "===> Epoch[148](1/10): Loss_D: 0.2566 Loss_G: 0.5579\n",
            "===> Epoch[148](2/10): Loss_D: 0.2543 Loss_G: 0.6567\n",
            "===> Epoch[148](3/10): Loss_D: 0.2554 Loss_G: 0.6284\n",
            "===> Epoch[148](4/10): Loss_D: 0.2503 Loss_G: 0.6331\n",
            "===> Epoch[148](5/10): Loss_D: 0.2441 Loss_G: 0.6856\n",
            "===> Epoch[148](6/10): Loss_D: 0.2510 Loss_G: 0.5930\n",
            "===> Epoch[148](7/10): Loss_D: 0.2490 Loss_G: 0.7049\n",
            "===> Epoch[148](8/10): Loss_D: 0.2487 Loss_G: 0.6825\n",
            "===> Epoch[148](9/10): Loss_D: 0.2581 Loss_G: 0.5538\n",
            "===> Epoch[148](10/10): Loss_D: 0.2532 Loss_G: 0.5898\n",
            "learning rate = 0.0001030\n",
            "learning rate = 0.0001030\n",
            "===> Avg. PSNR: 14.1322 dB\n",
            "===> Epoch[149](1/10): Loss_D: 0.2519 Loss_G: 0.5798\n",
            "===> Epoch[149](2/10): Loss_D: 0.2558 Loss_G: 0.6063\n",
            "===> Epoch[149](3/10): Loss_D: 0.2464 Loss_G: 0.7286\n",
            "===> Epoch[149](4/10): Loss_D: 0.2539 Loss_G: 0.6138\n",
            "===> Epoch[149](5/10): Loss_D: 0.2530 Loss_G: 0.6258\n",
            "===> Epoch[149](6/10): Loss_D: 0.2484 Loss_G: 0.6433\n",
            "===> Epoch[149](7/10): Loss_D: 0.2543 Loss_G: 0.6118\n",
            "===> Epoch[149](8/10): Loss_D: 0.2498 Loss_G: 0.6727\n",
            "===> Epoch[149](9/10): Loss_D: 0.2490 Loss_G: 0.6176\n",
            "===> Epoch[149](10/10): Loss_D: 0.2507 Loss_G: 0.6132\n",
            "learning rate = 0.0001010\n",
            "learning rate = 0.0001010\n",
            "===> Avg. PSNR: 14.7668 dB\n",
            "===> Epoch[150](1/10): Loss_D: 0.2541 Loss_G: 0.5740\n",
            "===> Epoch[150](2/10): Loss_D: 0.2460 Loss_G: 0.6176\n",
            "===> Epoch[150](3/10): Loss_D: 0.2450 Loss_G: 0.6458\n",
            "===> Epoch[150](4/10): Loss_D: 0.2527 Loss_G: 0.6485\n",
            "===> Epoch[150](5/10): Loss_D: 0.2509 Loss_G: 0.5994\n",
            "===> Epoch[150](6/10): Loss_D: 0.2511 Loss_G: 0.6223\n",
            "===> Epoch[150](7/10): Loss_D: 0.2517 Loss_G: 0.5948\n",
            "===> Epoch[150](8/10): Loss_D: 0.2540 Loss_G: 0.6158\n",
            "===> Epoch[150](9/10): Loss_D: 0.2465 Loss_G: 0.6705\n",
            "===> Epoch[150](10/10): Loss_D: 0.2492 Loss_G: 0.6984\n",
            "learning rate = 0.0000990\n",
            "learning rate = 0.0000990\n",
            "===> Avg. PSNR: 14.9573 dB\n",
            "Checkpoint saved to checkpoint./football\n",
            "===> Epoch[151](1/10): Loss_D: 0.2488 Loss_G: 0.6053\n",
            "===> Epoch[151](2/10): Loss_D: 0.2504 Loss_G: 0.6503\n",
            "===> Epoch[151](3/10): Loss_D: 0.2541 Loss_G: 0.5803\n",
            "===> Epoch[151](4/10): Loss_D: 0.2507 Loss_G: 0.6063\n",
            "===> Epoch[151](5/10): Loss_D: 0.2494 Loss_G: 0.5998\n",
            "===> Epoch[151](6/10): Loss_D: 0.2477 Loss_G: 0.6738\n",
            "===> Epoch[151](7/10): Loss_D: 0.2507 Loss_G: 0.6226\n",
            "===> Epoch[151](8/10): Loss_D: 0.2495 Loss_G: 0.6552\n",
            "===> Epoch[151](9/10): Loss_D: 0.2649 Loss_G: 0.5715\n",
            "===> Epoch[151](10/10): Loss_D: 0.2584 Loss_G: 0.6022\n",
            "learning rate = 0.0000970\n",
            "learning rate = 0.0000970\n",
            "===> Avg. PSNR: 14.7715 dB\n",
            "===> Epoch[152](1/10): Loss_D: 0.2547 Loss_G: 0.6077\n",
            "===> Epoch[152](2/10): Loss_D: 0.2528 Loss_G: 0.5836\n",
            "===> Epoch[152](3/10): Loss_D: 0.2498 Loss_G: 0.6426\n",
            "===> Epoch[152](4/10): Loss_D: 0.2546 Loss_G: 0.5917\n",
            "===> Epoch[152](5/10): Loss_D: 0.2495 Loss_G: 0.6211\n",
            "===> Epoch[152](6/10): Loss_D: 0.2563 Loss_G: 0.5731\n",
            "===> Epoch[152](7/10): Loss_D: 0.2464 Loss_G: 0.5729\n",
            "===> Epoch[152](8/10): Loss_D: 0.2518 Loss_G: 0.6520\n",
            "===> Epoch[152](9/10): Loss_D: 0.2481 Loss_G: 0.6038\n",
            "===> Epoch[152](10/10): Loss_D: 0.2503 Loss_G: 0.6417\n",
            "learning rate = 0.0000950\n",
            "learning rate = 0.0000950\n",
            "===> Avg. PSNR: 15.3976 dB\n",
            "===> Epoch[153](1/10): Loss_D: 0.2549 Loss_G: 0.5468\n",
            "===> Epoch[153](2/10): Loss_D: 0.2515 Loss_G: 0.6052\n",
            "===> Epoch[153](3/10): Loss_D: 0.2516 Loss_G: 0.5814\n",
            "===> Epoch[153](4/10): Loss_D: 0.2486 Loss_G: 0.6274\n",
            "===> Epoch[153](5/10): Loss_D: 0.2515 Loss_G: 0.6214\n",
            "===> Epoch[153](6/10): Loss_D: 0.2498 Loss_G: 0.6762\n",
            "===> Epoch[153](7/10): Loss_D: 0.2473 Loss_G: 0.6416\n",
            "===> Epoch[153](8/10): Loss_D: 0.2528 Loss_G: 0.5890\n",
            "===> Epoch[153](9/10): Loss_D: 0.2490 Loss_G: 0.5553\n",
            "===> Epoch[153](10/10): Loss_D: 0.2453 Loss_G: 0.6888\n",
            "learning rate = 0.0000931\n",
            "learning rate = 0.0000931\n",
            "===> Avg. PSNR: 15.5914 dB\n",
            "===> Epoch[154](1/10): Loss_D: 0.2482 Loss_G: 0.5799\n",
            "===> Epoch[154](2/10): Loss_D: 0.2517 Loss_G: 0.5800\n",
            "===> Epoch[154](3/10): Loss_D: 0.2507 Loss_G: 0.6696\n",
            "===> Epoch[154](4/10): Loss_D: 0.2499 Loss_G: 0.7359\n",
            "===> Epoch[154](5/10): Loss_D: 0.2450 Loss_G: 0.6816\n",
            "===> Epoch[154](6/10): Loss_D: 0.2522 Loss_G: 0.6523\n",
            "===> Epoch[154](7/10): Loss_D: 0.2540 Loss_G: 0.6033\n",
            "===> Epoch[154](8/10): Loss_D: 0.2527 Loss_G: 0.6455\n",
            "===> Epoch[154](9/10): Loss_D: 0.2493 Loss_G: 0.5691\n",
            "===> Epoch[154](10/10): Loss_D: 0.2507 Loss_G: 0.5879\n",
            "learning rate = 0.0000911\n",
            "learning rate = 0.0000911\n",
            "===> Avg. PSNR: 14.9229 dB\n",
            "===> Epoch[155](1/10): Loss_D: 0.2493 Loss_G: 0.5608\n",
            "===> Epoch[155](2/10): Loss_D: 0.2456 Loss_G: 0.7253\n",
            "===> Epoch[155](3/10): Loss_D: 0.2483 Loss_G: 0.6196\n",
            "===> Epoch[155](4/10): Loss_D: 0.2532 Loss_G: 0.6102\n",
            "===> Epoch[155](5/10): Loss_D: 0.2499 Loss_G: 0.6182\n",
            "===> Epoch[155](6/10): Loss_D: 0.2446 Loss_G: 0.6002\n",
            "===> Epoch[155](7/10): Loss_D: 0.2442 Loss_G: 0.5605\n",
            "===> Epoch[155](8/10): Loss_D: 0.2521 Loss_G: 0.6234\n",
            "===> Epoch[155](9/10): Loss_D: 0.2510 Loss_G: 0.6706\n",
            "===> Epoch[155](10/10): Loss_D: 0.2494 Loss_G: 0.6326\n",
            "learning rate = 0.0000891\n",
            "learning rate = 0.0000891\n",
            "===> Avg. PSNR: 14.0132 dB\n",
            "===> Epoch[156](1/10): Loss_D: 0.2520 Loss_G: 0.5775\n",
            "===> Epoch[156](2/10): Loss_D: 0.2495 Loss_G: 0.5658\n",
            "===> Epoch[156](3/10): Loss_D: 0.2526 Loss_G: 0.6602\n",
            "===> Epoch[156](4/10): Loss_D: 0.2496 Loss_G: 0.6661\n",
            "===> Epoch[156](5/10): Loss_D: 0.2492 Loss_G: 0.6313\n",
            "===> Epoch[156](6/10): Loss_D: 0.2506 Loss_G: 0.5503\n",
            "===> Epoch[156](7/10): Loss_D: 0.2549 Loss_G: 0.5755\n",
            "===> Epoch[156](8/10): Loss_D: 0.2534 Loss_G: 0.6320\n",
            "===> Epoch[156](9/10): Loss_D: 0.2521 Loss_G: 0.5630\n",
            "===> Epoch[156](10/10): Loss_D: 0.2538 Loss_G: 0.6383\n",
            "learning rate = 0.0000871\n",
            "learning rate = 0.0000871\n",
            "===> Avg. PSNR: 14.6253 dB\n",
            "===> Epoch[157](1/10): Loss_D: 0.2565 Loss_G: 0.5982\n",
            "===> Epoch[157](2/10): Loss_D: 0.2483 Loss_G: 0.5895\n",
            "===> Epoch[157](3/10): Loss_D: 0.2533 Loss_G: 0.6518\n",
            "===> Epoch[157](4/10): Loss_D: 0.2509 Loss_G: 0.6066\n",
            "===> Epoch[157](5/10): Loss_D: 0.2516 Loss_G: 0.5584\n",
            "===> Epoch[157](6/10): Loss_D: 0.2484 Loss_G: 0.6069\n",
            "===> Epoch[157](7/10): Loss_D: 0.2523 Loss_G: 0.5633\n",
            "===> Epoch[157](8/10): Loss_D: 0.2538 Loss_G: 0.5646\n",
            "===> Epoch[157](9/10): Loss_D: 0.2523 Loss_G: 0.6298\n",
            "===> Epoch[157](10/10): Loss_D: 0.2460 Loss_G: 0.6285\n",
            "learning rate = 0.0000851\n",
            "learning rate = 0.0000851\n",
            "===> Avg. PSNR: 15.3609 dB\n",
            "===> Epoch[158](1/10): Loss_D: 0.2453 Loss_G: 0.5931\n",
            "===> Epoch[158](2/10): Loss_D: 0.2517 Loss_G: 0.6200\n",
            "===> Epoch[158](3/10): Loss_D: 0.2471 Loss_G: 0.6148\n",
            "===> Epoch[158](4/10): Loss_D: 0.2520 Loss_G: 0.6210\n",
            "===> Epoch[158](5/10): Loss_D: 0.2509 Loss_G: 0.5952\n",
            "===> Epoch[158](6/10): Loss_D: 0.2485 Loss_G: 0.6078\n",
            "===> Epoch[158](7/10): Loss_D: 0.2569 Loss_G: 0.5905\n",
            "===> Epoch[158](8/10): Loss_D: 0.2543 Loss_G: 0.5876\n",
            "===> Epoch[158](9/10): Loss_D: 0.2532 Loss_G: 0.5682\n",
            "===> Epoch[158](10/10): Loss_D: 0.2477 Loss_G: 0.6777\n",
            "learning rate = 0.0000832\n",
            "learning rate = 0.0000832\n",
            "===> Avg. PSNR: 15.2153 dB\n",
            "===> Epoch[159](1/10): Loss_D: 0.2523 Loss_G: 0.5528\n",
            "===> Epoch[159](2/10): Loss_D: 0.2516 Loss_G: 0.6200\n",
            "===> Epoch[159](3/10): Loss_D: 0.2482 Loss_G: 0.6086\n",
            "===> Epoch[159](4/10): Loss_D: 0.2456 Loss_G: 0.5962\n",
            "===> Epoch[159](5/10): Loss_D: 0.2495 Loss_G: 0.5813\n",
            "===> Epoch[159](6/10): Loss_D: 0.2547 Loss_G: 0.5722\n",
            "===> Epoch[159](7/10): Loss_D: 0.2464 Loss_G: 0.5601\n",
            "===> Epoch[159](8/10): Loss_D: 0.2448 Loss_G: 0.6536\n",
            "===> Epoch[159](9/10): Loss_D: 0.2525 Loss_G: 0.6266\n",
            "===> Epoch[159](10/10): Loss_D: 0.2609 Loss_G: 0.5390\n",
            "learning rate = 0.0000812\n",
            "learning rate = 0.0000812\n",
            "===> Avg. PSNR: 15.2975 dB\n",
            "===> Epoch[160](1/10): Loss_D: 0.2492 Loss_G: 0.5788\n",
            "===> Epoch[160](2/10): Loss_D: 0.2473 Loss_G: 0.6017\n",
            "===> Epoch[160](3/10): Loss_D: 0.2519 Loss_G: 0.5906\n",
            "===> Epoch[160](4/10): Loss_D: 0.2504 Loss_G: 0.5913\n",
            "===> Epoch[160](5/10): Loss_D: 0.2526 Loss_G: 0.5829\n",
            "===> Epoch[160](6/10): Loss_D: 0.2542 Loss_G: 0.5422\n",
            "===> Epoch[160](7/10): Loss_D: 0.2510 Loss_G: 0.5802\n",
            "===> Epoch[160](8/10): Loss_D: 0.2514 Loss_G: 0.6204\n",
            "===> Epoch[160](9/10): Loss_D: 0.2483 Loss_G: 0.6235\n",
            "===> Epoch[160](10/10): Loss_D: 0.2502 Loss_G: 0.6110\n",
            "learning rate = 0.0000792\n",
            "learning rate = 0.0000792\n",
            "===> Avg. PSNR: 16.0821 dB\n",
            "===> Epoch[161](1/10): Loss_D: 0.2473 Loss_G: 0.6175\n",
            "===> Epoch[161](2/10): Loss_D: 0.2450 Loss_G: 0.5776\n",
            "===> Epoch[161](3/10): Loss_D: 0.2503 Loss_G: 0.6151\n",
            "===> Epoch[161](4/10): Loss_D: 0.2514 Loss_G: 0.6233\n",
            "===> Epoch[161](5/10): Loss_D: 0.2557 Loss_G: 0.5455\n",
            "===> Epoch[161](6/10): Loss_D: 0.2465 Loss_G: 0.5987\n",
            "===> Epoch[161](7/10): Loss_D: 0.2495 Loss_G: 0.5835\n",
            "===> Epoch[161](8/10): Loss_D: 0.2474 Loss_G: 0.6469\n",
            "===> Epoch[161](9/10): Loss_D: 0.2550 Loss_G: 0.6659\n",
            "===> Epoch[161](10/10): Loss_D: 0.2508 Loss_G: 0.6401\n",
            "learning rate = 0.0000772\n",
            "learning rate = 0.0000772\n",
            "===> Avg. PSNR: 14.9022 dB\n",
            "===> Epoch[162](1/10): Loss_D: 0.2541 Loss_G: 0.6357\n",
            "===> Epoch[162](2/10): Loss_D: 0.2796 Loss_G: 0.6057\n",
            "===> Epoch[162](3/10): Loss_D: 0.2548 Loss_G: 0.6182\n",
            "===> Epoch[162](4/10): Loss_D: 0.2505 Loss_G: 0.6147\n",
            "===> Epoch[162](5/10): Loss_D: 0.2509 Loss_G: 0.5635\n",
            "===> Epoch[162](6/10): Loss_D: 0.2495 Loss_G: 0.6277\n",
            "===> Epoch[162](7/10): Loss_D: 0.2494 Loss_G: 0.6456\n",
            "===> Epoch[162](8/10): Loss_D: 0.2473 Loss_G: 0.6331\n",
            "===> Epoch[162](9/10): Loss_D: 0.2497 Loss_G: 0.5157\n",
            "===> Epoch[162](10/10): Loss_D: 0.2510 Loss_G: 0.6109\n",
            "learning rate = 0.0000752\n",
            "learning rate = 0.0000752\n",
            "===> Avg. PSNR: 15.0211 dB\n",
            "===> Epoch[163](1/10): Loss_D: 0.2497 Loss_G: 0.5912\n",
            "===> Epoch[163](2/10): Loss_D: 0.2482 Loss_G: 0.5944\n",
            "===> Epoch[163](3/10): Loss_D: 0.2503 Loss_G: 0.6298\n",
            "===> Epoch[163](4/10): Loss_D: 0.2521 Loss_G: 0.6131\n",
            "===> Epoch[163](5/10): Loss_D: 0.2443 Loss_G: 0.5790\n",
            "===> Epoch[163](6/10): Loss_D: 0.2508 Loss_G: 0.5394\n",
            "===> Epoch[163](7/10): Loss_D: 0.2484 Loss_G: 0.5950\n",
            "===> Epoch[163](8/10): Loss_D: 0.2454 Loss_G: 0.5999\n",
            "===> Epoch[163](9/10): Loss_D: 0.2476 Loss_G: 0.6234\n",
            "===> Epoch[163](10/10): Loss_D: 0.2521 Loss_G: 0.6406\n",
            "learning rate = 0.0000733\n",
            "learning rate = 0.0000733\n",
            "===> Avg. PSNR: 15.7586 dB\n",
            "===> Epoch[164](1/10): Loss_D: 0.2500 Loss_G: 0.5611\n",
            "===> Epoch[164](2/10): Loss_D: 0.2525 Loss_G: 0.5980\n",
            "===> Epoch[164](3/10): Loss_D: 0.2482 Loss_G: 0.6598\n",
            "===> Epoch[164](4/10): Loss_D: 0.2442 Loss_G: 0.5814\n",
            "===> Epoch[164](5/10): Loss_D: 0.2484 Loss_G: 0.5603\n",
            "===> Epoch[164](6/10): Loss_D: 0.2524 Loss_G: 0.5700\n",
            "===> Epoch[164](7/10): Loss_D: 0.2527 Loss_G: 0.6078\n",
            "===> Epoch[164](8/10): Loss_D: 0.2487 Loss_G: 0.5819\n",
            "===> Epoch[164](9/10): Loss_D: 0.2520 Loss_G: 0.5930\n",
            "===> Epoch[164](10/10): Loss_D: 0.2488 Loss_G: 0.6331\n",
            "learning rate = 0.0000713\n",
            "learning rate = 0.0000713\n",
            "===> Avg. PSNR: 16.3017 dB\n",
            "===> Epoch[165](1/10): Loss_D: 0.2476 Loss_G: 0.5306\n",
            "===> Epoch[165](2/10): Loss_D: 0.2476 Loss_G: 0.6615\n",
            "===> Epoch[165](3/10): Loss_D: 0.2478 Loss_G: 0.6089\n",
            "===> Epoch[165](4/10): Loss_D: 0.2525 Loss_G: 0.5722\n",
            "===> Epoch[165](5/10): Loss_D: 0.2525 Loss_G: 0.5459\n",
            "===> Epoch[165](6/10): Loss_D: 0.2532 Loss_G: 0.6151\n",
            "===> Epoch[165](7/10): Loss_D: 0.2484 Loss_G: 0.5761\n",
            "===> Epoch[165](8/10): Loss_D: 0.2472 Loss_G: 0.6403\n",
            "===> Epoch[165](9/10): Loss_D: 0.2517 Loss_G: 0.5720\n",
            "===> Epoch[165](10/10): Loss_D: 0.2479 Loss_G: 0.5916\n",
            "learning rate = 0.0000693\n",
            "learning rate = 0.0000693\n",
            "===> Avg. PSNR: 15.6878 dB\n",
            "===> Epoch[166](1/10): Loss_D: 0.2494 Loss_G: 0.5999\n",
            "===> Epoch[166](2/10): Loss_D: 0.2575 Loss_G: 0.5129\n",
            "===> Epoch[166](3/10): Loss_D: 0.2559 Loss_G: 0.5921\n",
            "===> Epoch[166](4/10): Loss_D: 0.2511 Loss_G: 0.5271\n",
            "===> Epoch[166](5/10): Loss_D: 0.2490 Loss_G: 0.6015\n",
            "===> Epoch[166](6/10): Loss_D: 0.2515 Loss_G: 0.5535\n",
            "===> Epoch[166](7/10): Loss_D: 0.2456 Loss_G: 0.5773\n",
            "===> Epoch[166](8/10): Loss_D: 0.2506 Loss_G: 0.5812\n",
            "===> Epoch[166](9/10): Loss_D: 0.2504 Loss_G: 0.6304\n",
            "===> Epoch[166](10/10): Loss_D: 0.2504 Loss_G: 0.6764\n",
            "learning rate = 0.0000673\n",
            "learning rate = 0.0000673\n",
            "===> Avg. PSNR: 15.2060 dB\n",
            "===> Epoch[167](1/10): Loss_D: 0.2495 Loss_G: 0.5607\n",
            "===> Epoch[167](2/10): Loss_D: 0.2469 Loss_G: 0.5530\n",
            "===> Epoch[167](3/10): Loss_D: 0.2449 Loss_G: 0.6174\n",
            "===> Epoch[167](4/10): Loss_D: 0.2517 Loss_G: 0.5868\n",
            "===> Epoch[167](5/10): Loss_D: 0.2492 Loss_G: 0.5900\n",
            "===> Epoch[167](6/10): Loss_D: 0.2484 Loss_G: 0.5428\n",
            "===> Epoch[167](7/10): Loss_D: 0.2510 Loss_G: 0.6129\n",
            "===> Epoch[167](8/10): Loss_D: 0.2492 Loss_G: 0.6541\n",
            "===> Epoch[167](9/10): Loss_D: 0.2532 Loss_G: 0.5373\n",
            "===> Epoch[167](10/10): Loss_D: 0.2536 Loss_G: 0.5292\n",
            "learning rate = 0.0000653\n",
            "learning rate = 0.0000653\n",
            "===> Avg. PSNR: 15.4462 dB\n",
            "===> Epoch[168](1/10): Loss_D: 0.2577 Loss_G: 0.6044\n",
            "===> Epoch[168](2/10): Loss_D: 0.2451 Loss_G: 0.5881\n",
            "===> Epoch[168](3/10): Loss_D: 0.2498 Loss_G: 0.6379\n",
            "===> Epoch[168](4/10): Loss_D: 0.2532 Loss_G: 0.5325\n",
            "===> Epoch[168](5/10): Loss_D: 0.2502 Loss_G: 0.5907\n",
            "===> Epoch[168](6/10): Loss_D: 0.2512 Loss_G: 0.5285\n",
            "===> Epoch[168](7/10): Loss_D: 0.2488 Loss_G: 0.6124\n",
            "===> Epoch[168](8/10): Loss_D: 0.2508 Loss_G: 0.5814\n",
            "===> Epoch[168](9/10): Loss_D: 0.2487 Loss_G: 0.5861\n",
            "===> Epoch[168](10/10): Loss_D: 0.2544 Loss_G: 0.5087\n",
            "learning rate = 0.0000634\n",
            "learning rate = 0.0000634\n",
            "===> Avg. PSNR: 15.0241 dB\n",
            "===> Epoch[169](1/10): Loss_D: 0.2535 Loss_G: 0.5807\n",
            "===> Epoch[169](2/10): Loss_D: 0.2527 Loss_G: 0.6267\n",
            "===> Epoch[169](3/10): Loss_D: 0.2467 Loss_G: 0.5595\n",
            "===> Epoch[169](4/10): Loss_D: 0.2492 Loss_G: 0.5699\n",
            "===> Epoch[169](5/10): Loss_D: 0.2515 Loss_G: 0.5892\n",
            "===> Epoch[169](6/10): Loss_D: 0.2509 Loss_G: 0.5595\n",
            "===> Epoch[169](7/10): Loss_D: 0.2503 Loss_G: 0.5421\n",
            "===> Epoch[169](8/10): Loss_D: 0.2536 Loss_G: 0.6484\n",
            "===> Epoch[169](9/10): Loss_D: 0.2568 Loss_G: 0.5505\n",
            "===> Epoch[169](10/10): Loss_D: 0.2442 Loss_G: 0.5999\n",
            "learning rate = 0.0000614\n",
            "learning rate = 0.0000614\n",
            "===> Avg. PSNR: 14.8698 dB\n",
            "===> Epoch[170](1/10): Loss_D: 0.2468 Loss_G: 0.5276\n",
            "===> Epoch[170](2/10): Loss_D: 0.2485 Loss_G: 0.5591\n",
            "===> Epoch[170](3/10): Loss_D: 0.2504 Loss_G: 0.5416\n",
            "===> Epoch[170](4/10): Loss_D: 0.2489 Loss_G: 0.6059\n",
            "===> Epoch[170](5/10): Loss_D: 0.2432 Loss_G: 0.6200\n",
            "===> Epoch[170](6/10): Loss_D: 0.2481 Loss_G: 0.6736\n",
            "===> Epoch[170](7/10): Loss_D: 0.2484 Loss_G: 0.5712\n",
            "===> Epoch[170](8/10): Loss_D: 0.2525 Loss_G: 0.5255\n",
            "===> Epoch[170](9/10): Loss_D: 0.2499 Loss_G: 0.5681\n",
            "===> Epoch[170](10/10): Loss_D: 0.2500 Loss_G: 0.5685\n",
            "learning rate = 0.0000594\n",
            "learning rate = 0.0000594\n",
            "===> Avg. PSNR: 16.2817 dB\n",
            "===> Epoch[171](1/10): Loss_D: 0.2497 Loss_G: 0.5521\n",
            "===> Epoch[171](2/10): Loss_D: 0.2500 Loss_G: 0.5831\n",
            "===> Epoch[171](3/10): Loss_D: 0.2506 Loss_G: 0.5736\n",
            "===> Epoch[171](4/10): Loss_D: 0.2508 Loss_G: 0.5346\n",
            "===> Epoch[171](5/10): Loss_D: 0.2524 Loss_G: 0.5647\n",
            "===> Epoch[171](6/10): Loss_D: 0.2465 Loss_G: 0.6338\n",
            "===> Epoch[171](7/10): Loss_D: 0.2517 Loss_G: 0.5751\n",
            "===> Epoch[171](8/10): Loss_D: 0.2506 Loss_G: 0.5324\n",
            "===> Epoch[171](9/10): Loss_D: 0.2519 Loss_G: 0.5878\n",
            "===> Epoch[171](10/10): Loss_D: 0.2498 Loss_G: 0.5878\n",
            "learning rate = 0.0000574\n",
            "learning rate = 0.0000574\n",
            "===> Avg. PSNR: 16.4050 dB\n",
            "===> Epoch[172](1/10): Loss_D: 0.2502 Loss_G: 0.5461\n",
            "===> Epoch[172](2/10): Loss_D: 0.2516 Loss_G: 0.5794\n",
            "===> Epoch[172](3/10): Loss_D: 0.2524 Loss_G: 0.5863\n",
            "===> Epoch[172](4/10): Loss_D: 0.2426 Loss_G: 0.5884\n",
            "===> Epoch[172](5/10): Loss_D: 0.2438 Loss_G: 0.5912\n",
            "===> Epoch[172](6/10): Loss_D: 0.2528 Loss_G: 0.5552\n",
            "===> Epoch[172](7/10): Loss_D: 0.2439 Loss_G: 0.6491\n",
            "===> Epoch[172](8/10): Loss_D: 0.2416 Loss_G: 0.6238\n",
            "===> Epoch[172](9/10): Loss_D: 0.2545 Loss_G: 0.5337\n",
            "===> Epoch[172](10/10): Loss_D: 0.2515 Loss_G: 0.5138\n",
            "learning rate = 0.0000554\n",
            "learning rate = 0.0000554\n",
            "===> Avg. PSNR: 15.4115 dB\n",
            "===> Epoch[173](1/10): Loss_D: 0.2488 Loss_G: 0.5230\n",
            "===> Epoch[173](2/10): Loss_D: 0.2521 Loss_G: 0.5521\n",
            "===> Epoch[173](3/10): Loss_D: 0.2537 Loss_G: 0.5728\n",
            "===> Epoch[173](4/10): Loss_D: 0.2517 Loss_G: 0.5519\n",
            "===> Epoch[173](5/10): Loss_D: 0.2472 Loss_G: 0.6017\n",
            "===> Epoch[173](6/10): Loss_D: 0.2550 Loss_G: 0.5498\n",
            "===> Epoch[173](7/10): Loss_D: 0.2515 Loss_G: 0.6037\n",
            "===> Epoch[173](8/10): Loss_D: 0.2511 Loss_G: 0.5320\n",
            "===> Epoch[173](9/10): Loss_D: 0.2488 Loss_G: 0.6156\n",
            "===> Epoch[173](10/10): Loss_D: 0.2520 Loss_G: 0.5664\n",
            "learning rate = 0.0000535\n",
            "learning rate = 0.0000535\n",
            "===> Avg. PSNR: 15.5767 dB\n",
            "===> Epoch[174](1/10): Loss_D: 0.2528 Loss_G: 0.5781\n",
            "===> Epoch[174](2/10): Loss_D: 0.2459 Loss_G: 0.5912\n",
            "===> Epoch[174](3/10): Loss_D: 0.2509 Loss_G: 0.5880\n",
            "===> Epoch[174](4/10): Loss_D: 0.2499 Loss_G: 0.5227\n",
            "===> Epoch[174](5/10): Loss_D: 0.2508 Loss_G: 0.6098\n",
            "===> Epoch[174](6/10): Loss_D: 0.2478 Loss_G: 0.5742\n",
            "===> Epoch[174](7/10): Loss_D: 0.2513 Loss_G: 0.5297\n",
            "===> Epoch[174](8/10): Loss_D: 0.2534 Loss_G: 0.5508\n",
            "===> Epoch[174](9/10): Loss_D: 0.2520 Loss_G: 0.5127\n",
            "===> Epoch[174](10/10): Loss_D: 0.2528 Loss_G: 0.6000\n",
            "learning rate = 0.0000515\n",
            "learning rate = 0.0000515\n",
            "===> Avg. PSNR: 16.2283 dB\n",
            "===> Epoch[175](1/10): Loss_D: 0.2492 Loss_G: 0.5112\n",
            "===> Epoch[175](2/10): Loss_D: 0.2488 Loss_G: 0.5763\n",
            "===> Epoch[175](3/10): Loss_D: 0.2448 Loss_G: 0.5903\n",
            "===> Epoch[175](4/10): Loss_D: 0.2521 Loss_G: 0.5210\n",
            "===> Epoch[175](5/10): Loss_D: 0.2493 Loss_G: 0.5655\n",
            "===> Epoch[175](6/10): Loss_D: 0.2528 Loss_G: 0.5659\n",
            "===> Epoch[175](7/10): Loss_D: 0.2520 Loss_G: 0.5531\n",
            "===> Epoch[175](8/10): Loss_D: 0.2475 Loss_G: 0.5969\n",
            "===> Epoch[175](9/10): Loss_D: 0.2445 Loss_G: 0.6364\n",
            "===> Epoch[175](10/10): Loss_D: 0.2437 Loss_G: 0.5472\n",
            "learning rate = 0.0000495\n",
            "learning rate = 0.0000495\n",
            "===> Avg. PSNR: 15.3488 dB\n",
            "===> Epoch[176](1/10): Loss_D: 0.2504 Loss_G: 0.5708\n",
            "===> Epoch[176](2/10): Loss_D: 0.2497 Loss_G: 0.5723\n",
            "===> Epoch[176](3/10): Loss_D: 0.2515 Loss_G: 0.5224\n",
            "===> Epoch[176](4/10): Loss_D: 0.2484 Loss_G: 0.5682\n",
            "===> Epoch[176](5/10): Loss_D: 0.2488 Loss_G: 0.5372\n",
            "===> Epoch[176](6/10): Loss_D: 0.2519 Loss_G: 0.5432\n",
            "===> Epoch[176](7/10): Loss_D: 0.2458 Loss_G: 0.5861\n",
            "===> Epoch[176](8/10): Loss_D: 0.2523 Loss_G: 0.5477\n",
            "===> Epoch[176](9/10): Loss_D: 0.2510 Loss_G: 0.5709\n",
            "===> Epoch[176](10/10): Loss_D: 0.2445 Loss_G: 0.6017\n",
            "learning rate = 0.0000475\n",
            "learning rate = 0.0000475\n",
            "===> Avg. PSNR: 16.0839 dB\n",
            "===> Epoch[177](1/10): Loss_D: 0.2516 Loss_G: 0.5423\n",
            "===> Epoch[177](2/10): Loss_D: 0.2505 Loss_G: 0.5213\n",
            "===> Epoch[177](3/10): Loss_D: 0.2510 Loss_G: 0.5173\n",
            "===> Epoch[177](4/10): Loss_D: 0.2503 Loss_G: 0.5423\n",
            "===> Epoch[177](5/10): Loss_D: 0.2533 Loss_G: 0.5584\n",
            "===> Epoch[177](6/10): Loss_D: 0.2444 Loss_G: 0.5682\n",
            "===> Epoch[177](7/10): Loss_D: 0.2496 Loss_G: 0.5620\n",
            "===> Epoch[177](8/10): Loss_D: 0.2493 Loss_G: 0.6222\n",
            "===> Epoch[177](9/10): Loss_D: 0.2491 Loss_G: 0.5960\n",
            "===> Epoch[177](10/10): Loss_D: 0.2548 Loss_G: 0.5192\n",
            "learning rate = 0.0000455\n",
            "learning rate = 0.0000455\n",
            "===> Avg. PSNR: 16.7861 dB\n",
            "===> Epoch[178](1/10): Loss_D: 0.2512 Loss_G: 0.5126\n",
            "===> Epoch[178](2/10): Loss_D: 0.2458 Loss_G: 0.5254\n",
            "===> Epoch[178](3/10): Loss_D: 0.2523 Loss_G: 0.5434\n",
            "===> Epoch[178](4/10): Loss_D: 0.2541 Loss_G: 0.5841\n",
            "===> Epoch[178](5/10): Loss_D: 0.2500 Loss_G: 0.6193\n",
            "===> Epoch[178](6/10): Loss_D: 0.2517 Loss_G: 0.5047\n",
            "===> Epoch[178](7/10): Loss_D: 0.2515 Loss_G: 0.5324\n",
            "===> Epoch[178](8/10): Loss_D: 0.2418 Loss_G: 0.6108\n",
            "===> Epoch[178](9/10): Loss_D: 0.2488 Loss_G: 0.5314\n",
            "===> Epoch[178](10/10): Loss_D: 0.2494 Loss_G: 0.5679\n",
            "learning rate = 0.0000436\n",
            "learning rate = 0.0000436\n",
            "===> Avg. PSNR: 16.1914 dB\n",
            "===> Epoch[179](1/10): Loss_D: 0.2493 Loss_G: 0.6063\n",
            "===> Epoch[179](2/10): Loss_D: 0.2465 Loss_G: 0.5427\n",
            "===> Epoch[179](3/10): Loss_D: 0.2488 Loss_G: 0.5415\n",
            "===> Epoch[179](4/10): Loss_D: 0.2525 Loss_G: 0.5514\n",
            "===> Epoch[179](5/10): Loss_D: 0.2502 Loss_G: 0.5386\n",
            "===> Epoch[179](6/10): Loss_D: 0.2445 Loss_G: 0.5351\n",
            "===> Epoch[179](7/10): Loss_D: 0.2525 Loss_G: 0.5175\n",
            "===> Epoch[179](8/10): Loss_D: 0.2486 Loss_G: 0.5694\n",
            "===> Epoch[179](9/10): Loss_D: 0.2528 Loss_G: 0.5507\n",
            "===> Epoch[179](10/10): Loss_D: 0.2555 Loss_G: 0.5673\n",
            "learning rate = 0.0000416\n",
            "learning rate = 0.0000416\n",
            "===> Avg. PSNR: 15.8254 dB\n",
            "===> Epoch[180](1/10): Loss_D: 0.2515 Loss_G: 0.5831\n",
            "===> Epoch[180](2/10): Loss_D: 0.2512 Loss_G: 0.5345\n",
            "===> Epoch[180](3/10): Loss_D: 0.2505 Loss_G: 0.5367\n",
            "===> Epoch[180](4/10): Loss_D: 0.2510 Loss_G: 0.5490\n",
            "===> Epoch[180](5/10): Loss_D: 0.2502 Loss_G: 0.5178\n",
            "===> Epoch[180](6/10): Loss_D: 0.2508 Loss_G: 0.4810\n",
            "===> Epoch[180](7/10): Loss_D: 0.2508 Loss_G: 0.5261\n",
            "===> Epoch[180](8/10): Loss_D: 0.2463 Loss_G: 0.5675\n",
            "===> Epoch[180](9/10): Loss_D: 0.2497 Loss_G: 0.5534\n",
            "===> Epoch[180](10/10): Loss_D: 0.2498 Loss_G: 0.5566\n",
            "learning rate = 0.0000396\n",
            "learning rate = 0.0000396\n",
            "===> Avg. PSNR: 14.5278 dB\n",
            "===> Epoch[181](1/10): Loss_D: 0.2446 Loss_G: 0.5805\n",
            "===> Epoch[181](2/10): Loss_D: 0.2495 Loss_G: 0.6087\n",
            "===> Epoch[181](3/10): Loss_D: 0.2566 Loss_G: 0.5772\n",
            "===> Epoch[181](4/10): Loss_D: 0.2416 Loss_G: 0.6061\n",
            "===> Epoch[181](5/10): Loss_D: 0.2511 Loss_G: 0.4969\n",
            "===> Epoch[181](6/10): Loss_D: 0.2468 Loss_G: 0.5475\n",
            "===> Epoch[181](7/10): Loss_D: 0.2516 Loss_G: 0.5380\n",
            "===> Epoch[181](8/10): Loss_D: 0.2532 Loss_G: 0.4927\n",
            "===> Epoch[181](9/10): Loss_D: 0.2461 Loss_G: 0.5671\n",
            "===> Epoch[181](10/10): Loss_D: 0.2533 Loss_G: 0.5947\n",
            "learning rate = 0.0000376\n",
            "learning rate = 0.0000376\n",
            "===> Avg. PSNR: 16.4451 dB\n",
            "===> Epoch[182](1/10): Loss_D: 0.2499 Loss_G: 0.5397\n",
            "===> Epoch[182](2/10): Loss_D: 0.2482 Loss_G: 0.5468\n",
            "===> Epoch[182](3/10): Loss_D: 0.2504 Loss_G: 0.5088\n",
            "===> Epoch[182](4/10): Loss_D: 0.2532 Loss_G: 0.5229\n",
            "===> Epoch[182](5/10): Loss_D: 0.2503 Loss_G: 0.5414\n",
            "===> Epoch[182](6/10): Loss_D: 0.2399 Loss_G: 0.5766\n",
            "===> Epoch[182](7/10): Loss_D: 0.2518 Loss_G: 0.5637\n",
            "===> Epoch[182](8/10): Loss_D: 0.2507 Loss_G: 0.5521\n",
            "===> Epoch[182](9/10): Loss_D: 0.2503 Loss_G: 0.5455\n",
            "===> Epoch[182](10/10): Loss_D: 0.2531 Loss_G: 0.5275\n",
            "learning rate = 0.0000356\n",
            "learning rate = 0.0000356\n",
            "===> Avg. PSNR: 16.3475 dB\n",
            "===> Epoch[183](1/10): Loss_D: 0.2524 Loss_G: 0.5243\n",
            "===> Epoch[183](2/10): Loss_D: 0.2507 Loss_G: 0.5473\n",
            "===> Epoch[183](3/10): Loss_D: 0.2506 Loss_G: 0.5154\n",
            "===> Epoch[183](4/10): Loss_D: 0.2460 Loss_G: 0.5342\n",
            "===> Epoch[183](5/10): Loss_D: 0.2482 Loss_G: 0.4993\n",
            "===> Epoch[183](6/10): Loss_D: 0.2498 Loss_G: 0.5431\n",
            "===> Epoch[183](7/10): Loss_D: 0.2487 Loss_G: 0.5639\n",
            "===> Epoch[183](8/10): Loss_D: 0.2466 Loss_G: 0.5681\n",
            "===> Epoch[183](9/10): Loss_D: 0.2467 Loss_G: 0.5856\n",
            "===> Epoch[183](10/10): Loss_D: 0.2509 Loss_G: 0.5138\n",
            "learning rate = 0.0000337\n",
            "learning rate = 0.0000337\n",
            "===> Avg. PSNR: 16.0298 dB\n",
            "===> Epoch[184](1/10): Loss_D: 0.2506 Loss_G: 0.5641\n",
            "===> Epoch[184](2/10): Loss_D: 0.2472 Loss_G: 0.5834\n",
            "===> Epoch[184](3/10): Loss_D: 0.2522 Loss_G: 0.5497\n",
            "===> Epoch[184](4/10): Loss_D: 0.2524 Loss_G: 0.5475\n",
            "===> Epoch[184](5/10): Loss_D: 0.2516 Loss_G: 0.5549\n",
            "===> Epoch[184](6/10): Loss_D: 0.2520 Loss_G: 0.5469\n",
            "===> Epoch[184](7/10): Loss_D: 0.2484 Loss_G: 0.5334\n",
            "===> Epoch[184](8/10): Loss_D: 0.2478 Loss_G: 0.5325\n",
            "===> Epoch[184](9/10): Loss_D: 0.2485 Loss_G: 0.5809\n",
            "===> Epoch[184](10/10): Loss_D: 0.2435 Loss_G: 0.5581\n",
            "learning rate = 0.0000317\n",
            "learning rate = 0.0000317\n",
            "===> Avg. PSNR: 16.4303 dB\n",
            "===> Epoch[185](1/10): Loss_D: 0.2507 Loss_G: 0.5404\n",
            "===> Epoch[185](2/10): Loss_D: 0.2499 Loss_G: 0.5204\n",
            "===> Epoch[185](3/10): Loss_D: 0.2521 Loss_G: 0.5105\n",
            "===> Epoch[185](4/10): Loss_D: 0.2492 Loss_G: 0.5586\n",
            "===> Epoch[185](5/10): Loss_D: 0.2509 Loss_G: 0.5614\n",
            "===> Epoch[185](6/10): Loss_D: 0.2489 Loss_G: 0.5010\n",
            "===> Epoch[185](7/10): Loss_D: 0.2508 Loss_G: 0.5445\n",
            "===> Epoch[185](8/10): Loss_D: 0.2492 Loss_G: 0.5049\n",
            "===> Epoch[185](9/10): Loss_D: 0.2504 Loss_G: 0.5446\n",
            "===> Epoch[185](10/10): Loss_D: 0.2520 Loss_G: 0.5145\n",
            "learning rate = 0.0000297\n",
            "learning rate = 0.0000297\n",
            "===> Avg. PSNR: 15.1158 dB\n",
            "===> Epoch[186](1/10): Loss_D: 0.2506 Loss_G: 0.5246\n",
            "===> Epoch[186](2/10): Loss_D: 0.2491 Loss_G: 0.5924\n",
            "===> Epoch[186](3/10): Loss_D: 0.2482 Loss_G: 0.5340\n",
            "===> Epoch[186](4/10): Loss_D: 0.2459 Loss_G: 0.5890\n",
            "===> Epoch[186](5/10): Loss_D: 0.2508 Loss_G: 0.5314\n",
            "===> Epoch[186](6/10): Loss_D: 0.2489 Loss_G: 0.5687\n",
            "===> Epoch[186](7/10): Loss_D: 0.2466 Loss_G: 0.5529\n",
            "===> Epoch[186](8/10): Loss_D: 0.2489 Loss_G: 0.5452\n",
            "===> Epoch[186](9/10): Loss_D: 0.2532 Loss_G: 0.5314\n",
            "===> Epoch[186](10/10): Loss_D: 0.2461 Loss_G: 0.5462\n",
            "learning rate = 0.0000277\n",
            "learning rate = 0.0000277\n",
            "===> Avg. PSNR: 15.7096 dB\n",
            "===> Epoch[187](1/10): Loss_D: 0.2449 Loss_G: 0.5277\n",
            "===> Epoch[187](2/10): Loss_D: 0.2515 Loss_G: 0.5264\n",
            "===> Epoch[187](3/10): Loss_D: 0.2522 Loss_G: 0.5496\n",
            "===> Epoch[187](4/10): Loss_D: 0.2531 Loss_G: 0.5760\n",
            "===> Epoch[187](5/10): Loss_D: 0.2530 Loss_G: 0.5049\n",
            "===> Epoch[187](6/10): Loss_D: 0.2522 Loss_G: 0.5102\n",
            "===> Epoch[187](7/10): Loss_D: 0.2448 Loss_G: 0.5111\n",
            "===> Epoch[187](8/10): Loss_D: 0.2515 Loss_G: 0.5781\n",
            "===> Epoch[187](9/10): Loss_D: 0.2473 Loss_G: 0.5516\n",
            "===> Epoch[187](10/10): Loss_D: 0.2534 Loss_G: 0.5429\n",
            "learning rate = 0.0000257\n",
            "learning rate = 0.0000257\n",
            "===> Avg. PSNR: 16.4284 dB\n",
            "===> Epoch[188](1/10): Loss_D: 0.2483 Loss_G: 0.4900\n",
            "===> Epoch[188](2/10): Loss_D: 0.2457 Loss_G: 0.5545\n",
            "===> Epoch[188](3/10): Loss_D: 0.2489 Loss_G: 0.5633\n",
            "===> Epoch[188](4/10): Loss_D: 0.2513 Loss_G: 0.5798\n",
            "===> Epoch[188](5/10): Loss_D: 0.2492 Loss_G: 0.5476\n",
            "===> Epoch[188](6/10): Loss_D: 0.2534 Loss_G: 0.4841\n",
            "===> Epoch[188](7/10): Loss_D: 0.2534 Loss_G: 0.5065\n",
            "===> Epoch[188](8/10): Loss_D: 0.2443 Loss_G: 0.5260\n",
            "===> Epoch[188](9/10): Loss_D: 0.2513 Loss_G: 0.5308\n",
            "===> Epoch[188](10/10): Loss_D: 0.2396 Loss_G: 0.5583\n",
            "learning rate = 0.0000238\n",
            "learning rate = 0.0000238\n",
            "===> Avg. PSNR: 15.9148 dB\n",
            "===> Epoch[189](1/10): Loss_D: 0.2470 Loss_G: 0.5852\n",
            "===> Epoch[189](2/10): Loss_D: 0.2514 Loss_G: 0.5285\n",
            "===> Epoch[189](3/10): Loss_D: 0.2445 Loss_G: 0.5649\n",
            "===> Epoch[189](4/10): Loss_D: 0.2517 Loss_G: 0.4961\n",
            "===> Epoch[189](5/10): Loss_D: 0.2526 Loss_G: 0.5099\n",
            "===> Epoch[189](6/10): Loss_D: 0.2460 Loss_G: 0.5556\n",
            "===> Epoch[189](7/10): Loss_D: 0.2511 Loss_G: 0.4791\n",
            "===> Epoch[189](8/10): Loss_D: 0.2524 Loss_G: 0.5033\n",
            "===> Epoch[189](9/10): Loss_D: 0.2436 Loss_G: 0.5699\n",
            "===> Epoch[189](10/10): Loss_D: 0.2510 Loss_G: 0.4908\n",
            "learning rate = 0.0000218\n",
            "learning rate = 0.0000218\n",
            "===> Avg. PSNR: 16.8399 dB\n",
            "===> Epoch[190](1/10): Loss_D: 0.2493 Loss_G: 0.5152\n",
            "===> Epoch[190](2/10): Loss_D: 0.2525 Loss_G: 0.5118\n",
            "===> Epoch[190](3/10): Loss_D: 0.2518 Loss_G: 0.5319\n",
            "===> Epoch[190](4/10): Loss_D: 0.2506 Loss_G: 0.5354\n",
            "===> Epoch[190](5/10): Loss_D: 0.2507 Loss_G: 0.5410\n",
            "===> Epoch[190](6/10): Loss_D: 0.2481 Loss_G: 0.5304\n",
            "===> Epoch[190](7/10): Loss_D: 0.2496 Loss_G: 0.5405\n",
            "===> Epoch[190](8/10): Loss_D: 0.2520 Loss_G: 0.5161\n",
            "===> Epoch[190](9/10): Loss_D: 0.2506 Loss_G: 0.5169\n",
            "===> Epoch[190](10/10): Loss_D: 0.2517 Loss_G: 0.4590\n",
            "learning rate = 0.0000198\n",
            "learning rate = 0.0000198\n",
            "===> Avg. PSNR: 16.1260 dB\n",
            "===> Epoch[191](1/10): Loss_D: 0.2520 Loss_G: 0.4787\n",
            "===> Epoch[191](2/10): Loss_D: 0.2477 Loss_G: 0.5498\n",
            "===> Epoch[191](3/10): Loss_D: 0.2447 Loss_G: 0.5753\n",
            "===> Epoch[191](4/10): Loss_D: 0.2527 Loss_G: 0.4932\n",
            "===> Epoch[191](5/10): Loss_D: 0.2494 Loss_G: 0.5247\n",
            "===> Epoch[191](6/10): Loss_D: 0.2499 Loss_G: 0.5564\n",
            "===> Epoch[191](7/10): Loss_D: 0.2532 Loss_G: 0.4992\n",
            "===> Epoch[191](8/10): Loss_D: 0.2516 Loss_G: 0.5023\n",
            "===> Epoch[191](9/10): Loss_D: 0.2498 Loss_G: 0.5550\n",
            "===> Epoch[191](10/10): Loss_D: 0.2496 Loss_G: 0.5012\n",
            "learning rate = 0.0000178\n",
            "learning rate = 0.0000178\n",
            "===> Avg. PSNR: 16.2556 dB\n",
            "===> Epoch[192](1/10): Loss_D: 0.2535 Loss_G: 0.5203\n",
            "===> Epoch[192](2/10): Loss_D: 0.2526 Loss_G: 0.5037\n",
            "===> Epoch[192](3/10): Loss_D: 0.2436 Loss_G: 0.5740\n",
            "===> Epoch[192](4/10): Loss_D: 0.2453 Loss_G: 0.5260\n",
            "===> Epoch[192](5/10): Loss_D: 0.2465 Loss_G: 0.5061\n",
            "===> Epoch[192](6/10): Loss_D: 0.2538 Loss_G: 0.5203\n",
            "===> Epoch[192](7/10): Loss_D: 0.2493 Loss_G: 0.5229\n",
            "===> Epoch[192](8/10): Loss_D: 0.2486 Loss_G: 0.5254\n",
            "===> Epoch[192](9/10): Loss_D: 0.2498 Loss_G: 0.5021\n",
            "===> Epoch[192](10/10): Loss_D: 0.2482 Loss_G: 0.5324\n",
            "learning rate = 0.0000158\n",
            "learning rate = 0.0000158\n",
            "===> Avg. PSNR: 15.8919 dB\n",
            "===> Epoch[193](1/10): Loss_D: 0.2496 Loss_G: 0.5337\n",
            "===> Epoch[193](2/10): Loss_D: 0.2530 Loss_G: 0.5361\n",
            "===> Epoch[193](3/10): Loss_D: 0.2465 Loss_G: 0.5286\n",
            "===> Epoch[193](4/10): Loss_D: 0.2458 Loss_G: 0.5083\n",
            "===> Epoch[193](5/10): Loss_D: 0.2507 Loss_G: 0.5287\n",
            "===> Epoch[193](6/10): Loss_D: 0.2528 Loss_G: 0.4826\n",
            "===> Epoch[193](7/10): Loss_D: 0.2502 Loss_G: 0.4899\n",
            "===> Epoch[193](8/10): Loss_D: 0.2493 Loss_G: 0.6197\n",
            "===> Epoch[193](9/10): Loss_D: 0.2465 Loss_G: 0.5381\n",
            "===> Epoch[193](10/10): Loss_D: 0.2533 Loss_G: 0.5268\n",
            "learning rate = 0.0000139\n",
            "learning rate = 0.0000139\n",
            "===> Avg. PSNR: 16.0508 dB\n",
            "===> Epoch[194](1/10): Loss_D: 0.2507 Loss_G: 0.5307\n",
            "===> Epoch[194](2/10): Loss_D: 0.2446 Loss_G: 0.5435\n",
            "===> Epoch[194](3/10): Loss_D: 0.2495 Loss_G: 0.4925\n",
            "===> Epoch[194](4/10): Loss_D: 0.2429 Loss_G: 0.5199\n",
            "===> Epoch[194](5/10): Loss_D: 0.2489 Loss_G: 0.4998\n",
            "===> Epoch[194](6/10): Loss_D: 0.2468 Loss_G: 0.5298\n",
            "===> Epoch[194](7/10): Loss_D: 0.2508 Loss_G: 0.5361\n",
            "===> Epoch[194](8/10): Loss_D: 0.2457 Loss_G: 0.5332\n",
            "===> Epoch[194](9/10): Loss_D: 0.2516 Loss_G: 0.4928\n",
            "===> Epoch[194](10/10): Loss_D: 0.2521 Loss_G: 0.5193\n",
            "learning rate = 0.0000119\n",
            "learning rate = 0.0000119\n",
            "===> Avg. PSNR: 15.6913 dB\n",
            "===> Epoch[195](1/10): Loss_D: 0.2513 Loss_G: 0.5097\n",
            "===> Epoch[195](2/10): Loss_D: 0.2444 Loss_G: 0.5246\n",
            "===> Epoch[195](3/10): Loss_D: 0.2485 Loss_G: 0.4726\n",
            "===> Epoch[195](4/10): Loss_D: 0.2506 Loss_G: 0.4713\n",
            "===> Epoch[195](5/10): Loss_D: 0.2461 Loss_G: 0.5265\n",
            "===> Epoch[195](6/10): Loss_D: 0.2497 Loss_G: 0.5269\n",
            "===> Epoch[195](7/10): Loss_D: 0.2501 Loss_G: 0.5403\n",
            "===> Epoch[195](8/10): Loss_D: 0.2486 Loss_G: 0.5112\n",
            "===> Epoch[195](9/10): Loss_D: 0.2521 Loss_G: 0.5311\n",
            "===> Epoch[195](10/10): Loss_D: 0.2500 Loss_G: 0.5107\n",
            "learning rate = 0.0000099\n",
            "learning rate = 0.0000099\n",
            "===> Avg. PSNR: 16.7433 dB\n",
            "===> Epoch[196](1/10): Loss_D: 0.2511 Loss_G: 0.4793\n",
            "===> Epoch[196](2/10): Loss_D: 0.2486 Loss_G: 0.5604\n",
            "===> Epoch[196](3/10): Loss_D: 0.2492 Loss_G: 0.5298\n",
            "===> Epoch[196](4/10): Loss_D: 0.2502 Loss_G: 0.4777\n",
            "===> Epoch[196](5/10): Loss_D: 0.2522 Loss_G: 0.5005\n",
            "===> Epoch[196](6/10): Loss_D: 0.2498 Loss_G: 0.5490\n",
            "===> Epoch[196](7/10): Loss_D: 0.2482 Loss_G: 0.5301\n",
            "===> Epoch[196](8/10): Loss_D: 0.2461 Loss_G: 0.4857\n",
            "===> Epoch[196](9/10): Loss_D: 0.2525 Loss_G: 0.5301\n",
            "===> Epoch[196](10/10): Loss_D: 0.2512 Loss_G: 0.4783\n",
            "learning rate = 0.0000079\n",
            "learning rate = 0.0000079\n",
            "===> Avg. PSNR: 15.1097 dB\n",
            "===> Epoch[197](1/10): Loss_D: 0.2440 Loss_G: 0.5653\n",
            "===> Epoch[197](2/10): Loss_D: 0.2500 Loss_G: 0.5623\n",
            "===> Epoch[197](3/10): Loss_D: 0.2512 Loss_G: 0.4750\n",
            "===> Epoch[197](4/10): Loss_D: 0.2528 Loss_G: 0.5138\n",
            "===> Epoch[197](5/10): Loss_D: 0.2478 Loss_G: 0.5386\n",
            "===> Epoch[197](6/10): Loss_D: 0.2510 Loss_G: 0.4958\n",
            "===> Epoch[197](7/10): Loss_D: 0.2521 Loss_G: 0.4805\n",
            "===> Epoch[197](8/10): Loss_D: 0.2408 Loss_G: 0.5596\n",
            "===> Epoch[197](9/10): Loss_D: 0.2502 Loss_G: 0.5211\n",
            "===> Epoch[197](10/10): Loss_D: 0.2545 Loss_G: 0.5032\n",
            "learning rate = 0.0000059\n",
            "learning rate = 0.0000059\n",
            "===> Avg. PSNR: 15.7042 dB\n",
            "===> Epoch[198](1/10): Loss_D: 0.2474 Loss_G: 0.4852\n",
            "===> Epoch[198](2/10): Loss_D: 0.2516 Loss_G: 0.5194\n",
            "===> Epoch[198](3/10): Loss_D: 0.2427 Loss_G: 0.5496\n",
            "===> Epoch[198](4/10): Loss_D: 0.2458 Loss_G: 0.5135\n",
            "===> Epoch[198](5/10): Loss_D: 0.2481 Loss_G: 0.5112\n",
            "===> Epoch[198](6/10): Loss_D: 0.2488 Loss_G: 0.4996\n",
            "===> Epoch[198](7/10): Loss_D: 0.2512 Loss_G: 0.5367\n",
            "===> Epoch[198](8/10): Loss_D: 0.2518 Loss_G: 0.5383\n",
            "===> Epoch[198](9/10): Loss_D: 0.2512 Loss_G: 0.5241\n",
            "===> Epoch[198](10/10): Loss_D: 0.2500 Loss_G: 0.4886\n",
            "learning rate = 0.0000040\n",
            "learning rate = 0.0000040\n",
            "===> Avg. PSNR: 15.6266 dB\n",
            "===> Epoch[199](1/10): Loss_D: 0.2447 Loss_G: 0.5589\n",
            "===> Epoch[199](2/10): Loss_D: 0.2528 Loss_G: 0.4706\n",
            "===> Epoch[199](3/10): Loss_D: 0.2417 Loss_G: 0.5582\n",
            "===> Epoch[199](4/10): Loss_D: 0.2480 Loss_G: 0.5346\n",
            "===> Epoch[199](5/10): Loss_D: 0.2499 Loss_G: 0.5179\n",
            "===> Epoch[199](6/10): Loss_D: 0.2482 Loss_G: 0.5005\n",
            "===> Epoch[199](7/10): Loss_D: 0.2494 Loss_G: 0.4998\n",
            "===> Epoch[199](8/10): Loss_D: 0.2492 Loss_G: 0.5299\n",
            "===> Epoch[199](9/10): Loss_D: 0.2522 Loss_G: 0.5404\n",
            "===> Epoch[199](10/10): Loss_D: 0.2507 Loss_G: 0.5105\n",
            "learning rate = 0.0000020\n",
            "learning rate = 0.0000020\n",
            "===> Avg. PSNR: 15.7469 dB\n",
            "===> Epoch[200](1/10): Loss_D: 0.2489 Loss_G: 0.5030\n",
            "===> Epoch[200](2/10): Loss_D: 0.2450 Loss_G: 0.5754\n",
            "===> Epoch[200](3/10): Loss_D: 0.2522 Loss_G: 0.5484\n",
            "===> Epoch[200](4/10): Loss_D: 0.2522 Loss_G: 0.4842\n",
            "===> Epoch[200](5/10): Loss_D: 0.2485 Loss_G: 0.4839\n",
            "===> Epoch[200](6/10): Loss_D: 0.2526 Loss_G: 0.4893\n",
            "===> Epoch[200](7/10): Loss_D: 0.2473 Loss_G: 0.5481\n",
            "===> Epoch[200](8/10): Loss_D: 0.2465 Loss_G: 0.5187\n",
            "===> Epoch[200](9/10): Loss_D: 0.2497 Loss_G: 0.5071\n",
            "===> Epoch[200](10/10): Loss_D: 0.2515 Loss_G: 0.5160\n",
            "learning rate = 0.0000000\n",
            "learning rate = 0.0000000\n",
            "===> Avg. PSNR: 16.1832 dB\n",
            "Checkpoint saved to checkpoint./football\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRkrCAN3h8qT"
      },
      "source": [
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description='pix2pix-pytorch-implementation')\n",
        "parser.add_argument('--dataset', required=True, help='facades')\n",
        "parser.add_argument('--batch_size', type=int, default=1, help='training batch size')\n",
        "parser.add_argument('--test_batch_size', type=int, default=1, help='testing batch size')\n",
        "parser.add_argument('--direction', type=str, default='b2a', help='a2b or b2a')\n",
        "parser.add_argument('--input_nc', type=int, default=3, help='input image channels')\n",
        "parser.add_argument('--output_nc', type=int, default=3, help='output image channels')\n",
        "parser.add_argument('--ngf', type=int, default=64, help='generator filters in first conv layer')\n",
        "parser.add_argument('--ndf', type=int, default=64, help='discriminator filters in first conv layer')\n",
        "parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count')\n",
        "parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n",
        "parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
        "parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau|cosine')\n",
        "parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
        "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
        "parser.add_argument('--cuda', action='store_true', help='use cuda?')\n",
        "parser.add_argument('--threads', type=int, default=4, help='number of threads for data loader to use')\n",
        "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
        "parser.add_argument('--lamb', type=int, default=10, help='weight on L1 term in objective')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "print(opt)\n",
        "\n",
        "if opt.cuda and not torch.cuda.is_available():\n",
        "    raise Exception(\"No GPU found, please run without --cuda\")\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "torch.manual_seed(opt.seed)\n",
        "if opt.cuda:\n",
        "   \n",
        "    torch.cuda.manual_seed(opt.seed)\n",
        "\n",
        "print('===> Loading datasets')\n",
        "root_path = \"dataset/\"\n",
        "train_set = get_training_set(root_path + opt.dataset, opt.direction)\n",
        "test_set = get_test_set(root_path + opt.dataset, opt.direction)\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batch_size, shuffle=True)\n",
        "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.test_batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
        "\n",
        "print('===> Building models')\n",
        "net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\n",
        "net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'basic', gpu_id=device)\n",
        "\n",
        "criterionGAN = GANLoss().to(device)\n",
        "criterionL1 = nn.L1Loss().to(device)\n",
        "criterionMSE = nn.MSELoss().to(device)\n",
        "\n",
        "# setup optimizer\n",
        "optimizer_g = optim.Adam(net_g.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "optimizer_d = optim.Adam(net_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "net_g_scheduler = get_scheduler(optimizer_g, opt)\n",
        "net_d_scheduler = get_scheduler(optimizer_d, opt)\n",
        "\n",
        "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n",
        "    # train\n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\n",
        "        # forward\n",
        "        real_a, real_b = batch[0].to(device), batch[1].to(device)\n",
        "        fake_b = net_g(real_a)\n",
        "\n",
        "        ######################\n",
        "        # (1) Update D network\n",
        "        ######################\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "        \n",
        "        # train with fake\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = net_d.forward(fake_ab.detach())\n",
        "        loss_d_fake = criterionGAN(pred_fake, False)\n",
        "\n",
        "        # train with real\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\n",
        "        pred_real = net_d.forward(real_ab)\n",
        "        loss_d_real = criterionGAN(pred_real, True)\n",
        "        \n",
        "        # Combined D loss\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
        "\n",
        "        loss_d.backward()\n",
        "       \n",
        "        optimizer_d.step()\n",
        "\n",
        "        ######################\n",
        "        # (2) Update G network\n",
        "        ######################\n",
        "\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = net_d.forward(fake_ab)\n",
        "        loss_g_gan = criterionGAN(pred_fake, True)\n",
        "\n",
        "        # Second, G(A) = B\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * opt.lamb\n",
        "        \n",
        "        loss_g = loss_g_gan + loss_g_l1\n",
        "        \n",
        "        loss_g.backward()\n",
        "\n",
        "        optimizer_g.step()\n",
        "\n",
        "        print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
        "            epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\n",
        "\n",
        "    update_learning_rate(net_g_scheduler, optimizer_g)\n",
        "    update_learning_rate(net_d_scheduler, optimizer_d)\n",
        "\n",
        "    # test\n",
        "    avg_psnr = 0\n",
        "    for batch in testing_data_loader:\n",
        "        input, target = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        prediction = net_g(input)\n",
        "        mse = criterionMSE(prediction, target)\n",
        "        psnr = 10 * log10(1 / mse.item())\n",
        "        avg_psnr += psnr\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n",
        "\n",
        "    #checkpoint\n",
        "    if epoch % 50 == 0:\n",
        "        if not os.path.exists(\"checkpoint\"):\n",
        "            os.mkdir(\"checkpoint\")\n",
        "        if not os.path.exists(os.path.join(\"checkpoint\", opt.dataset)):\n",
        "            os.mkdir(os.path.join(\"checkpoint\", opt.dataset))\n",
        "        net_g_model_out_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(opt.dataset, epoch)\n",
        "        net_d_model_out_path = \"checkpoint/{}/netD_model_epoch_{}.pth\".format(opt.dataset, epoch)\n",
        "        torch.save(net_g, net_g_model_out_path)\n",
        "        torch.save(net_d, net_d_model_out_path)\n",
        "        print(\"Checkpoint saved to {}\".format(\"checkpoint\" + opt.dataset))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AMORDFMCCwn",
        "outputId": "aa2f17f7-ac9e-44d7-d31c-2d356a038445"
      },
      "source": [
        "!python test.py --dataset ./football --direction a2b --nepochs 200  --cuda "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(cuda=True, dataset='./football', direction='a2b', nepochs=200)\n",
            "Image saved as result/./football/3.jpg\n",
            "Image saved as result/./football/5.jpg\n",
            "Image saved as result/./football/2.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diDUeFskFBDj"
      },
      "source": [
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z8ZI_QRFCw5"
      },
      "source": [
        "unorm = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUSjY7rQDPYh"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from utils import is_image_file, load_img, save_img\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "dataset = 'football'\n",
        "model_path = \"checkpoint/\" + dataset + \"/netG_model_epoch_200.pth\"\n",
        "net_g = torch.load(model_path).to(device)\n",
        "\n",
        "\n",
        "image_dir = \"./dataset/test/\"\n",
        "\n",
        "\n",
        "image_filenames = [x for x in os.listdir(image_dir) if is_image_file(x)]\n",
        "\n",
        "transform_list = [transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "transform = transforms.Compose(transform_list)\n",
        "\n",
        "\n",
        "for image_name in image_filenames:\n",
        "    img_unorm = load_img(image_dir + image_name)\n",
        "    img = transform(img_unorm)\n",
        "    input = img.unsqueeze(0).to(device)\n",
        "    out = net_g(input)\n",
        "    out_img = out.detach().squeeze(0).cpu()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,12))\n",
        "    ax1.imshow(img_unorm)\n",
        "    ax2.imshow(unorm(out_img).permute(1,2,0))\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioiYzV2uUFAM"
      },
      "source": [
        "torch.save(net_g.state_dict(),  './checkpoint/football/netg_state_dict_E200')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "VwwRqvGHCCHC",
        "outputId": "6f497fb2-adfc-46c9-ba9d-ce83c82c6c8d"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from utils import is_image_file, load_img, save_img\n",
        "\n",
        "# Testing settings\n",
        "parser = argparse.ArgumentParser(description='pix2pix-pytorch-implementation')\n",
        "parser.add_argument('--dataset', required=True, help='facades')\n",
        "parser.add_argument('--direction', type=str, default='b2a', help='a2b or b2a')\n",
        "parser.add_argument('--nepochs', type=int, default=200, help='saved model of which epochs')\n",
        "parser.add_argument('--cuda', action='store_true', help='use cuda')\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
        "\n",
        "model_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(opt.dataset, opt.nepochs)\n",
        "\n",
        "net_g = torch.load(model_path).to(device)\n",
        "\n",
        "if opt.direction == \"a2b\":\n",
        "    image_dir = \"dataset/{}/test/a/\".format(opt.dataset)\n",
        "else:\n",
        "    image_dir = \"dataset/{}/test/b/\".format(opt.dataset)\n",
        "\n",
        "image_filenames = [x for x in os.listdir(image_dir) if is_image_file(x)]\n",
        "\n",
        "transform_list = [transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "transform = transforms.Compose(transform_list)\n",
        "\n",
        "for image_name in image_filenames:\n",
        "    img = load_img(image_dir + image_name)\n",
        "    img = transform(img)\n",
        "    input = img.unsqueeze(0).to(device)\n",
        "    out = net_g(input)\n",
        "    out_img = out.detach().squeeze(0).cpu()\n",
        "\n",
        "    if not os.path.exists(os.path.join(\"result\", opt.dataset)):\n",
        "        os.makedirs(os.path.join(\"result\", opt.dataset))\n",
        "    save_img(out_img, \"result/{}/{}\".format(opt.dataset, image_name))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] --dataset DATASET [--direction DIRECTION]\n",
            "                             [--nepochs NEPOCHS] [--cuda]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --dataset\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}